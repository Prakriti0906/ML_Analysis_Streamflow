{"cells":[{"cell_type":"markdown","metadata":{"id":"cOxzuH_kOfSF"},"source":["# **Start**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25310,"status":"ok","timestamp":1762764238763,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"},"user_tz":-330},"id":"d5vKr6J_3p22","outputId":"6d7d6214-fbcd-46d1-8d8f-716a754bf4d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":303904,"status":"ok","timestamp":1762764542679,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"},"user_tz":-330},"id":"t1jgcsTHOkrm","outputId":"e5dc068e-2818-490d-c218-4b36cc95477e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: scikit-learn 1.6.1\n","Uninstalling scikit-learn-1.6.1:\n","  Successfully uninstalled scikit-learn-1.6.1\n","Collecting scikit-learn==1.5.2\n","  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n","Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-learn\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed scikit-learn-1.5.2\n","Collecting bayesian-optimization\n","  Downloading bayesian_optimization-3.1.0-py3-none-any.whl.metadata (11 kB)\n","Collecting colorama>=0.4.6 (from bayesian-optimization)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (2.0.2)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.5.2)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (3.6.0)\n","Downloading bayesian_optimization-3.1.0-py3-none-any.whl (36 kB)\n","Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: colorama, bayesian-optimization\n","Successfully installed bayesian-optimization-3.1.0 colorama-0.4.6\n","Collecting optuna\n","  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n","Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, optuna\n","Successfully installed colorlog-6.10.1 optuna-4.6.0\n","Collecting gpboost==1.6.1\n","  Downloading gpboost-1.6.1-py3-none-manylinux1_x86_64.whl.metadata (7.9 kB)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from gpboost==1.6.1) (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from gpboost==1.6.1) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from gpboost==1.6.1) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from gpboost==1.6.1) (1.16.3)\n","Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.12/dist-packages (from gpboost==1.6.1) (1.5.2)\n","Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (from gpboost==1.6.1) (4.6.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0->gpboost==1.6.1) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0->gpboost==1.6.1) (3.6.0)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->gpboost==1.6.1) (1.17.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna->gpboost==1.6.1) (6.10.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna->gpboost==1.6.1) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->gpboost==1.6.1) (2.0.44)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna->gpboost==1.6.1) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna->gpboost==1.6.1) (6.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->gpboost==1.6.1) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->gpboost==1.6.1) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->gpboost==1.6.1) (2025.2)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->gpboost==1.6.1) (1.3.10)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->gpboost==1.6.1) (4.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->gpboost==1.6.1) (1.17.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->gpboost==1.6.1) (3.2.4)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna->gpboost==1.6.1) (3.0.3)\n","Downloading gpboost-1.6.1-py3-none-manylinux1_x86_64.whl (5.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gpboost\n","Successfully installed gpboost-1.6.1\n","Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.49.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from shap) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.16.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from shap) (1.5.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n","Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n","Collecting ngboost\n","  Downloading ngboost-0.5.7-py3-none-any.whl.metadata (4.2 kB)\n","Collecting lifelines>=0.25 (from ngboost)\n","  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from ngboost) (2.0.2)\n","Collecting scikit-learn<2.0,>=1.6 (from ngboost)\n","  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: scipy>=1.7.2 in /usr/local/lib/python3.12/dist-packages (from ngboost) (1.16.3)\n","Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.12/dist-packages (from ngboost) (4.67.1)\n","Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.12/dist-packages (from lifelines>=0.25->ngboost) (2.2.2)\n","Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.12/dist-packages (from lifelines>=0.25->ngboost) (3.10.0)\n","Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.12/dist-packages (from lifelines>=0.25->ngboost) (1.8.0)\n","Collecting autograd-gamma>=0.3 (from lifelines>=0.25->ngboost)\n","  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting formulaic>=0.2.2 (from lifelines>=0.25->ngboost)\n","  Downloading formulaic-1.2.1-py3-none-any.whl.metadata (7.0 kB)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0,>=1.6->ngboost) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0,>=1.6->ngboost) (3.6.0)\n","Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n","  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: narwhals>=1.17 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (2.10.2)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (4.15.0)\n","Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (2.0.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1->lifelines>=0.25->ngboost) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1->lifelines>=0.25->ngboost) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines>=0.25->ngboost) (1.17.0)\n","Downloading ngboost-0.5.7-py3-none-any.whl (35 kB)\n","Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading formulaic-1.2.1-py3-none-any.whl (117 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: autograd-gamma\n","  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=589073aa817b33f7011eacf92d6dc6c434af7ffab39fea01926d7162cffd037f\n","  Stored in directory: /root/.cache/pip/wheels/50/37/21/0a719b9d89c635e89ff24bd93b862882ad675279552013b2fb\n","Successfully built autograd-gamma\n","Installing collected packages: interface-meta, scikit-learn, autograd-gamma, formulaic, lifelines, ngboost\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.5.2\n","    Uninstalling scikit-learn-1.5.2:\n","      Successfully uninstalled scikit-learn-1.5.2\n","Successfully installed autograd-gamma-0.5.0 formulaic-1.2.1 interface-meta-1.3.0 lifelines-0.30.0 ngboost-0.5.7 scikit-learn-1.7.2\n","Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.12/dist-packages (2025.5.0)\n","Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (8.3.0)\n","Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (3.1.2)\n","Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (2025.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (25.0)\n","Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (1.4.2)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (6.0.3)\n","Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (0.12.1)\n","Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (2.2.2)\n","Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (18.1.0)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->dask[dataframe]) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->dask[dataframe]) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.2)\n","Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.17.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n","Collecting lime\n","  Downloading lime-0.2.0.1.tar.gz (275 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lime) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.7.2)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.5)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.10.16)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (25.0)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n","Building wheels for collected packages: lime\n","  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=99f849f3154ca667f9ac8ef7e3c9185ec026f1d04d14ebff46017db19e07a5a0\n","  Stored in directory: /root/.cache/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n","Successfully built lime\n","Installing collected packages: lime\n","Successfully installed lime-0.2.0.1\n","Collecting interpret\n","  Downloading interpret-0.7.3-py3-none-any.whl.metadata (1.2 kB)\n","Collecting interpret-core==0.7.3 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading interpret_core-0.7.3-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.0.2)\n","Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.12/dist-packages (from interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.2.2)\n","Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.7.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.5.2)\n","Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (5.9.5)\n","Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (6.17.1)\n","Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (7.34.0)\n","Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (5.24.1)\n","Collecting SALib>=1.3.3 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading salib-1.5.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.49.1)\n","Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.3.8)\n","Collecting aplr>=10.6.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading aplr-10.18.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n","Collecting dash<3.0.0,>=2.0.0 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n","Collecting dash-cytoscape>=0.1.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading dash_cytoscape-1.0.2.tar.gz (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gevent>=1.3.6 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading gevent-25.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.32.4)\n","Collecting Flask<3.1,>=1.0.4 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n","Collecting Werkzeug<3.1 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n","Collecting dash-html-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n","Collecting dash-core-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting dash-table==5.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (8.7.0)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.15.0)\n","Collecting retrying (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (75.2.0)\n","Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.2.4)\n","Collecting zope.event (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading zope_event-6.1-py3-none-any.whl.metadata (5.1 kB)\n","Collecting zope.interface (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading zope_interface-8.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.8.15)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (7.4.9)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.2.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (25.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (26.2.1)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (6.5.1)\n","Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (5.7.1)\n","Collecting jedi>=0.16 (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.0.52)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.19.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.9.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19.2->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19.2->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19.2->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2025.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (8.5.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2025.10.5)\n","Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.12/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.10.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.70.16)\n","Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.12/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.16.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.6.0)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.67.1)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.0.8)\n","Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.1.2)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (8.3.0)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.12/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.8.5)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.4)\n","Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (5.9.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.2.5)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.43.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.2.14)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.0.3)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.23.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.5.0)\n","Downloading interpret-0.7.3-py3-none-any.whl (1.4 kB)\n","Downloading interpret_core-0.7.3-py3-none-any.whl (16.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aplr-10.18.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n","Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n","Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n","Downloading gevent-25.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading salib-1.5.2-py3-none-any.whl (780 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.1/780.1 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n","Downloading zope_event-6.1-py3-none-any.whl (6.4 kB)\n","Downloading zope_interface-8.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.7/264.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: dash-cytoscape\n","  Building wheel for dash-cytoscape (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dash-cytoscape: filename=dash_cytoscape-1.0.2-py3-none-any.whl size=4010717 sha256=ca4662f6e241e308b785859a208bd8c40e221d18da726b0198efd8412675963e\n","  Stored in directory: /root/.cache/pip/wheels/0c/db/f6/9dcb225e9adf45dfef713542769556b1f508170a0759053892\n","Successfully built dash-cytoscape\n","Installing collected packages: dash-table, dash-html-components, dash-core-components, zope.interface, zope.event, Werkzeug, retrying, jedi, gevent, Flask, SALib, interpret-core, dash, aplr, dash-cytoscape, interpret\n","  Attempting uninstall: Werkzeug\n","    Found existing installation: Werkzeug 3.1.3\n","    Uninstalling Werkzeug-3.1.3:\n","      Successfully uninstalled Werkzeug-3.1.3\n","  Attempting uninstall: Flask\n","    Found existing installation: Flask 3.1.2\n","    Uninstalling Flask-3.1.2:\n","      Successfully uninstalled Flask-3.1.2\n","Successfully installed Flask-3.0.3 SALib-1.5.2 Werkzeug-3.0.6 aplr-10.18.1 dash-2.18.2 dash-core-components-2.0.0 dash-cytoscape-1.0.2 dash-html-components-2.0.0 dash-table-5.0.0 gevent-25.9.1 interpret-0.7.3 interpret-core-0.7.3 jedi-0.19.2 retrying-1.4.2 zope.event-6.1 zope.interface-8.1\n","Collecting optunahub\n","  Downloading optunahub-0.4.0-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (from optunahub) (4.6.0)\n","Requirement already satisfied: GitPython in /usr/local/lib/python3.12/dist-packages (from optunahub) (3.1.45)\n","Collecting PyGithub>=1.59 (from optunahub)\n","  Downloading pygithub-2.8.1-py3-none-any.whl.metadata (3.9 kB)\n","Collecting pynacl>=1.4.0 (from PyGithub>=1.59->optunahub)\n","  Downloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (9.4 kB)\n","Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub>=1.59->optunahub) (2.32.4)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub>=1.59->optunahub) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub>=1.59->optunahub) (4.15.0)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub>=1.59->optunahub) (2.5.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from GitPython->optunahub) (4.0.12)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->optunahub) (1.17.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna->optunahub) (6.10.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna->optunahub) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna->optunahub) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->optunahub) (2.0.44)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna->optunahub) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna->optunahub) (6.0.3)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->optunahub) (1.3.10)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->GitPython->optunahub) (5.0.2)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub>=1.59->optunahub) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from pynacl>=1.4.0->PyGithub>=1.59->optunahub) (2.0.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub>=1.59->optunahub) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub>=1.59->optunahub) (3.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub>=1.59->optunahub) (2025.10.5)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->optunahub) (3.2.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub>=1.59->optunahub) (2.23)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna->optunahub) (3.0.3)\n","Downloading optunahub-0.4.0-py3-none-any.whl (12 kB)\n","Downloading pygithub-2.8.1-py3-none-any.whl (432 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.7/432.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pynacl, PyGithub, optunahub\n","Successfully installed PyGithub-2.8.1 optunahub-0.4.0 pynacl-1.6.0\n","Collecting cmaes\n","  Downloading cmaes-0.12.0-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from cmaes) (2.0.2)\n","Downloading cmaes-0.12.0-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: cmaes\n","Successfully installed cmaes-0.12.0\n","Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n","Collecting kaleido\n","  Downloading kaleido-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n","Collecting choreographer>=1.1.1 (from kaleido)\n","  Downloading choreographer-1.2.1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting logistro>=1.0.8 (from kaleido)\n","  Downloading logistro-2.0.1-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from kaleido) (3.11.4)\n","Collecting pytest-timeout>=2.4.0 (from kaleido)\n","  Downloading pytest_timeout-2.4.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.12/dist-packages (from choreographer>=1.1.1->kaleido) (3.20.2)\n","Requirement already satisfied: pytest>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from pytest-timeout>=2.4.0->kaleido) (8.4.2)\n","Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.3.0)\n","Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.6.0)\n","Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.19.2)\n","Downloading kaleido-1.2.0-py3-none-any.whl (68 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading choreographer-1.2.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading logistro-2.0.1-py3-none-any.whl (8.6 kB)\n","Downloading pytest_timeout-2.4.0-py3-none-any.whl (14 kB)\n","Installing collected packages: logistro, pytest-timeout, choreographer, kaleido\n","Successfully installed choreographer-1.2.1 kaleido-1.2.0 logistro-2.0.1 pytest-timeout-2.4.0\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: kaleido in /usr/local/lib/python3.12/dist-packages (1.2.0)\n","Requirement already satisfied: choreographer>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from kaleido) (1.2.1)\n","Requirement already satisfied: logistro>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from kaleido) (2.0.1)\n","Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from kaleido) (3.11.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kaleido) (25.0)\n","Requirement already satisfied: pytest-timeout>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from kaleido) (2.4.0)\n","Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.12/dist-packages (from choreographer>=1.1.1->kaleido) (3.20.2)\n","Requirement already satisfied: pytest>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from pytest-timeout>=2.4.0->kaleido) (8.4.2)\n","Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.3.0)\n","Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.6.0)\n","Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.19.2)\n","Collecting properscoring\n","  Downloading properscoring-0.1-py2.py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from properscoring) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from properscoring) (1.16.3)\n","Downloading properscoring-0.1-py2.py3-none-any.whl (23 kB)\n","Installing collected packages: properscoring\n","Successfully installed properscoring-0.1\n","Collecting XlsxWriter\n","  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n","Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: XlsxWriter\n","Successfully installed XlsxWriter-3.2.9\n","Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (3.0.12)\n","Collecting pgbm\n","  Downloading pgbm-2.2.0.tar.gz (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from pgbm) (1.7.2)\n","Collecting ninja>=1.10.2.2 (from pgbm)\n","  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n","Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.12/dist-packages (from pgbm) (0.60.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.56->pgbm) (0.43.0)\n","Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.12/dist-packages (from numba>=0.56->pgbm) (2.0.2)\n","Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.2->pgbm) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.2->pgbm) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.2->pgbm) (3.6.0)\n","Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pgbm\n","  Building wheel for pgbm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pgbm: filename=pgbm-2.2.0-cp312-cp312-linux_x86_64.whl size=5358050 sha256=f2ed2aa5ef2cf546c4dbd042f314e408d6374431b93f175ef868be1726ddf83c\n","  Stored in directory: /root/.cache/pip/wheels/29/f6/5e/cfb50c9a6356c0773742d3d55c0b7bc9813376d152c206521a\n","Successfully built pgbm\n","Installing collected packages: ninja, pgbm\n","Successfully installed ninja-1.13.0 pgbm-2.2.0\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Collecting cp\n","  Downloading cp-2020.12.3.tar.gz (1.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: cp\n","  Building wheel for cp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cp: filename=cp-2020.12.3-py3-none-any.whl size=1420 sha256=7b2e41582a083bec35ea6eb5ec4d848a045df293a05e5395c58c684f61559bf5\n","  Stored in directory: /root/.cache/pip/wheels/ce/63/fc/1a421c2c2a8fd4cec74f2b4077b1db348c8f154556b751d2b0\n","Successfully built cp\n","Installing collected packages: cp\n","Successfully installed cp-2020.12.3\n","Collecting mapie\n","  Downloading mapie-1.1.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: scikit-learn>=1.4 in /usr/local/lib/python3.12/dist-packages (from mapie) (1.7.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from mapie) (1.16.3)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from mapie) (2.0.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4->mapie) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4->mapie) (3.6.0)\n","Downloading mapie-1.1.0-py3-none-any.whl (187 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.1/187.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: mapie\n","Successfully installed mapie-1.1.0\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Collecting skorch\n","  Downloading skorch-1.2.0-py3-none-any.whl.metadata (11 kB)\n","Collecting puncc\n","  Downloading puncc-0.8.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from skorch) (2.0.2)\n","Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (1.7.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (1.16.3)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch) (0.9.0)\n","Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from skorch) (4.67.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from puncc) (1.5.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from puncc) (3.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from puncc) (2.2.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->puncc) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->puncc) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->puncc) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->puncc) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->puncc) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->puncc) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->puncc) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->puncc) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->puncc) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->puncc) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->puncc) (1.17.0)\n","Downloading skorch-1.2.0-py3-none-any.whl (263 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.1/263.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading puncc-0.8.0-py3-none-any.whl (70 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.8/70.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: skorch, puncc\n","Successfully installed puncc-0.8.0 skorch-1.2.0\n","Found existing installation: scikit-learn 1.7.2\n","Uninstalling scikit-learn-1.7.2:\n","  Successfully uninstalled scikit-learn-1.7.2\n","Collecting scikit-learn==1.6.1\n","  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (3.6.0)\n","Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-learn\n","Successfully installed scikit-learn-1.6.1\n","Collecting numpy==1.26.4\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","salib 1.5.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"7db2b30ea0bf4226829c520fbb07412a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n","Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.8\n","Collecting pytorch-tabnet2\n","  Downloading pytorch_tabnet2-4.5.3-py3-none-any.whl.metadata (5.5 kB)\n","Collecting activations-plus>=0.1.1 (from pytorch-tabnet2)\n","  Downloading activations_plus-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet2) (1.26.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet2) (1.6.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet2) (2.8.0+cu126)\n","Collecting torcheval (from pytorch-tabnet2)\n","  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch-tabnet2) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch-tabnet2) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch-tabnet2) (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-tabnet2) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->pytorch-tabnet2) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->pytorch-tabnet2) (3.0.3)\n","Downloading pytorch_tabnet2-4.5.3-py3-none-any.whl (70 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading activations_plus-0.1.1-py3-none-any.whl (15 kB)\n","Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torcheval, activations-plus, pytorch-tabnet2\n","Successfully installed activations-plus-0.1.1 pytorch-tabnet2-4.5.3 torcheval-0.0.7\n"]}],"source":["# Uninstall existing scikit-learn to avoid conflicts\n","!pip uninstall -y scikit-learn\n","# Install specific versions of libraries to avoid conflicts\n","!pip install scikit-learn==1.5.2\n","!pip install bayesian-optimization\n","!pip install optuna\n","!pip install gpboost==1.6.1\n","!pip install shap\n","!pip install ngboost\n","!pip install dask[dataframe]\n","!pip install torch seaborn\n","!pip install lightgbm\n","!pip install xgboost\n","!pip install lime\n","!pip install interpret\n","!pip install optunahub\n","!pip install cmaes\n","!pip install plotly kaleido\n","!pip install openpyxl\n","!pip install -U kaleido\n","!pip install properscoring\n","!pip install XlsxWriter\n","!pip install cython\n","!pip install pgbm\n","!pip install torch\n","!pip install cp\n","!pip install mapie\n","!pip install torch skorch puncc\n","# Reinstall scikit-learn to the version required by ngboost\n","!pip uninstall -y scikit-learn\n","!pip install scikit-learn==1.6.1\n","# Reinstall numpy first\n","!pip install numpy==1.26.4  # Use the version compatible with catboost\n","# Reinstall catboost\n","!pip install catboost\n","!pip install pytorch-tabnet2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QL-bkHqPwKM"},"outputs":[],"source":["# Restart the runtime to apply changes\n","import os\n","os._exit(00)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":59928,"status":"ok","timestamp":1762764628450,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"},"user_tz":-330},"id":"0kBPL1V9O2sA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"20de5c13-4d2c-4109-9e07-d61e215cdbac"},"outputs":[{"output_type":"stream","name":"stderr","text":["W1110 08:50:01.720000 3183 torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import ngboost\n","from scipy.stats import randint\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.svm import SVR\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from catboost import CatBoostRegressor\n","from gpboost import GPBoostRegressor\n","from ngboost import NGBRegressor\n","import optuna\n","import optunahub\n","from sklearn.experimental import enable_halving_search_cv\n","from sklearn.model_selection import HalvingGridSearchCV\n","from interpret import show\n","from interpret.blackbox import LimeTabular, ShapKernel\n","from optuna.samplers import RandomSampler\n","import random\n","import time\n","from ngboost.distns import Normal\n","from ngboost.scores import LogScore\n","from scipy.stats import norm\n","from interpret import set_visualize_provider\n","from interpret.provider import InlineProvider\n","from interpret.glassbox import ExplainableBoostingRegressor\n","from interpret import show\n","import plotly.express as px\n","from io import BytesIO\n","from openpyxl import Workbook, load_workbook\n","import os\n","from openpyxl.drawing.image import Image as openpyxlImage\n","import warnings\n","import xlsxwriter\n","from openpyxl.drawing.image import Image\n","from pgbm.sklearn import HistGradientBoostingRegressor\n","import torch\n","from pgbm.torch import PGBM\n","import plotly.graph_objects as go\n","warnings.filterwarnings('ignore')\n","import pickle\n","import json\n","from pytorch_tabnet import TabNetRegressor"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2251,"status":"ok","timestamp":1762764630712,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"},"user_tz":-330},"id":"D_rzxWyr3pKj","outputId":"5413d047-d9aa-456b-c6e5-62968ba6e47e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training data loaded successfully.\n","Test data loaded successfully.\n"]}],"source":["train_data_path = \"./drive/MyDrive/streamflow/TDS/train.csv\"\n","test_data_path = \"./drive/MyDrive/streamflow/TDS/test.csv\"\n","train_data = pd.read_csv(train_data_path)\n","test_data = pd.read_csv(test_data_path)\n","print(\"Training data loaded successfully.\")\n","print(\"Test data loaded successfully.\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1762764630725,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"},"user_tz":-330},"id":"sjMvmRLkPB0x","outputId":"46f002b0-2bf3-45b0-ad9f-bcc43fda44f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Shape of training data: (192, 5)\n","First 5 rows of training data:\n","     pH  Salinity  Turbidity   Water Temperature  TDS\n","0  7.6       0.2         3.0               19.9  290\n","1  7.4       0.1         0.5               20.7  120\n","2  7.3       0.2        10.7               23.6  200\n","3  8.0       0.1         3.0               27.0   94\n","4  7.8       0.4         1.4               28.4  330\n","\n","Shape of test data: (96, 5)\n","First 5 rows of test data:\n","     pH  Salinity  Turbidity   Water Temperature  TDS\n","0  7.2       0.1         9.1               18.8  160\n","1  7.4       0.2         3.7               26.3  190\n","2  7.0       0.1         3.0               21.6  180\n","3  8.1       0.1         2.2               17.2  110\n","4  7.9       0.1        14.1               27.0  200\n"]}],"source":["print(\"\\nShape of training data:\", train_data.shape)\n","print(\"First 5 rows of training data:\\n\", train_data.head(5))\n","print(\"\\nShape of test data:\", test_data.shape)\n","print(\"First 5 rows of test data:\\n\", test_data.head(5))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1762764630741,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"},"user_tz":-330},"id":"1YOqfRGLPLjK","outputId":"ca1af7b2-9ca7-48db-870d-029b052e28f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Shape of X_train: (192, 4)\n","Shape of y_train: (192,)\n","Shape of X_test: (96, 4)\n","Shape of y_test: (96,)\n"]}],"source":["X_train = train_data.iloc[:, :-1]\n","y_train = train_data.iloc[:, -1]\n","X_test = test_data.iloc[:, :-1]\n","y_test = test_data.iloc[:, -1]\n","x_test= X_test\n","print(\"\\nShape of X_train:\", X_train.shape)\n","print(\"Shape of y_train:\", y_train.shape)\n","print(\"Shape of X_test:\", X_test.shape)\n","print(\"Shape of y_test:\", y_test.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1762764630767,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"},"user_tz":-330},"id":"8Uw5ISznPMaa","outputId":"3c3f1cc8-d35e-410d-ffcc-d247e82060d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","First five rows of normalized X_train:\n","[[-0.1272241   0.06262254 -0.51513341 -1.14481478]\n"," [-0.57957647 -0.38434874 -0.6817983  -0.97088446]\n"," [-0.80575265  0.06262254 -0.00180554 -0.34038706]\n"," [ 0.77748062 -0.38434874 -0.51513341  0.39881678]\n"," [ 0.32512826  0.95656508 -0.62179894  0.70319483]]\n","\n","First five rows of normalized X_test:\n","[[-1.03192883 -0.38434874 -0.10847107 -1.38396896]\n"," [-0.57957647  0.06262254 -0.46846724  0.24662775]\n"," [-1.48428119 -0.38434874 -0.51513341 -0.77521285]\n"," [ 1.00365681 -0.38434874 -0.56846617 -1.73182959]\n"," [ 0.55130444 -0.38434874  0.22485872  0.39881678]]\n"]}],"source":["# Apply z-score normalization\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Print the first five rows of the normalized data\n","print(\"\\nFirst five rows of normalized X_train:\")\n","print(X_train[:5])\n","\n","print(\"\\nFirst five rows of normalized X_test:\")\n","print(X_test[:5])"]},{"cell_type":"markdown","source":["# **Functions**"],"metadata":{"id":"_m2a0KOG9ujJ"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"PHT6BWytPScM","executionInfo":{"status":"ok","timestamp":1762764630803,"user_tz":-330,"elapsed":39,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"}}},"outputs":[],"source":["# Define the model classes\n","model_classes = {\n","    'Random Forest': RandomForestRegressor,\n","    'Gradient Boosting': GradientBoostingRegressor,\n","    'XGBoost': XGBRegressor,\n","    'LightGBM': LGBMRegressor,\n","    'GPBoost': GPBoostRegressor,\n","    'CatBoost': CatBoostRegressor,\n","    'NGBoost': NGBRegressor,\n","    'TabNet' : TabNetRegressor\n","}\n","\n","feature_names = ['ph', 'Salinity', 'Turbidity', 'Water Temperature']\n"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from openpyxl import load_workbook\n","from openpyxl.drawing.image import Image\n","\n","def plot_best_scores(best_scores_ran, excel_file_path):\n","    # Find best scores for each model (by pruner)\n","    best_rmse_scores = {}\n","    best_corr_coef_scores = {}\n","\n","    for (model_name, pruner_name), scores in best_scores_ran.items():\n","        # Defensive: skip if score info missing\n","        if \"test_rmse\" not in scores or \"test_corr_coef\" not in scores:\n","            print(f\"Skipping {model_name} ({pruner_name}): missing required test metrics.\")\n","            continue\n","\n","        # Best RMSE (minimize)\n","        if model_name not in best_rmse_scores or scores['test_rmse'] < best_rmse_scores[model_name][0]:\n","            best_rmse_scores[model_name] = (scores['test_rmse'], pruner_name)\n","        # Best Correlation (maximize)\n","        if model_name not in best_corr_coef_scores or scores['test_corr_coef'] > best_corr_coef_scores[model_name][0]:\n","            best_corr_coef_scores[model_name] = (scores['test_corr_coef'], pruner_name)\n","\n","    # -- Defensive checks --\n","    if not best_rmse_scores or not best_corr_coef_scores:\n","        print(\"No models found with valid scores, exiting function.\")\n","        return\n","\n","    # For RMSE plot\n","    model_names_rmse = [f\"{m} ({p})\" for m, (rmse, p) in best_rmse_scores.items()]\n","    rmse_values = [rmse for rmse, _ in best_rmse_scores.values()]\n","    # For Corr plot\n","    model_names_corr = [f\"{m} ({p})\" for m, (corr, p) in best_corr_coef_scores.items()]\n","    corr_values = [corr for corr, _ in best_corr_coef_scores.values()]\n","\n","    generated_images = []\n","\n","    # ----- RMSE Plot -----\n","    if model_names_rmse and rmse_values:\n","        fig, ax = plt.subplots(figsize=(12, 6))\n","        bars = ax.bar(model_names_rmse, rmse_values, color='skyblue')\n","        best_idx = int(np.argmin(rmse_values))\n","        bars[best_idx].set_color('orange')\n","        for i, bar in enumerate(bars):\n","            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 0.05,\n","                    f'{rmse_values[i]:.5f}', ha='center', va='bottom', color='black')\n","        plt.xlabel('Model (Pruner)')\n","        plt.ylabel('Test RMSE')\n","        plt.title('Best Test RMSE for Each Model')\n","        plt.xticks(rotation=45, ha='right')\n","        plt.tight_layout()\n","        rmse_image_path = 'rmse_plot.png'\n","        plt.savefig(rmse_image_path)\n","        plt.close()\n","        generated_images.append(('RMSE', rmse_image_path))\n","\n","    # ----- Corr Plot -----\n","    if model_names_corr and corr_values:\n","        fig, ax = plt.subplots(figsize=(12, 6))\n","        bars = ax.bar(model_names_corr, corr_values, color='lightgreen')\n","        best_idx = int(np.argmax(corr_values))\n","        bars[best_idx].set_color('orange')\n","        for i, bar in enumerate(bars):\n","            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 0.05,\n","                    f'{corr_values[i]:.5f}', ha='center', va='bottom', color='black')\n","        plt.xlabel('Model (Pruner)')\n","        plt.ylabel('Correlation Coefficient')\n","        plt.title('Best Correlation Coefficient for Each Model')\n","        plt.xticks(rotation=45, ha='right')\n","        plt.tight_layout()\n","        corr_image_path = 'corr_plot.png'\n","        plt.savefig(corr_image_path)\n","        plt.close()\n","        generated_images.append(('CORR', corr_image_path))\n","\n","    # ---- Insert into Excel (only actually created images) ----\n","    workbook = load_workbook(excel_file_path)\n","    sheet_name = 'Best Models Plots'\n","    sheet = workbook[sheet_name] if sheet_name in workbook.sheetnames else workbook.create_sheet(title=sheet_name)\n","    start_rows = {'RMSE': 1, 'CORR': 20}\n","\n","    for desc, img_path in generated_images:\n","        if os.path.exists(img_path):\n","            img = Image(img_path)\n","            pos = f\"A{start_rows[desc]}\"\n","            sheet.add_image(img, pos)\n","        else:\n","            print(f\"Warning: Image file {img_path} not found; skipping insertion.\")\n","\n","    workbook.save(excel_file_path)\n","\n","    # Clean up any generated images\n","    for _, img_path in generated_images:\n","        if os.path.exists(img_path):\n","            os.remove(img_path)"],"metadata":{"id":"ZxS4SwQrOIhm","executionInfo":{"status":"ok","timestamp":1762764630827,"user_tz":-330,"elapsed":59,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"v9rnFhjC3pKl","executionInfo":{"status":"ok","timestamp":1762764630842,"user_tz":-330,"elapsed":11,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from openpyxl import load_workbook\n","from openpyxl.drawing.image import Image\n","\n","def generate_interpretml_explanations_summary_pruners(\n","    results_dict, X_train, y_train, X_test, feature_names, instance_indices=None, excel_file_path=None\n","):\n","    if instance_indices is None:\n","        instance_indices = range(len(X_test))\n","    elif isinstance(instance_indices, int):\n","        instance_indices = [instance_indices]\n","\n","    valid_indices = [idx for idx in instance_indices if 0 <= idx < len(X_test)]\n","    if not valid_indices:\n","        print(\"No valid instance indices provided.\")\n","        return\n","\n","    if isinstance(X_test, pd.DataFrame):\n","        instances_to_explain = X_test.iloc[valid_indices]\n","    else:\n","        instances_to_explain = X_test[valid_indices]\n","\n","    best_model_pruners = {}\n","    for model_key, model_info in results_dict.items():\n","        if isinstance(model_key, tuple):\n","            model_name, pruner_name = model_key\n","        else:\n","            model_name = model_key\n","            pruner_name = None\n","\n","        best_score = model_info.get('best_score')\n","        if best_score is None:\n","            print(f\"No 'best_score' found for {model_key}. Skipping this combination.\")\n","            continue\n","\n","        if model_name not in best_model_pruners:\n","            best_model_pruners[model_name] = {\n","                'pruner_name': pruner_name,\n","                'model_info': model_info,\n","                'best_score': best_score\n","            }\n","        else:\n","            current_best_score = best_model_pruners[model_name]['best_score']\n","            if best_score < current_best_score:\n","                best_model_pruners[model_name] = {\n","                    'pruner_name': pruner_name,\n","                    'model_info': model_info,\n","                    'best_score': best_score\n","                }\n","\n","    for model_name, info in best_model_pruners.items():\n","        pruner_name = info['pruner_name']\n","        model_info = info['model_info']\n","        best_params = dict(model_info['best_params'])  # don't mutate original!\n","        model_class = model_classes.get(model_name)\n","\n","        if model_class is None:\n","            print(f\"Model {model_name} is not supported or not available.\")\n","            continue\n","\n","        if model_name == 'CatBoost':\n","            best_params['verbose'] = 0\n","\n","        # ------- Main model fit logic ---------\n","        if model_name == \"TabNet\":\n","            # TabNet: reshape y, fit, flatten pred for LIME/SHAP, etc.\n","            y_train_tabnet = np.array(y_train).reshape(-1, 1)\n","            try:\n","                model = model_class(**{k: v for k, v in best_params.items() if k != \"verbose\"})\n","            except TypeError:\n","                model = model_class()\n","            model.fit(np.array(X_train), y_train_tabnet, max_epochs=100, patience=10, batch_size=1024, eval_set=[(np.array(X_train), y_train_tabnet)])\n","            def predict_fn(data):\n","                preds = model.predict(np.array(data))\n","                # flatten for interpreters\n","                return preds.flatten()\n","        else:\n","            try:\n","                model = model_class(**best_params)\n","            except TypeError:\n","                model = model_class()\n","            model.fit(X_train, y_train)\n","            def predict_fn(data):\n","                return model.predict(data)\n","\n","        if isinstance(X_train, pd.DataFrame):\n","            data_for_explainer = X_train.values\n","        else:\n","            data_for_explainer = X_train\n","\n","        if isinstance(instances_to_explain, pd.DataFrame):\n","            data_for_explanation = instances_to_explain.values\n","        else:\n","            data_for_explanation = instances_to_explain\n","\n","        # Generate LIME explanations\n","        lime_explainer = LimeTabular(\n","            predict_fn,\n","            data=data_for_explainer,\n","            feature_names=feature_names,\n","            random_state=1,\n","            mode='regression'\n","        )\n","        lime_explanation = lime_explainer.explain_local(data_for_explanation)\n","\n","        feature_importances_lime = {}\n","        num_instances = len(valid_indices)\n","        for idx in range(num_instances):\n","            explanation = lime_explanation.data(idx)\n","            for feature_name, feature_score in zip(explanation['names'], explanation['scores']):\n","                feature_importances_lime[feature_name] = feature_importances_lime.get(feature_name, 0) + abs(feature_score)\n","        feature_importances_lime = {k: v / num_instances for k, v in feature_importances_lime.items()}\n","        feature_importances_lime = {k: round(v, 3) for k, v in feature_importances_lime.items()}\n","\n","        # Generate SHAP explanations using ShapKernel\n","        try:\n","            shap_explainer = ShapKernel(predict_fn, data_for_explainer, feature_names=feature_names)\n","            shap_explanation = shap_explainer.explain_local(data_for_explanation)\n","\n","            feature_importances_shap = {}\n","            for idx in range(num_instances):\n","                explanation = shap_explanation.data(idx)\n","                for feature_name, feature_score in zip(explanation['names'], explanation['scores']):\n","                    feature_importances_shap[feature_name] = feature_importances_shap.get(feature_name, 0) + abs(feature_score)\n","\n","            feature_importances_shap = {k: v / num_instances for k, v in feature_importances_shap.items()}\n","            feature_importances_shap = {k: round(v, 3) for k, v in feature_importances_shap.items()}\n","        except Exception as e:\n","            print(f\"Could not compute SHAP values for model {model_name}: {e}\")\n","            feature_importances_shap = {}\n","\n","        # Plot LIME and SHAP feature importances side by side\n","        fig, axes = plt.subplots(1, 2, figsize=(34, 36))\n","\n","        # Plot LIME feature importances\n","        lime_importances_df = pd.DataFrame.from_dict(\n","            feature_importances_lime, orient='index', columns=['importance']\n","        )\n","        lime_importances_df.sort_values(by='importance', ascending=False, inplace=True)\n","        lime_importances_df.plot(kind='bar', legend=False, color='skyblue', ax=axes[0])\n","        axes[0].set_title(f\"LIME Feature Importances for {model_name}\")\n","        axes[0].set_ylabel(\"Average Absolute Importance Score\")\n","        axes[0].set_xlabel(\"Features\")\n","        axes[0].tick_params(axis='x', rotation=45)\n","\n","        for p in axes[0].patches:\n","            height = p.get_height()\n","            axes[0].annotate(f'{height:.3f}',\n","                             (p.get_x() + p.get_width() / 2., height),\n","                             ha='center', va='bottom', fontsize=8)\n","\n","        # Plot SHAP feature importances\n","        shap_importances_df = pd.DataFrame.from_dict(\n","            feature_importances_shap, orient='index', columns=['importance']\n","        )\n","        shap_importances_df.sort_values(by='importance', ascending=False, inplace=True)\n","        shap_importances_df.plot(kind='bar', legend=False, color='orange', ax=axes[1])\n","        axes[1].set_title(f\"SHAP Feature Importances for {model_name}\")\n","        axes[1].set_ylabel(\"Average Absolute SHAP Value\")\n","        axes[1].set_xlabel(\"Features\")\n","        axes[1].tick_params(axis='x', rotation=45)\n","\n","        for p in axes[1].patches:\n","            height = p.get_height()\n","            axes[1].annotate(f'{height:.3f}',\n","                             (p.get_x() + p.get_width() / 2., height),\n","                             ha='center', va='bottom', fontsize=8)\n","\n","        plt.tight_layout()\n","\n","        # Save plots as images\n","        image_path = f'feature_importances_{model_name}.png'\n","        fig.savefig(image_path)\n","        plt.close(fig)\n","\n","        # Optionally insert images and scores into an Excel file\n","        if excel_file_path:\n","            workbook = load_workbook(excel_file_path)\n","            sheet_name = f'{model_name} Explanations'\n","            if sheet_name in workbook.sheetnames:\n","                sheet = workbook[sheet_name]\n","            else:\n","                sheet = workbook.create_sheet(title=sheet_name)\n","\n","            # Insert images into the new Excel sheet\n","            img = Image(image_path)\n","            sheet.add_image(img, 'A1')\n","\n","            # Create a new sheet for feature importance scores\n","            scores_sheet_name = f'{model_name} Scores'\n","            if scores_sheet_name in workbook.sheetnames:\n","                scores_sheet = workbook[scores_sheet_name]\n","            else:\n","                scores_sheet = workbook.create_sheet(title=scores_sheet_name)\n","\n","            # Write LIME scores\n","            scores_sheet.append(['Feature', 'LIME Importance'])\n","            for feature, importance in feature_importances_lime.items():\n","                scores_sheet.append([feature, importance])\n","\n","            # Write SHAP scores if available\n","            if feature_importances_shap:\n","                scores_sheet.append(['Feature', 'SHAP Importance'])\n","                for feature, importance in feature_importances_shap.items():\n","                    scores_sheet.append([feature, importance])\n","\n","            # Save the workbook\n","            workbook.save(excel_file_path)\n","\n","            # Clean up the image file\n","            os.remove(image_path)"]},{"cell_type":"markdown","metadata":{"id":"Z_-QSaTL3pKm"},"source":["# **Autosampler by Optuna**"]},{"cell_type":"code","source":["# Suppress Optuna's verbose output\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","\n","# Define objective and metric functions for PGBM\n","def mseloss_objective(yhat, y, sample_weight=None):\n","    if not torch.is_tensor(yhat):\n","        yhat = torch.from_numpy(np.array(yhat)).float()\n","    if not torch.is_tensor(y):\n","        y = torch.from_numpy(np.array(y)).float()\n","    gradient = yhat - y\n","    hessian = torch.ones_like(yhat)\n","    return gradient, hessian\n","\n","def rmseloss_metric(yhat, y, sample_weight=None):\n","    if not torch.is_tensor(yhat):\n","        yhat = torch.from_numpy(np.array(yhat)).float()\n","    if not torch.is_tensor(y):\n","        y = torch.from_numpy(np.array(y)).float()\n","    loss = torch.sqrt(torch.mean((yhat - y) ** 2))\n","    return loss\n","\n","def hyperparameter_tuning_all(X_train, y_train, X_test, y_test, excel_path):\n","    X_train = np.array(X_train)\n","    y_train = np.array(y_train)\n","    X_test = np.array(X_test)\n","    y_test = np.array(y_test)\n","\n","    models = {\n","        'Random Forest': (RandomForestRegressor, {\n","            'n_estimators': [100, 200, 300, 500, 700],\n","            'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n","            'max_depth': [None, 10, 20, 30, 40],\n","            'min_samples_split': [2, 5, 10, 0.01],\n","            'min_samples_leaf': [1, 3, 5, 0.01],\n","            'min_weight_fraction_leaf': [0.0, 0.01, 0.1, 0.2],\n","            'max_features': [1.0, 'sqrt', 'log2', 0.3, 0.5],\n","            'max_leaf_nodes': [None, 50, 100, 200],\n","            'min_impurity_decrease': [0.0, 0.01, 0.1, 0.2],\n","            'n_jobs': [-1],\n","            'random_state': [42],\n","            'verbose': [0],\n","            'warm_start': [False],\n","            'ccp_alpha': [0.0, 0.001, 0.01, 0.05, 0.1]\n","        }),\n","        'Gradient Boosting': (GradientBoostingRegressor, {\n","            'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n","            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n","            'n_estimators': [100, 200, 300, 500, 700],\n","            'subsample': [1.0, 0.9, 0.7, 0.5],\n","            'criterion': ['friedman_mse', 'squared_error'],\n","            'min_samples_split': [2, 5, 10, 0.01],\n","            'min_samples_leaf': [1, 3, 5, 0.01],\n","            'min_weight_fraction_leaf': [0.0, 0.01, 0.05, 0.1],\n","            'max_depth': [3, 5, 7, 10],\n","            'min_impurity_decrease': [0.0, 0.01, 0.1],\n","            'init': [None],\n","            'random_state': [42],\n","            'max_features': [None, 'sqrt', 'log2', 0.5],\n","            'alpha': [0.9, 0.5, 0.1],\n","            'verbose': [0],\n","            'max_leaf_nodes': [None, 10, 30, 50],\n","            'warm_start': [False],\n","            'validation_fraction': [0.1],\n","            'n_iter_no_change': [None, 10, 20],\n","            'tol': [1e-4, 1e-3],\n","            'ccp_alpha': [0.0, 0.001, 0.01]\n","        }),\n","        'XGBoost': (XGBRegressor, {\n","            'n_estimators': [100, 200, 300, 400, 500],\n","            'learning_rate': [0.01, 0.05, 0.1, 0.15],\n","            'max_depth': [3, 5, 7],\n","            'min_child_weight': [1, 3, 5],\n","            'gamma': [0, 0.1, 0.5, 1],\n","            'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n","            'colsample_bytree': [0.5, 0.7, 0.9],\n","            'colsample_bylevel': [0.5, 0.7, 0.9],\n","            'reg_alpha': [0, 0.01, 0.1, 1],\n","            'reg_lambda': [0.1, 1, 5, 10],\n","            'objective': ['reg:squarederror'],\n","            'random_state': [42],\n","            'n_jobs': [-1]\n","        }),\n","        'LightGBM': (LGBMRegressor, {\n","            'n_estimators': [100, 200, 300, 400, 500],\n","            'learning_rate': [0.01, 0.05, 0.1, 0.15],\n","            'num_leaves': [15, 31, 63],\n","            'max_depth': [3, 5, 7, -1],\n","            'min_child_samples': [1, 5, 10, 20],\n","            'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n","            'colsample_bytree': [0.5, 0.7, 0.9, 1.0],\n","            'reg_alpha': [0, 0.01, 0.1, 1],\n","            'reg_lambda': [0, 0.1, 1, 10],\n","            'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1],\n","            'bagging_freq': [0, 1, 5],\n","            'objective': ['regression'],\n","            'random_state': [42],\n","            'n_jobs': [-1],\n","            'verbose': [-1]\n","        }),\n","        'GPBoost': (GPBoostRegressor, {\n","            'n_estimators': [100, 200, 300, 400, 500],\n","            'learning_rate': [0.01, 0.05, 0.1, 0.15],\n","            'max_depth': [3, 5, 7, -1],\n","            'num_leaves': [15, 31, 63],\n","            'min_child_samples': [1, 5, 10, 20],\n","            'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n","            'colsample_bytree': [0.5, 0.7, 0.9, 1.0],\n","            'reg_alpha': [0, 0.1, 0.5, 1.0],\n","            'reg_lambda': [0, 0.1, 0.5, 1.0],\n","            'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1],\n","            'random_state': [42],\n","            'n_jobs': [-1],\n","            'verbose': [-1]\n","        }),\n","        'CatBoost': (CatBoostRegressor, {\n","            'iterations': [200, 500, 1000],\n","            'learning_rate': [0.01, 0.03, 0.05, 0.1],\n","            'depth': [4, 6, 8, 10],\n","            'l2_leaf_reg': [1, 3, 5, 7, 9],\n","            'border_count': [32, 64, 128],\n","            'min_data_in_leaf': [1, 5, 10, 20],\n","            'rsm': [0.6, 0.8, 1.0],\n","            'bagging_temperature': [0, 1, 10],\n","            'random_seed': [42],\n","            'verbose': [0]\n","        }),\n","        'NGBoost': (NGBRegressor, {\n","            'n_estimators': [200, 500, 1000],\n","            'learning_rate': [0.01, 0.03, 0.05, 0.1],\n","            'natural_gradient': [True, False],\n","            'minibatch_frac': [0.5, 0.7, 0.9, 1.0],\n","            'col_sample': [0.5, 0.7, 0.9, 1.0],\n","            'Dist': [Normal],\n","            'Score': [LogScore],\n","            'random_state': [42],\n","            'verbose': [0]\n","        }),\n","        'HistGradientBoosting': (HistGradientBoostingRegressor, {\n","            'learning_rate': [0.01, 0.05, 0.1, 0.15],\n","            'max_iter': [100, 200, 300, 400, 500],\n","            'max_depth': [3, 5, 7, None],\n","            'min_samples_leaf': [5, 10, 20],\n","            'max_leaf_nodes': [15, 31, 63, None],\n","            'l2_regularization': [0.0, 0.1, 0.5, 1.0],\n","            'max_bins': [64, 128, 255],\n","            'early_stopping': [True, False],\n","            'validation_fraction': [0.1, 0.2],\n","            'n_iter_no_change': [5, 10, 15],\n","            'loss': ['squared_error'],\n","            'random_state': [42],\n","            'verbose': [0]\n","        }),\n","        'PGBM': (PGBM, {}), # PGBM is handled separately\n","        \"TabNet\": (TabNetRegressor, {\n","            'n_d': [8, 16, 32, 64],\n","            'n_a': [8, 16, 32, 64],\n","            'n_steps': [3, 5, 7, 10],\n","            'gamma': [1.0, 1.3, 1.5, 2.0],\n","            'lambda_sparse': [1e-4, 1e-3, 1e-2],\n","            'optimizer_params': [{'lr': 2e-2}], # Fixed learning rate as recommended\n","            'mask_type': ['sparsemax', 'entmax'],\n","            'n_shared': [1, 2, 3],\n","            'n_independent': [1, 2, 3],\n","            'scheduler_params': [{\"step_size\": 10, \"gamma\": 0.9}],\n","            'scheduler_fn': [torch.optim.lr_scheduler.StepLR],\n","            'seed': [42],\n","            'verbose': [0]\n","        })\n","    }\n","\n","\n","    pruners = [\n","        optuna.pruners.MedianPruner(),\n","        optuna.pruners.NopPruner(),\n","        optuna.pruners.PatientPruner(optuna.pruners.MedianPruner(), patience=3),\n","        optuna.pruners.PercentilePruner(25.0),\n","        optuna.pruners.SuccessiveHalvingPruner(),\n","        optuna.pruners.HyperbandPruner(),\n","        optuna.pruners.ThresholdPruner(lower=0.1),\n","        optuna.pruners.WilcoxonPruner()\n","    ]\n","\n","    best_scores = {}\n","    predictions_df = pd.DataFrame({'Actual': y_test})\n","\n","    for model_name, (model_class, param_space) in models.items():\n","        for pruner in pruners:\n","            pruner_name = pruner.__class__.__name__\n","            print(f\"Running Optuna for {model_name} with {pruner_name}...\")\n","            try:\n","                sampler = optunahub.load_module(\"samplers/auto_sampler\").AutoSampler()\n","                study = optuna.create_study(direction='minimize', sampler=sampler, pruner=pruner)\n","\n","                if model_name == 'PGBM':\n","                    def pgbm_objective(trial):\n","                        params = {\n","                            'n_estimators': trial.suggest_categorical('n_estimators', [100, 200, 300, 500]),\n","                            'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1, 0.15]),\n","                            'max_leaves': trial.suggest_int('max_leaves', 15, 63),\n","                            'min_split_gain': trial.suggest_categorical('min_split_gain', [0.0, 0.1, 0.5, 1.0]),\n","                            'reg_lambda': trial.suggest_categorical('reg_lambda', [0.1, 1.0, 5.0, 10.0]),\n","                            'feature_fraction': trial.suggest_categorical('feature_fraction', [0.5, 0.7, 0.9, 1.0]),\n","                            'bagging_fraction': trial.suggest_categorical('bagging_fraction', [0.5, 0.7, 0.9, 1.0]),\n","                            'tree_correlation': trial.suggest_categorical('tree_correlation', [0.0, 0.1, 0.2, 0.3]),\n","                            'min_data_in_leaf': trial.suggest_categorical('min_data_in_leaf', [3, 5, 10, 20]),\n","                            'max_bin': trial.suggest_categorical('max_bin', [64, 128, 256]),\n","                            'distribution': trial.suggest_categorical('distribution', ['normal', 'studentt', 'laplace']),\n","                            'objective': 'mse',\n","                            'metric': 'rmse',\n","                            'random_state': 42,\n","                            'verbose': 0\n","                        }\n","\n","                        model = PGBM()\n","                        model.train((X_train, y_train), objective=mseloss_objective, metric=rmseloss_metric, params=params)\n","                        y_pred = model.predict(X_test)\n","                        mse = mean_squared_error(y_test, y_pred)\n","                        return mse\n","\n","                    study.optimize(pgbm_objective, n_trials=50)\n","                else:\n","                    def objective(trial, model_name, model_class, param_space):\n","                        params = {}\n","                        for key, values in param_space.items():\n","                            if isinstance(values, list):\n","                                params[key] = trial.suggest_categorical(key, values)\n","                            # Add other suggestion types if needed for other models...\n","\n","                        # Special handling for TabNet's data shape requirement\n","                        if model_name == 'TabNet':\n","                            # Reshape target variables to 2D for TabNet\n","                            y_train_fit = y_train.reshape(-1, 1)\n","                            y_test_fit = y_test.reshape(-1, 1)\n","\n","                            model = model_class(**params)\n","                            # Use the reshaped data for fitting and evaluation\n","                            model.fit(\n","                                X_train, y_train_fit,\n","                                eval_set=[(X_test, y_test_fit)],\n","                                patience=10,\n","                                max_epochs=100\n","                            )\n","                            y_pred = model.predict(X_test)\n","                            mse = mean_squared_error(y_test_fit, y_pred)\n","                        else:\n","                            # Standard fitting for all other models\n","                            model = model_class(**params)\n","                            model.fit(X_train, y_train)\n","                            y_pred = model.predict(X_test)\n","                            mse = mean_squared_error(y_test, y_pred)\n","\n","                        return mse\n","\n","                    study.optimize(lambda trial: objective(trial, model_name, model_class, param_space), n_trials=50)\n","\n","                best_params = study.best_params\n","                # Create an instance of the best model\n","                best_model = model_class(**best_params) if model_name != 'PGBM' else PGBM()\n","\n","                # Retrain the best model on the full data\n","                if model_name == 'PGBM':\n","                    best_model.train((X_train, y_train), objective=mseloss_objective, metric=rmseloss_metric, params=best_params)\n","                elif model_name == 'TabNet':\n","                    # *** THIS IS THE FIX: Reshape y_train just for TabNet's final fit ***\n","                    best_model.fit(X_train, y_train.reshape(-1, 1))\n","                else:\n","                    # All other models use the original y_train\n","                    best_model.fit(X_train, y_train)\n","\n","                y_pred = best_model.predict(X_test)\n","                # *** THIS IS THE FIX: Flatten the prediction array to 1D ***\n","                y_pred_flat = y_pred.flatten()\n","\n","                # Now use the flattened array for all subsequent operations\n","                mse = mean_squared_error(y_test, y_pred_flat)\n","                rmse = np.sqrt(mse)\n","                corr_coef = np.corrcoef(y_test, y_pred_flat)[0, 1]\n","\n","                # Save the flattened predictions to the DataFrame\n","                predictions_df[f'{model_name}_{pruner_name}_Predicted'] = y_pred_flat\n","\n","                best_scores[(model_name, pruner_name)] = {\n","                    'best_score': mse,\n","                    'best_params': best_params,\n","                    'test_mse': mse,\n","                    'test_rmse': rmse,\n","                    'test_corr_coef': corr_coef,\n","                    'pruner': pruner_name\n","                }\n","                print(f\"Best MSE for {model_name} with {pruner_name}: {mse} with params: {best_params}\")\n","                print(f\"Best RMSE for {model_name} with {pruner_name}: {rmse}\")\n","                print(f\"Correlation Coefficient for {model_name} with {pruner_name}: {corr_coef}\")\n","            except Exception as e:\n","                print(f\"Failed to run Optuna for {model_name} with {pruner_name}. Error: {e}\")\n","\n","    # Write predictions to Excel\n","    with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n","        predictions_df.to_excel(writer, sheet_name='Predictions', index=False)\n","\n","    if best_scores:\n","        best_model_name, best_pruner_name = min(best_scores, key=lambda k: best_scores[k]['test_mse'])\n","        best_model_info = best_scores[(best_model_name, best_pruner_name)]\n","        print(f\"\\nBest model on test data: {best_model_name} with {best_pruner_name}\")\n","        print(f\"Test MSE: {best_model_info['test_mse']}\")\n","        print(f\"Test RMSE: {best_model_info['test_rmse']}\")\n","        print(f\"Correlation Coefficient: {best_model_info['test_corr_coef']}\")\n","        print(f\"Best Parameters: {best_model_info['best_params']}\")\n","        print(f\"Pruner Used: {best_model_info['pruner']}\")\n","    else:\n","        print(\"No valid model configurations found.\")\n","\n","    return best_scores\n","\n","# Example usage\n","best_scores_autosampler = hyperparameter_tuning_all(X_train, y_train, X_test, y_test, \"./drive/MyDrive/streamflow/TDS/tds_ml_tabnet/HyperParameter_Tuning/test.xlsx\")"],"metadata":{"id":"cuiOvohCUDm8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca2710c1-40b5-4481-f031-ce39bf84f0f8","executionInfo":{"status":"ok","timestamp":1762771196748,"user_tz":-330,"elapsed":6565902,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Optuna for Random Forest with MedianPruner...\n","Best MSE for Random Forest with MedianPruner: 11899.272160644423 with params: {'n_estimators': 200, 'criterion': 'poisson', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 0.01, 'min_weight_fraction_leaf': 0.01, 'max_features': 'sqrt', 'max_leaf_nodes': 200, 'min_impurity_decrease': 0.2, 'n_jobs': -1, 'random_state': 42, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.001}\n","Best RMSE for Random Forest with MedianPruner: 109.08378504912828\n","Correlation Coefficient for Random Forest with MedianPruner: 0.8477575383559034\n","Running Optuna for Random Forest with NopPruner...\n","Best MSE for Random Forest with NopPruner: 12448.972212788854 with params: {'n_estimators': 300, 'criterion': 'poisson', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 0.01, 'min_weight_fraction_leaf': 0.01, 'max_features': 'sqrt', 'max_leaf_nodes': 50, 'min_impurity_decrease': 0.2, 'n_jobs': -1, 'random_state': 42, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.05}\n","Best RMSE for Random Forest with NopPruner: 111.5749623024308\n","Correlation Coefficient for Random Forest with NopPruner: 0.844618446645913\n","Running Optuna for Random Forest with PatientPruner...\n","Best MSE for Random Forest with PatientPruner: 11575.713346171628 with params: {'n_estimators': 200, 'criterion': 'poisson', 'max_depth': 10, 'min_samples_split': 0.01, 'min_samples_leaf': 0.01, 'min_weight_fraction_leaf': 0.0, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.01, 'n_jobs': -1, 'random_state': 42, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.05}\n","Best RMSE for Random Forest with PatientPruner: 107.59048910648016\n","Correlation Coefficient for Random Forest with PatientPruner: 0.8546567655496594\n","Running Optuna for Random Forest with PercentilePruner...\n","Best MSE for Random Forest with PercentilePruner: 11915.042116962484 with params: {'n_estimators': 200, 'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.0, 'max_features': 1.0, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.01, 'n_jobs': -1, 'random_state': 42, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.001}\n","Best RMSE for Random Forest with PercentilePruner: 109.15604480266992\n","Correlation Coefficient for Random Forest with PercentilePruner: 0.8400541584020661\n","Running Optuna for Random Forest with SuccessiveHalvingPruner...\n","Best MSE for Random Forest with SuccessiveHalvingPruner: 11796.492013891693 with params: {'n_estimators': 200, 'criterion': 'poisson', 'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 0.01, 'min_weight_fraction_leaf': 0.0, 'max_features': 'log2', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.01, 'n_jobs': -1, 'random_state': 42, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.001}\n","Best RMSE for Random Forest with SuccessiveHalvingPruner: 108.61165689690814\n","Correlation Coefficient for Random Forest with SuccessiveHalvingPruner: 0.8507910192885686\n","Running Optuna for Random Forest with HyperbandPruner...\n","Best MSE for Random Forest with HyperbandPruner: 11915.068920540138 with params: {'n_estimators': 200, 'criterion': 'squared_error', 'max_depth': 40, 'min_samples_split': 10, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.01, 'max_features': 1.0, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.2, 'n_jobs': -1, 'random_state': 42, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.05}\n","Best RMSE for Random Forest with HyperbandPruner: 109.15616757902477\n","Correlation Coefficient for Random Forest with HyperbandPruner: 0.8400536224944962\n","Running Optuna for Random Forest with ThresholdPruner...\n","Best MSE for Random Forest with ThresholdPruner: 11396.714504124559 with params: {'n_estimators': 200, 'criterion': 'poisson', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.0, 'max_features': 1.0, 'max_leaf_nodes': 50, 'min_impurity_decrease': 0.2, 'n_jobs': -1, 'random_state': 42, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.01}\n","Best RMSE for Random Forest with ThresholdPruner: 106.75539566750038\n","Correlation Coefficient for Random Forest with ThresholdPruner: 0.8545175710230446\n","Running Optuna for Random Forest with WilcoxonPruner...\n","Best MSE for Random Forest with WilcoxonPruner: 11899.27216064442 with params: {'n_estimators': 200, 'criterion': 'poisson', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 0.01, 'min_weight_fraction_leaf': 0.0, 'max_features': 'log2', 'max_leaf_nodes': 100, 'min_impurity_decrease': 0.2, 'n_jobs': -1, 'random_state': 42, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.0}\n","Best RMSE for Random Forest with WilcoxonPruner: 109.08378504912827\n","Correlation Coefficient for Random Forest with WilcoxonPruner: 0.8477575383559033\n","Running Optuna for Gradient Boosting with MedianPruner...\n","Best MSE for Gradient Boosting with MedianPruner: 7970.963664616332 with params: {'loss': 'squared_error', 'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 1.0, 'criterion': 'friedman_mse', 'min_samples_split': 0.01, 'min_samples_leaf': 3, 'min_weight_fraction_leaf': 0.05, 'max_depth': 7, 'min_impurity_decrease': 0.01, 'init': None, 'random_state': 42, 'max_features': None, 'alpha': 0.9, 'verbose': 0, 'max_leaf_nodes': 10, 'warm_start': False, 'validation_fraction': 0.1, 'n_iter_no_change': 10, 'tol': 0.001, 'ccp_alpha': 0.001}\n","Best RMSE for Gradient Boosting with MedianPruner: 89.28025349771545\n","Correlation Coefficient for Gradient Boosting with MedianPruner: 0.8929816720083197\n","Running Optuna for Gradient Boosting with NopPruner...\n","Best MSE for Gradient Boosting with NopPruner: 8095.507840454411 with params: {'loss': 'squared_error', 'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 1.0, 'criterion': 'friedman_mse', 'min_samples_split': 5, 'min_samples_leaf': 3, 'min_weight_fraction_leaf': 0.05, 'max_depth': 10, 'min_impurity_decrease': 0.1, 'init': None, 'random_state': 42, 'max_features': 'log2', 'alpha': 0.9, 'verbose': 0, 'max_leaf_nodes': 10, 'warm_start': False, 'validation_fraction': 0.1, 'n_iter_no_change': 20, 'tol': 0.001, 'ccp_alpha': 0.0}\n","Best RMSE for Gradient Boosting with NopPruner: 89.97504009698696\n","Correlation Coefficient for Gradient Boosting with NopPruner: 0.8972665861442432\n","Running Optuna for Gradient Boosting with PatientPruner...\n","Best MSE for Gradient Boosting with PatientPruner: 7939.9986575438825 with params: {'loss': 'squared_error', 'learning_rate': 0.05, 'n_estimators': 500, 'subsample': 0.9, 'criterion': 'squared_error', 'min_samples_split': 10, 'min_samples_leaf': 3, 'min_weight_fraction_leaf': 0.05, 'max_depth': 5, 'min_impurity_decrease': 0.1, 'init': None, 'random_state': 42, 'max_features': 'log2', 'alpha': 0.9, 'verbose': 0, 'max_leaf_nodes': 30, 'warm_start': False, 'validation_fraction': 0.1, 'n_iter_no_change': 10, 'tol': 0.001, 'ccp_alpha': 0.01}\n","Best RMSE for Gradient Boosting with PatientPruner: 89.10667010692231\n","Correlation Coefficient for Gradient Boosting with PatientPruner: 0.8990304260652475\n","Running Optuna for Gradient Boosting with PercentilePruner...\n","Best MSE for Gradient Boosting with PercentilePruner: 7895.175496459338 with params: {'loss': 'squared_error', 'learning_rate': 0.05, 'n_estimators': 700, 'subsample': 1.0, 'criterion': 'squared_error', 'min_samples_split': 0.01, 'min_samples_leaf': 0.01, 'min_weight_fraction_leaf': 0.05, 'max_depth': 3, 'min_impurity_decrease': 0.01, 'init': None, 'random_state': 42, 'max_features': None, 'alpha': 0.9, 'verbose': 0, 'max_leaf_nodes': 30, 'warm_start': False, 'validation_fraction': 0.1, 'n_iter_no_change': 10, 'tol': 0.001, 'ccp_alpha': 0.001}\n","Best RMSE for Gradient Boosting with PercentilePruner: 88.85480007551274\n","Correlation Coefficient for Gradient Boosting with PercentilePruner: 0.906381821876527\n","Running Optuna for Gradient Boosting with SuccessiveHalvingPruner...\n","Best MSE for Gradient Boosting with SuccessiveHalvingPruner: 8814.538009628499 with params: {'loss': 'squared_error', 'learning_rate': 0.05, 'n_estimators': 300, 'subsample': 0.5, 'criterion': 'squared_error', 'min_samples_split': 10, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.0, 'max_depth': 5, 'min_impurity_decrease': 0.0, 'init': None, 'random_state': 42, 'max_features': 'sqrt', 'alpha': 0.5, 'verbose': 0, 'max_leaf_nodes': None, 'warm_start': False, 'validation_fraction': 0.1, 'n_iter_no_change': 20, 'tol': 0.001, 'ccp_alpha': 0.01}\n","Best RMSE for Gradient Boosting with SuccessiveHalvingPruner: 93.8857710711719\n","Correlation Coefficient for Gradient Boosting with SuccessiveHalvingPruner: 0.8848669675879104\n","Running Optuna for Gradient Boosting with HyperbandPruner...\n","Best MSE for Gradient Boosting with HyperbandPruner: 8069.03802734413 with params: {'loss': 'squared_error', 'learning_rate': 0.1, 'n_estimators': 700, 'subsample': 0.9, 'criterion': 'squared_error', 'min_samples_split': 5, 'min_samples_leaf': 0.01, 'min_weight_fraction_leaf': 0.05, 'max_depth': 5, 'min_impurity_decrease': 0.01, 'init': None, 'random_state': 42, 'max_features': 'log2', 'alpha': 0.5, 'verbose': 0, 'max_leaf_nodes': 10, 'warm_start': False, 'validation_fraction': 0.1, 'n_iter_no_change': 10, 'tol': 0.0001, 'ccp_alpha': 0.001}\n","Best RMSE for Gradient Boosting with HyperbandPruner: 89.82782434938592\n","Correlation Coefficient for Gradient Boosting with HyperbandPruner: 0.8971614193988803\n","Running Optuna for Gradient Boosting with ThresholdPruner...\n","Best MSE for Gradient Boosting with ThresholdPruner: 8315.221560527307 with params: {'loss': 'huber', 'learning_rate': 0.05, 'n_estimators': 100, 'subsample': 1.0, 'criterion': 'squared_error', 'min_samples_split': 0.01, 'min_samples_leaf': 3, 'min_weight_fraction_leaf': 0.05, 'max_depth': 5, 'min_impurity_decrease': 0.1, 'init': None, 'random_state': 42, 'max_features': None, 'alpha': 0.9, 'verbose': 0, 'max_leaf_nodes': 10, 'warm_start': False, 'validation_fraction': 0.1, 'n_iter_no_change': 20, 'tol': 0.0001, 'ccp_alpha': 0.01}\n","Best RMSE for Gradient Boosting with ThresholdPruner: 91.18783669178312\n","Correlation Coefficient for Gradient Boosting with ThresholdPruner: 0.8872156687967951\n","Running Optuna for Gradient Boosting with WilcoxonPruner...\n","Best MSE for Gradient Boosting with WilcoxonPruner: 11155.980059064212 with params: {'loss': 'huber', 'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 0.7, 'criterion': 'squared_error', 'min_samples_split': 5, 'min_samples_leaf': 0.01, 'min_weight_fraction_leaf': 0.01, 'max_depth': 5, 'min_impurity_decrease': 0.0, 'init': None, 'random_state': 42, 'max_features': 'log2', 'alpha': 0.1, 'verbose': 0, 'max_leaf_nodes': None, 'warm_start': False, 'validation_fraction': 0.1, 'n_iter_no_change': 20, 'tol': 0.001, 'ccp_alpha': 0.0}\n","Best RMSE for Gradient Boosting with WilcoxonPruner: 105.62187301437241\n","Correlation Coefficient for Gradient Boosting with WilcoxonPruner: 0.8533098826130289\n","Running Optuna for XGBoost with MedianPruner...\n","Best MSE for XGBoost with MedianPruner: 9549.828125 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 5, 'gamma': 0.1, 'subsample': 0.5, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.7, 'reg_alpha': 0.01, 'reg_lambda': 0.1, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n","Best RMSE for XGBoost with MedianPruner: 97.72322203550188\n","Correlation Coefficient for XGBoost with MedianPruner: 0.8852101651430149\n","Running Optuna for XGBoost with NopPruner...\n","Best MSE for XGBoost with NopPruner: 10584.6494140625 with params: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 1, 'subsample': 0.5, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.7, 'reg_alpha': 0, 'reg_lambda': 5, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n","Best RMSE for XGBoost with NopPruner: 102.88172536491842\n","Correlation Coefficient for XGBoost with NopPruner: 0.8555665428449333\n","Running Optuna for XGBoost with PatientPruner...\n","Best MSE for XGBoost with PatientPruner: 9592.75 with params: {'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 5, 'gamma': 0, 'subsample': 0.5, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9, 'reg_alpha': 0.1, 'reg_lambda': 5, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n","Best RMSE for XGBoost with PatientPruner: 97.94258522215962\n","Correlation Coefficient for XGBoost with PatientPruner: 0.8744618085762377\n","Running Optuna for XGBoost with PercentilePruner...\n","Best MSE for XGBoost with PercentilePruner: 11422.75 with params: {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 1, 'subsample': 0.5, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.9, 'reg_alpha': 0.1, 'reg_lambda': 5, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n","Best RMSE for XGBoost with PercentilePruner: 106.87726605784786\n","Correlation Coefficient for XGBoost with PercentilePruner: 0.8392123989874094\n","Running Optuna for XGBoost with SuccessiveHalvingPruner...\n","Best MSE for XGBoost with SuccessiveHalvingPruner: 9704.0751953125 with params: {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 5, 'gamma': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9, 'reg_alpha': 0.1, 'reg_lambda': 5, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n","Best RMSE for XGBoost with SuccessiveHalvingPruner: 98.5092645151333\n","Correlation Coefficient for XGBoost with SuccessiveHalvingPruner: 0.8768954297682986\n","Running Optuna for XGBoost with HyperbandPruner...\n","Best MSE for XGBoost with HyperbandPruner: 10142.8193359375 with params: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'gamma': 0, 'subsample': 0.6, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.7, 'reg_alpha': 1, 'reg_lambda': 5, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n","Best RMSE for XGBoost with HyperbandPruner: 100.71156505554613\n","Correlation Coefficient for XGBoost with HyperbandPruner: 0.8631087766411594\n","Running Optuna for XGBoost with ThresholdPruner...\n","Best MSE for XGBoost with ThresholdPruner: 9715.6533203125 with params: {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 5, 'gamma': 1, 'subsample': 0.6, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.7, 'reg_alpha': 0, 'reg_lambda': 5, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n","Best RMSE for XGBoost with ThresholdPruner: 98.56801367742226\n","Correlation Coefficient for XGBoost with ThresholdPruner: 0.8766330625314297\n","Running Optuna for XGBoost with WilcoxonPruner...\n","Best MSE for XGBoost with WilcoxonPruner: 9901.001953125 with params: {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.1, 'subsample': 0.6, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9, 'reg_alpha': 0.01, 'reg_lambda': 5, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n","Best RMSE for XGBoost with WilcoxonPruner: 99.50377858717225\n","Correlation Coefficient for XGBoost with WilcoxonPruner: 0.8735680563612124\n","Running Optuna for LightGBM with MedianPruner...\n","Best MSE for LightGBM with MedianPruner: 7758.7734195111625 with params: {'n_estimators': 100, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': 7, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 0, 'reg_lambda': 10, 'min_child_weight': 1e-05, 'bagging_freq': 5, 'objective': 'regression', 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for LightGBM with MedianPruner: 88.08389988818139\n","Correlation Coefficient for LightGBM with MedianPruner: 0.8949937460822153\n","Running Optuna for LightGBM with NopPruner...\n","Best MSE for LightGBM with NopPruner: 7733.385730568155 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'num_leaves': 31, 'max_depth': 3, 'min_child_samples': 10, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 1, 'reg_lambda': 0, 'min_child_weight': 1e-05, 'bagging_freq': 1, 'objective': 'regression', 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for LightGBM with NopPruner: 87.93967097145722\n","Correlation Coefficient for LightGBM with NopPruner: 0.8950374906287867\n","Running Optuna for LightGBM with PatientPruner...\n","Best MSE for LightGBM with PatientPruner: 8340.646563999011 with params: {'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, 'max_depth': 3, 'min_child_samples': 10, 'subsample': 0.7, 'colsample_bytree': 0.9, 'reg_alpha': 1, 'reg_lambda': 10, 'min_child_weight': 0.01, 'bagging_freq': 1, 'objective': 'regression', 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for LightGBM with PatientPruner: 91.3271403472101\n","Correlation Coefficient for LightGBM with PatientPruner: 0.8855045388683417\n","Running Optuna for LightGBM with PercentilePruner...\n","Best MSE for LightGBM with PercentilePruner: 7999.619783578978 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'num_leaves': 15, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.8, 'colsample_bytree': 0.9, 'reg_alpha': 1, 'reg_lambda': 10, 'min_child_weight': 0.01, 'bagging_freq': 5, 'objective': 'regression', 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for LightGBM with PercentilePruner: 89.44059360032769\n","Correlation Coefficient for LightGBM with PercentilePruner: 0.8942225188076984\n","Running Optuna for LightGBM with SuccessiveHalvingPruner...\n","Best MSE for LightGBM with SuccessiveHalvingPruner: 8048.868195004266 with params: {'n_estimators': 100, 'learning_rate': 0.05, 'num_leaves': 15, 'max_depth': 7, 'min_child_samples': 10, 'subsample': 0.7, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 0, 'min_child_weight': 0.001, 'bagging_freq': 1, 'objective': 'regression', 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for LightGBM with SuccessiveHalvingPruner: 89.71548470026937\n","Correlation Coefficient for LightGBM with SuccessiveHalvingPruner: 0.8896934012654839\n","Running Optuna for LightGBM with HyperbandPruner...\n","Best MSE for LightGBM with HyperbandPruner: 8485.506620601334 with params: {'n_estimators': 200, 'learning_rate': 0.05, 'num_leaves': 15, 'max_depth': 3, 'min_child_samples': 10, 'subsample': 0.7, 'colsample_bytree': 0.9, 'reg_alpha': 0, 'reg_lambda': 0.1, 'min_child_weight': 0.01, 'bagging_freq': 5, 'objective': 'regression', 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for LightGBM with HyperbandPruner: 92.11680965275195\n","Correlation Coefficient for LightGBM with HyperbandPruner: 0.8844658090762936\n","Running Optuna for LightGBM with ThresholdPruner...\n","Best MSE for LightGBM with ThresholdPruner: 8070.687217418167 with params: {'n_estimators': 200, 'learning_rate': 0.01, 'num_leaves': 63, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 0.1, 'reg_lambda': 0, 'min_child_weight': 1e-05, 'bagging_freq': 5, 'objective': 'regression', 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for LightGBM with ThresholdPruner: 89.83700360885912\n","Correlation Coefficient for LightGBM with ThresholdPruner: 0.9053014098785445\n","Running Optuna for LightGBM with WilcoxonPruner...\n","Best MSE for LightGBM with WilcoxonPruner: 8113.147255773668 with params: {'n_estimators': 200, 'learning_rate': 0.01, 'num_leaves': 31, 'max_depth': 3, 'min_child_samples': 10, 'subsample': 0.8, 'colsample_bytree': 0.9, 'reg_alpha': 0, 'reg_lambda': 0, 'min_child_weight': 1e-05, 'bagging_freq': 0, 'objective': 'regression', 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for LightGBM with WilcoxonPruner: 90.07301069562219\n","Correlation Coefficient for LightGBM with WilcoxonPruner: 0.9026229998778147\n","Running Optuna for GPBoost with MedianPruner...\n","Best MSE for GPBoost with MedianPruner: 8176.113487642123 with params: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 5, 'num_leaves': 15, 'min_child_samples': 10, 'subsample': 0.6, 'colsample_bytree': 0.9, 'reg_alpha': 0.5, 'reg_lambda': 0, 'min_child_weight': 0.1, 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for GPBoost with MedianPruner: 90.42186399119475\n","Correlation Coefficient for GPBoost with MedianPruner: 0.8994771459169217\n","Running Optuna for GPBoost with NopPruner...\n","Best MSE for GPBoost with NopPruner: 7561.740338130348 with params: {'n_estimators': 300, 'learning_rate': 0.01, 'max_depth': 7, 'num_leaves': 31, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 1.0, 'reg_lambda': 0.1, 'min_child_weight': 0.1, 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for GPBoost with NopPruner: 86.9582677962846\n","Correlation Coefficient for GPBoost with NopPruner: 0.8988928982684636\n","Running Optuna for GPBoost with PatientPruner...\n","Best MSE for GPBoost with PatientPruner: 7556.738487349329 with params: {'n_estimators': 300, 'learning_rate': 0.01, 'max_depth': 5, 'num_leaves': 15, 'min_child_samples': 10, 'subsample': 0.8, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 1.0, 'min_child_weight': 0.01, 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for GPBoost with PatientPruner: 86.92950297424534\n","Correlation Coefficient for GPBoost with PatientPruner: 0.8998878429456466\n","Running Optuna for GPBoost with PercentilePruner...\n","Best MSE for GPBoost with PercentilePruner: 7695.892984238559 with params: {'n_estimators': 400, 'learning_rate': 0.01, 'max_depth': 3, 'num_leaves': 31, 'min_child_samples': 10, 'subsample': 0.6, 'colsample_bytree': 0.5, 'reg_alpha': 0.5, 'reg_lambda': 1.0, 'min_child_weight': 0.001, 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for GPBoost with PercentilePruner: 87.7262388583858\n","Correlation Coefficient for GPBoost with PercentilePruner: 0.8952597138838548\n","Running Optuna for GPBoost with SuccessiveHalvingPruner...\n","Best MSE for GPBoost with SuccessiveHalvingPruner: 7719.798660513351 with params: {'n_estimators': 400, 'learning_rate': 0.01, 'max_depth': 3, 'num_leaves': 15, 'min_child_samples': 10, 'subsample': 0.7, 'colsample_bytree': 0.5, 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'min_child_weight': 0.001, 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for GPBoost with SuccessiveHalvingPruner: 87.86238478731016\n","Correlation Coefficient for GPBoost with SuccessiveHalvingPruner: 0.8947212110299436\n","Running Optuna for GPBoost with HyperbandPruner...\n","Best MSE for GPBoost with HyperbandPruner: 7390.497112033827 with params: {'n_estimators': 300, 'learning_rate': 0.01, 'max_depth': 3, 'num_leaves': 63, 'min_child_samples': 10, 'subsample': 0.5, 'colsample_bytree': 0.5, 'reg_alpha': 0.5, 'reg_lambda': 0, 'min_child_weight': 0.01, 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for GPBoost with HyperbandPruner: 85.9680005120151\n","Correlation Coefficient for GPBoost with HyperbandPruner: 0.9021764089273122\n","Running Optuna for GPBoost with ThresholdPruner...\n","Best MSE for GPBoost with ThresholdPruner: 7752.413923785924 with params: {'n_estimators': 400, 'learning_rate': 0.01, 'max_depth': 3, 'num_leaves': 15, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'min_child_weight': 0.001, 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for GPBoost with ThresholdPruner: 88.04779340668297\n","Correlation Coefficient for GPBoost with ThresholdPruner: 0.8941771471912344\n","Running Optuna for GPBoost with WilcoxonPruner...\n","Best MSE for GPBoost with WilcoxonPruner: 7415.173781485694 with params: {'n_estimators': 300, 'learning_rate': 0.01, 'max_depth': 3, 'num_leaves': 31, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 0.1, 'min_child_weight': 0.1, 'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n","Best RMSE for GPBoost with WilcoxonPruner: 86.11140331852509\n","Correlation Coefficient for GPBoost with WilcoxonPruner: 0.9019365138758428\n","Running Optuna for CatBoost with MedianPruner...\n","Best MSE for CatBoost with MedianPruner: 12376.263521855422 with params: {'iterations': 1000, 'learning_rate': 0.01, 'depth': 4, 'l2_leaf_reg': 5, 'border_count': 64, 'min_data_in_leaf': 20, 'rsm': 0.6, 'bagging_temperature': 0, 'random_seed': 42, 'verbose': 0}\n","Best RMSE for CatBoost with MedianPruner: 111.2486562698868\n","Correlation Coefficient for CatBoost with MedianPruner: 0.8292568399631975\n","Running Optuna for CatBoost with NopPruner...\n","Best MSE for CatBoost with NopPruner: 12295.893812610366 with params: {'iterations': 1000, 'learning_rate': 0.01, 'depth': 4, 'l2_leaf_reg': 5, 'border_count': 32, 'min_data_in_leaf': 20, 'rsm': 0.6, 'bagging_temperature': 1, 'random_seed': 42, 'verbose': 0}\n","Best RMSE for CatBoost with NopPruner: 110.88685139641385\n","Correlation Coefficient for CatBoost with NopPruner: 0.8292402939039899\n","Running Optuna for CatBoost with PatientPruner...\n","Best MSE for CatBoost with PatientPruner: 12238.78020635146 with params: {'iterations': 200, 'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 1, 'border_count': 32, 'min_data_in_leaf': 5, 'rsm': 0.6, 'bagging_temperature': 10, 'random_seed': 42, 'verbose': 0}\n","Best RMSE for CatBoost with PatientPruner: 110.62902063360887\n","Correlation Coefficient for CatBoost with PatientPruner: 0.8361744446633028\n","Running Optuna for CatBoost with PercentilePruner...\n","Best MSE for CatBoost with PercentilePruner: 11828.352768700503 with params: {'iterations': 200, 'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 1, 'border_count': 32, 'min_data_in_leaf': 20, 'rsm': 1.0, 'bagging_temperature': 0, 'random_seed': 42, 'verbose': 0}\n","Best RMSE for CatBoost with PercentilePruner: 108.75823080898522\n","Correlation Coefficient for CatBoost with PercentilePruner: 0.8402340352771763\n","Running Optuna for CatBoost with SuccessiveHalvingPruner...\n","Best MSE for CatBoost with SuccessiveHalvingPruner: 12104.45825994089 with params: {'iterations': 500, 'learning_rate': 0.01, 'depth': 4, 'l2_leaf_reg': 1, 'border_count': 64, 'min_data_in_leaf': 1, 'rsm': 0.6, 'bagging_temperature': 1, 'random_seed': 42, 'verbose': 0}\n","Best RMSE for CatBoost with SuccessiveHalvingPruner: 110.02026295160765\n","Correlation Coefficient for CatBoost with SuccessiveHalvingPruner: 0.8375757155776365\n","Running Optuna for CatBoost with HyperbandPruner...\n","Best MSE for CatBoost with HyperbandPruner: 12143.915733308544 with params: {'iterations': 500, 'learning_rate': 0.01, 'depth': 4, 'l2_leaf_reg': 1, 'border_count': 32, 'min_data_in_leaf': 10, 'rsm': 1.0, 'bagging_temperature': 0, 'random_seed': 42, 'verbose': 0}\n","Best RMSE for CatBoost with HyperbandPruner: 110.19943617509367\n","Correlation Coefficient for CatBoost with HyperbandPruner: 0.8341941766725451\n","Running Optuna for CatBoost with ThresholdPruner...\n","Best MSE for CatBoost with ThresholdPruner: 12059.717630039499 with params: {'iterations': 200, 'learning_rate': 0.05, 'depth': 4, 'l2_leaf_reg': 5, 'border_count': 64, 'min_data_in_leaf': 10, 'rsm': 1.0, 'bagging_temperature': 1, 'random_seed': 42, 'verbose': 0}\n","Best RMSE for CatBoost with ThresholdPruner: 109.81674567223115\n","Correlation Coefficient for CatBoost with ThresholdPruner: 0.834682576760058\n","Running Optuna for CatBoost with WilcoxonPruner...\n","Best MSE for CatBoost with WilcoxonPruner: 12469.34371865179 with params: {'iterations': 500, 'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 9, 'border_count': 32, 'min_data_in_leaf': 5, 'rsm': 0.6, 'bagging_temperature': 10, 'random_seed': 42, 'verbose': 0}\n","Best RMSE for CatBoost with WilcoxonPruner: 111.66621565474398\n","Correlation Coefficient for CatBoost with WilcoxonPruner: 0.8262490043890625\n","Running Optuna for NGBoost with MedianPruner...\n","Best MSE for NGBoost with MedianPruner: 12350.956487191177 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'natural_gradient': True, 'minibatch_frac': 0.5, 'col_sample': 0.5, 'Dist': <class 'ngboost.distns.normal.Normal'>, 'Score': <class 'ngboost.scores.LogScore'>, 'random_state': 42, 'verbose': 0}\n","Best RMSE for NGBoost with MedianPruner: 111.1348572104683\n","Correlation Coefficient for NGBoost with MedianPruner: 0.8240968482667025\n","Running Optuna for NGBoost with NopPruner...\n","Best MSE for NGBoost with NopPruner: 12516.277127218891 with params: {'n_estimators': 200, 'learning_rate': 0.01, 'natural_gradient': True, 'minibatch_frac': 0.7, 'col_sample': 1.0, 'Dist': <class 'ngboost.distns.normal.Normal'>, 'Score': <class 'ngboost.scores.LogScore'>, 'random_state': 42, 'verbose': 0}\n","Best RMSE for NGBoost with NopPruner: 111.87616871889604\n","Correlation Coefficient for NGBoost with NopPruner: 0.8229323362566604\n","Running Optuna for NGBoost with PatientPruner...\n","Best MSE for NGBoost with PatientPruner: 12407.823695288273 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'natural_gradient': True, 'minibatch_frac': 0.5, 'col_sample': 0.5, 'Dist': <class 'ngboost.distns.normal.Normal'>, 'Score': <class 'ngboost.scores.LogScore'>, 'random_state': 42, 'verbose': 0}\n","Best RMSE for NGBoost with PatientPruner: 111.39041114605993\n","Correlation Coefficient for NGBoost with PatientPruner: 0.8236012244709813\n","Running Optuna for NGBoost with PercentilePruner...\n","Best MSE for NGBoost with PercentilePruner: 11891.196521152962 with params: {'n_estimators': 200, 'learning_rate': 0.01, 'natural_gradient': True, 'minibatch_frac': 0.5, 'col_sample': 0.9, 'Dist': <class 'ngboost.distns.normal.Normal'>, 'Score': <class 'ngboost.scores.LogScore'>, 'random_state': 42, 'verbose': 0}\n","Best RMSE for NGBoost with PercentilePruner: 109.0467630017185\n","Correlation Coefficient for NGBoost with PercentilePruner: 0.8320328778774883\n","Running Optuna for NGBoost with SuccessiveHalvingPruner...\n","Best MSE for NGBoost with SuccessiveHalvingPruner: 12081.228944892368 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'natural_gradient': True, 'minibatch_frac': 0.5, 'col_sample': 0.7, 'Dist': <class 'ngboost.distns.normal.Normal'>, 'Score': <class 'ngboost.scores.LogScore'>, 'random_state': 42, 'verbose': 0}\n","Best RMSE for NGBoost with SuccessiveHalvingPruner: 109.9146439055887\n","Correlation Coefficient for NGBoost with SuccessiveHalvingPruner: 0.8282018510834986\n","Running Optuna for NGBoost with HyperbandPruner...\n","Best MSE for NGBoost with HyperbandPruner: 12186.4936254796 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'natural_gradient': True, 'minibatch_frac': 0.5, 'col_sample': 0.5, 'Dist': <class 'ngboost.distns.normal.Normal'>, 'Score': <class 'ngboost.scores.LogScore'>, 'random_state': 42, 'verbose': 0}\n","Best RMSE for NGBoost with HyperbandPruner: 110.3924527559724\n","Correlation Coefficient for NGBoost with HyperbandPruner: 0.8265603520840819\n","Running Optuna for NGBoost with ThresholdPruner...\n","Best MSE for NGBoost with ThresholdPruner: 12267.687603954357 with params: {'n_estimators': 200, 'learning_rate': 0.01, 'natural_gradient': True, 'minibatch_frac': 0.7, 'col_sample': 0.9, 'Dist': <class 'ngboost.distns.normal.Normal'>, 'Score': <class 'ngboost.scores.LogScore'>, 'random_state': 42, 'verbose': 0}\n","Best RMSE for NGBoost with ThresholdPruner: 110.75959373324893\n","Correlation Coefficient for NGBoost with ThresholdPruner: 0.8284245213996452\n","Running Optuna for NGBoost with WilcoxonPruner...\n","Best MSE for NGBoost with WilcoxonPruner: 12801.060181481735 with params: {'n_estimators': 200, 'learning_rate': 0.01, 'natural_gradient': True, 'minibatch_frac': 0.7, 'col_sample': 0.9, 'Dist': <class 'ngboost.distns.normal.Normal'>, 'Score': <class 'ngboost.scores.LogScore'>, 'random_state': 42, 'verbose': 0}\n","Best RMSE for NGBoost with WilcoxonPruner: 113.1417702773018\n","Correlation Coefficient for NGBoost with WilcoxonPruner: 0.8190521037461904\n","Running Optuna for HistGradientBoosting with MedianPruner...\n","Best MSE for HistGradientBoosting with MedianPruner: 9140.53355576915 with params: {'learning_rate': 0.05, 'max_iter': 100, 'max_depth': 7, 'min_samples_leaf': 10, 'max_leaf_nodes': 63, 'l2_regularization': 0.0, 'max_bins': 128, 'early_stopping': False, 'validation_fraction': 0.2, 'n_iter_no_change': 5, 'loss': 'squared_error', 'random_state': 42, 'verbose': 0}\n","Best RMSE for HistGradientBoosting with MedianPruner: 95.60613764695837\n","Correlation Coefficient for HistGradientBoosting with MedianPruner: 0.8754575749315937\n","Running Optuna for HistGradientBoosting with NopPruner...\n","Best MSE for HistGradientBoosting with NopPruner: 8220.483111237245 with params: {'learning_rate': 0.01, 'max_iter': 200, 'max_depth': 3, 'min_samples_leaf': 10, 'max_leaf_nodes': 63, 'l2_regularization': 0.5, 'max_bins': 255, 'early_stopping': False, 'validation_fraction': 0.2, 'n_iter_no_change': 10, 'loss': 'squared_error', 'random_state': 42, 'verbose': 0}\n","Best RMSE for HistGradientBoosting with NopPruner: 90.66687990240563\n","Correlation Coefficient for HistGradientBoosting with NopPruner: 0.9031505169182412\n","Running Optuna for HistGradientBoosting with PatientPruner...\n","Best MSE for HistGradientBoosting with PatientPruner: 8079.834507712777 with params: {'learning_rate': 0.01, 'max_iter': 200, 'max_depth': 3, 'min_samples_leaf': 10, 'max_leaf_nodes': 31, 'l2_regularization': 0.0, 'max_bins': 255, 'early_stopping': False, 'validation_fraction': 0.1, 'n_iter_no_change': 15, 'loss': 'squared_error', 'random_state': 42, 'verbose': 0}\n","Best RMSE for HistGradientBoosting with PatientPruner: 89.88789967349764\n","Correlation Coefficient for HistGradientBoosting with PatientPruner: 0.9032771484501396\n","Running Optuna for HistGradientBoosting with PercentilePruner...\n","Best MSE for HistGradientBoosting with PercentilePruner: 8317.046438364536 with params: {'learning_rate': 0.01, 'max_iter': 300, 'max_depth': 3, 'min_samples_leaf': 10, 'max_leaf_nodes': None, 'l2_regularization': 1.0, 'max_bins': 128, 'early_stopping': False, 'validation_fraction': 0.2, 'n_iter_no_change': 5, 'loss': 'squared_error', 'random_state': 42, 'verbose': 0}\n","Best RMSE for HistGradientBoosting with PercentilePruner: 91.1978422900703\n","Correlation Coefficient for HistGradientBoosting with PercentilePruner: 0.886267177095813\n","Running Optuna for HistGradientBoosting with SuccessiveHalvingPruner...\n","Best MSE for HistGradientBoosting with SuccessiveHalvingPruner: 13963.238482618499 with params: {'learning_rate': 0.01, 'max_iter': 200, 'max_depth': 3, 'min_samples_leaf': 5, 'max_leaf_nodes': 63, 'l2_regularization': 0.0, 'max_bins': 64, 'early_stopping': True, 'validation_fraction': 0.1, 'n_iter_no_change': 15, 'loss': 'squared_error', 'random_state': 42, 'verbose': 0}\n","Best RMSE for HistGradientBoosting with SuccessiveHalvingPruner: 118.16614778615109\n","Correlation Coefficient for HistGradientBoosting with SuccessiveHalvingPruner: 0.8092102479436297\n","Running Optuna for HistGradientBoosting with HyperbandPruner...\n","Best MSE for HistGradientBoosting with HyperbandPruner: 8317.046438364536 with params: {'learning_rate': 0.01, 'max_iter': 300, 'max_depth': 3, 'min_samples_leaf': 10, 'max_leaf_nodes': 15, 'l2_regularization': 1.0, 'max_bins': 255, 'early_stopping': False, 'validation_fraction': 0.1, 'n_iter_no_change': 5, 'loss': 'squared_error', 'random_state': 42, 'verbose': 0}\n","Best RMSE for HistGradientBoosting with HyperbandPruner: 91.1978422900703\n","Correlation Coefficient for HistGradientBoosting with HyperbandPruner: 0.886267177095813\n","Running Optuna for HistGradientBoosting with ThresholdPruner...\n","Best MSE for HistGradientBoosting with ThresholdPruner: 9029.10721281066 with params: {'learning_rate': 0.05, 'max_iter': 100, 'max_depth': 7, 'min_samples_leaf': 10, 'max_leaf_nodes': None, 'l2_regularization': 0.5, 'max_bins': 64, 'early_stopping': False, 'validation_fraction': 0.1, 'n_iter_no_change': 15, 'loss': 'squared_error', 'random_state': 42, 'verbose': 0}\n","Best RMSE for HistGradientBoosting with ThresholdPruner: 95.02161445066412\n","Correlation Coefficient for HistGradientBoosting with ThresholdPruner: 0.8773438467284207\n","Running Optuna for HistGradientBoosting with WilcoxonPruner...\n","Best MSE for HistGradientBoosting with WilcoxonPruner: 14087.668901725148 with params: {'learning_rate': 0.01, 'max_iter': 500, 'max_depth': 3, 'min_samples_leaf': 5, 'max_leaf_nodes': 15, 'l2_regularization': 0.5, 'max_bins': 64, 'early_stopping': True, 'validation_fraction': 0.1, 'n_iter_no_change': 5, 'loss': 'squared_error', 'random_state': 42, 'verbose': 0}\n","Best RMSE for HistGradientBoosting with WilcoxonPruner: 118.69148622258105\n","Correlation Coefficient for HistGradientBoosting with WilcoxonPruner: 0.7977046077554625\n","Running Optuna for PGBM with MedianPruner...\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Estimator 0/100, Train metric: 283.0620\n","Estimator 1/100, Train metric: 273.9192\n","Estimator 2/100, Train metric: 265.3537\n","Estimator 3/100, Train metric: 263.0631\n","Estimator 4/100, Train metric: 254.4809\n","Estimator 5/100, Train metric: 247.4187\n","Estimator 6/100, Train metric: 240.4466\n","Estimator 7/100, Train metric: 234.0957\n","Estimator 8/100, Train metric: 228.0297\n","Estimator 9/100, Train metric: 223.4584\n","Estimator 10/100, Train metric: 217.2935\n","Estimator 11/100, Train metric: 213.3898\n","Estimator 12/100, Train metric: 208.8988\n","Estimator 13/100, Train metric: 204.3514\n","Estimator 14/100, Train metric: 203.3963\n","Estimator 15/100, Train metric: 201.3887\n","Estimator 16/100, Train metric: 197.6638\n","Estimator 17/100, Train metric: 194.3197\n","Estimator 18/100, Train metric: 192.4540\n","Estimator 19/100, Train metric: 191.0914\n","Estimator 20/100, Train metric: 188.1185\n","Estimator 21/100, Train metric: 186.8926\n","Estimator 22/100, Train metric: 184.4603\n","Estimator 23/100, Train metric: 182.9470\n","Estimator 24/100, Train metric: 181.9231\n","Estimator 25/100, Train metric: 180.7714\n","Estimator 26/100, Train metric: 178.3741\n","Estimator 27/100, Train metric: 175.3826\n","Estimator 28/100, Train metric: 173.1271\n","Estimator 29/100, Train metric: 171.0247\n","Estimator 30/100, Train metric: 168.9585\n","Estimator 31/100, Train metric: 168.2763\n","Estimator 32/100, Train metric: 166.5567\n","Estimator 33/100, Train metric: 165.4086\n","Estimator 34/100, Train metric: 163.5768\n","Estimator 35/100, Train metric: 161.9144\n","Estimator 36/100, Train metric: 160.6084\n","Estimator 37/100, Train metric: 159.4957\n","Estimator 38/100, Train metric: 159.0074\n","Estimator 39/100, Train metric: 157.6709\n","Estimator 40/100, Train metric: 157.0408\n","Estimator 41/100, Train metric: 156.0245\n","Estimator 42/100, Train metric: 155.1078\n","Estimator 43/100, Train metric: 154.4750\n","Estimator 44/100, Train metric: 153.8491\n","Estimator 45/100, Train metric: 152.9551\n","Estimator 46/100, Train metric: 152.3970\n","Estimator 47/100, Train metric: 151.3801\n","Estimator 48/100, Train metric: 150.7231\n","Estimator 49/100, Train metric: 150.2323\n","Estimator 50/100, Train metric: 149.6435\n","Estimator 51/100, Train metric: 148.8177\n","Estimator 52/100, Train metric: 147.8524\n","Estimator 53/100, Train metric: 147.2604\n","Estimator 54/100, Train metric: 146.7894\n","Estimator 55/100, Train metric: 145.1839\n","Estimator 56/100, Train metric: 144.6884\n","Estimator 57/100, Train metric: 144.2401\n","Estimator 58/100, Train metric: 143.4703\n","Estimator 59/100, Train metric: 143.0430\n","Estimator 60/100, Train metric: 142.0806\n","Estimator 61/100, Train metric: 141.4134\n","Estimator 62/100, Train metric: 141.0260\n","Estimator 63/100, Train metric: 140.4923\n","Estimator 64/100, Train metric: 139.8054\n","Estimator 65/100, Train metric: 139.1116\n","Estimator 66/100, Train metric: 138.6458\n","Estimator 67/100, Train metric: 138.0534\n","Estimator 68/100, Train metric: 137.5891\n","Estimator 69/100, Train metric: 136.1185\n","Estimator 70/100, Train metric: 134.8891\n","Estimator 71/100, Train metric: 134.2518\n","Estimator 72/100, Train metric: 133.8418\n","Estimator 73/100, Train metric: 132.5074\n","Estimator 74/100, Train metric: 132.0540\n","Estimator 75/100, Train metric: 131.6951\n","Estimator 76/100, Train metric: 131.3396\n","Estimator 77/100, Train metric: 130.7998\n","Estimator 78/100, Train metric: 129.3837\n","Estimator 79/100, Train metric: 129.0971\n","Estimator 80/100, Train metric: 128.7822\n","Estimator 81/100, Train metric: 127.4856\n","Estimator 82/100, Train metric: 127.0036\n","Estimator 83/100, Train metric: 126.7923\n","Estimator 84/100, Train metric: 126.5968\n","Estimator 85/100, Train metric: 125.8644\n","Estimator 86/100, Train metric: 125.2724\n","Estimator 87/100, Train metric: 124.9277\n","Estimator 88/100, Train metric: 124.5663\n","Estimator 89/100, Train metric: 123.7865\n","Estimator 90/100, Train metric: 123.4715\n","Estimator 91/100, Train metric: 122.3353\n","Estimator 92/100, Train metric: 121.8313\n","Estimator 93/100, Train metric: 121.3807\n","Estimator 94/100, Train metric: 121.1928\n","Estimator 95/100, Train metric: 120.6061\n","Estimator 96/100, Train metric: 120.1647\n","Estimator 97/100, Train metric: 119.6909\n","Estimator 98/100, Train metric: 119.1761\n","Estimator 99/100, Train metric: 118.5586\n","Best MSE for PGBM with MedianPruner: 8211.289334879875 with params: {'n_estimators': 100, 'learning_rate': 0.05, 'max_leaves': 29, 'min_split_gain': 0.1, 'reg_lambda': 0.1, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'tree_correlation': 0.1, 'min_data_in_leaf': 10, 'max_bin': 64, 'distribution': 'laplace'}\n","Best RMSE for PGBM with MedianPruner: 90.61616486521527\n","Correlation Coefficient for PGBM with MedianPruner: 0.8883623538234999\n","Running Optuna for PGBM with NopPruner...\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Estimator 0/500, Train metric: 284.2808\n","Estimator 1/500, Train metric: 282.5529\n","Estimator 2/500, Train metric: 280.8342\n","Estimator 3/500, Train metric: 280.3847\n","Estimator 4/500, Train metric: 278.5672\n","Estimator 5/500, Train metric: 276.9746\n","Estimator 6/500, Train metric: 275.3138\n","Estimator 7/500, Train metric: 273.7082\n","Estimator 8/500, Train metric: 272.0602\n","Estimator 9/500, Train metric: 270.6949\n","Estimator 10/500, Train metric: 268.9191\n","Estimator 11/500, Train metric: 267.5644\n","Estimator 12/500, Train metric: 266.0201\n","Estimator 13/500, Train metric: 264.4114\n","Estimator 14/500, Train metric: 264.1534\n","Estimator 15/500, Train metric: 263.6612\n","Estimator 16/500, Train metric: 262.1527\n","Estimator 17/500, Train metric: 260.7075\n","Estimator 18/500, Train metric: 260.1642\n","Estimator 19/500, Train metric: 259.7554\n","Estimator 20/500, Train metric: 258.3411\n","Estimator 21/500, Train metric: 257.9560\n","Estimator 22/500, Train metric: 256.6648\n","Estimator 23/500, Train metric: 256.1513\n","Estimator 24/500, Train metric: 255.7805\n","Estimator 25/500, Train metric: 255.4100\n","Estimator 26/500, Train metric: 254.0810\n","Estimator 27/500, Train metric: 252.5413\n","Estimator 28/500, Train metric: 251.1473\n","Estimator 29/500, Train metric: 249.7847\n","Estimator 30/500, Train metric: 248.3902\n","Estimator 31/500, Train metric: 248.0497\n","Estimator 32/500, Train metric: 246.7606\n","Estimator 33/500, Train metric: 246.3730\n","Estimator 34/500, Train metric: 244.9755\n","Estimator 35/500, Train metric: 243.6407\n","Estimator 36/500, Train metric: 242.3558\n","Estimator 37/500, Train metric: 241.1265\n","Estimator 38/500, Train metric: 240.9195\n","Estimator 39/500, Train metric: 239.5765\n","Estimator 40/500, Train metric: 239.4094\n","Estimator 41/500, Train metric: 238.1740\n","Estimator 42/500, Train metric: 236.9536\n","Estimator 43/500, Train metric: 235.8705\n","Estimator 44/500, Train metric: 235.1041\n","Estimator 45/500, Train metric: 233.8554\n","Estimator 46/500, Train metric: 232.8421\n","Estimator 47/500, Train metric: 232.3934\n","Estimator 48/500, Train metric: 231.3036\n","Estimator 49/500, Train metric: 230.2320\n","Estimator 50/500, Train metric: 229.1683\n","Estimator 51/500, Train metric: 228.7648\n","Estimator 52/500, Train metric: 228.3511\n","Estimator 53/500, Train metric: 227.2588\n","Estimator 54/500, Train metric: 227.0636\n","Estimator 55/500, Train metric: 226.1632\n","Estimator 56/500, Train metric: 225.3213\n","Estimator 57/500, Train metric: 224.2623\n","Estimator 58/500, Train metric: 223.8139\n","Estimator 59/500, Train metric: 223.6404\n","Estimator 60/500, Train metric: 222.7771\n","Estimator 61/500, Train metric: 222.4353\n","Estimator 62/500, Train metric: 222.1920\n","Estimator 63/500, Train metric: 221.1686\n","Estimator 64/500, Train metric: 220.9390\n","Estimator 65/500, Train metric: 220.6531\n","Estimator 66/500, Train metric: 219.6451\n","Estimator 67/500, Train metric: 219.3220\n","Estimator 68/500, Train metric: 218.2854\n","Estimator 69/500, Train metric: 217.3871\n","Estimator 70/500, Train metric: 216.4848\n","Estimator 71/500, Train metric: 216.2043\n","Estimator 72/500, Train metric: 215.2851\n","Estimator 73/500, Train metric: 214.4362\n","Estimator 74/500, Train metric: 213.4147\n","Estimator 75/500, Train metric: 212.4862\n","Estimator 76/500, Train metric: 211.6328\n","Estimator 77/500, Train metric: 211.3913\n","Estimator 78/500, Train metric: 210.5031\n","Estimator 79/500, Train metric: 209.6378\n","Estimator 80/500, Train metric: 208.7313\n","Estimator 81/500, Train metric: 207.8841\n","Estimator 82/500, Train metric: 207.0415\n","Estimator 83/500, Train metric: 206.4863\n","Estimator 84/500, Train metric: 205.6697\n","Estimator 85/500, Train metric: 205.3720\n","Estimator 86/500, Train metric: 205.1318\n","Estimator 87/500, Train metric: 204.2370\n","Estimator 88/500, Train metric: 203.4619\n","Estimator 89/500, Train metric: 202.6881\n","Estimator 90/500, Train metric: 201.7532\n","Estimator 91/500, Train metric: 201.0302\n","Estimator 92/500, Train metric: 200.7402\n","Estimator 93/500, Train metric: 200.0039\n","Estimator 94/500, Train metric: 199.3024\n","Estimator 95/500, Train metric: 198.6439\n","Estimator 96/500, Train metric: 198.3296\n","Estimator 97/500, Train metric: 197.9885\n","Estimator 98/500, Train metric: 197.3587\n","Estimator 99/500, Train metric: 197.0696\n","Estimator 100/500, Train metric: 196.3708\n","Estimator 101/500, Train metric: 195.7553\n","Estimator 102/500, Train metric: 195.5455\n","Estimator 103/500, Train metric: 194.8616\n","Estimator 104/500, Train metric: 194.2536\n","Estimator 105/500, Train metric: 193.5856\n","Estimator 106/500, Train metric: 193.2617\n","Estimator 107/500, Train metric: 192.6116\n","Estimator 108/500, Train metric: 191.9762\n","Estimator 109/500, Train metric: 191.2895\n","Estimator 110/500, Train metric: 190.6726\n","Estimator 111/500, Train metric: 190.1235\n","Estimator 112/500, Train metric: 189.4783\n","Estimator 113/500, Train metric: 189.1628\n","Estimator 114/500, Train metric: 188.5674\n","Estimator 115/500, Train metric: 187.9843\n","Estimator 116/500, Train metric: 187.7046\n","Estimator 117/500, Train metric: 187.4227\n","Estimator 118/500, Train metric: 186.7372\n","Estimator 119/500, Train metric: 186.1753\n","Estimator 120/500, Train metric: 185.8584\n","Estimator 121/500, Train metric: 185.3054\n","Estimator 122/500, Train metric: 184.7568\n","Estimator 123/500, Train metric: 184.2200\n","Estimator 124/500, Train metric: 183.9415\n","Estimator 125/500, Train metric: 183.4380\n","Estimator 126/500, Train metric: 182.9189\n","Estimator 127/500, Train metric: 182.4051\n","Estimator 128/500, Train metric: 181.9566\n","Estimator 129/500, Train metric: 181.5036\n","Estimator 130/500, Train metric: 180.9707\n","Estimator 131/500, Train metric: 180.4732\n","Estimator 132/500, Train metric: 179.9058\n","Estimator 133/500, Train metric: 179.4356\n","Estimator 134/500, Train metric: 179.0485\n","Estimator 135/500, Train metric: 178.5517\n","Estimator 136/500, Train metric: 178.0478\n","Estimator 137/500, Train metric: 177.7590\n","Estimator 138/500, Train metric: 177.2605\n","Estimator 139/500, Train metric: 176.7963\n","Estimator 140/500, Train metric: 176.3645\n","Estimator 141/500, Train metric: 176.1022\n","Estimator 142/500, Train metric: 175.6892\n","Estimator 143/500, Train metric: 175.2923\n","Estimator 144/500, Train metric: 175.0688\n","Estimator 145/500, Train metric: 174.6492\n","Estimator 146/500, Train metric: 174.4157\n","Estimator 147/500, Train metric: 174.1143\n","Estimator 148/500, Train metric: 173.8910\n","Estimator 149/500, Train metric: 173.5691\n","Estimator 150/500, Train metric: 173.0963\n","Estimator 151/500, Train metric: 172.7431\n","Estimator 152/500, Train metric: 172.3630\n","Estimator 153/500, Train metric: 171.9992\n","Estimator 154/500, Train metric: 171.6369\n","Estimator 155/500, Train metric: 171.2756\n","Estimator 156/500, Train metric: 171.0793\n","Estimator 157/500, Train metric: 170.6688\n","Estimator 158/500, Train metric: 170.4377\n","Estimator 159/500, Train metric: 170.0452\n","Estimator 160/500, Train metric: 169.8032\n","Estimator 161/500, Train metric: 169.3242\n","Estimator 162/500, Train metric: 168.9920\n","Estimator 163/500, Train metric: 168.6202\n","Estimator 164/500, Train metric: 168.4204\n","Estimator 165/500, Train metric: 168.0953\n","Estimator 166/500, Train metric: 167.7615\n","Estimator 167/500, Train metric: 167.5258\n","Estimator 168/500, Train metric: 167.1405\n","Estimator 169/500, Train metric: 166.8758\n","Estimator 170/500, Train metric: 166.5741\n","Estimator 171/500, Train metric: 166.2557\n","Estimator 172/500, Train metric: 166.0530\n","Estimator 173/500, Train metric: 165.7251\n","Estimator 174/500, Train metric: 165.4583\n","Estimator 175/500, Train metric: 165.2578\n","Estimator 176/500, Train metric: 164.9081\n","Estimator 177/500, Train metric: 164.6292\n","Estimator 178/500, Train metric: 164.3257\n","Estimator 179/500, Train metric: 164.0617\n","Estimator 180/500, Train metric: 163.8691\n","Estimator 181/500, Train metric: 163.6042\n","Estimator 182/500, Train metric: 163.3769\n","Estimator 183/500, Train metric: 163.1241\n","Estimator 184/500, Train metric: 162.9041\n","Estimator 185/500, Train metric: 162.6909\n","Estimator 186/500, Train metric: 162.4930\n","Estimator 187/500, Train metric: 162.1387\n","Estimator 188/500, Train metric: 161.9022\n","Estimator 189/500, Train metric: 161.6910\n","Estimator 190/500, Train metric: 161.5046\n","Estimator 191/500, Train metric: 161.2429\n","Estimator 192/500, Train metric: 161.0137\n","Estimator 193/500, Train metric: 160.7299\n","Estimator 194/500, Train metric: 160.5105\n","Estimator 195/500, Train metric: 160.2611\n","Estimator 196/500, Train metric: 160.0757\n","Estimator 197/500, Train metric: 159.9045\n","Estimator 198/500, Train metric: 159.6830\n","Estimator 199/500, Train metric: 159.4634\n","Estimator 200/500, Train metric: 159.2435\n","Estimator 201/500, Train metric: 159.0479\n","Estimator 202/500, Train metric: 158.8586\n","Estimator 203/500, Train metric: 158.6670\n","Estimator 204/500, Train metric: 158.5111\n","Estimator 205/500, Train metric: 158.1373\n","Estimator 206/500, Train metric: 157.9134\n","Estimator 207/500, Train metric: 157.7476\n","Estimator 208/500, Train metric: 157.5578\n","Estimator 209/500, Train metric: 157.3510\n","Estimator 210/500, Train metric: 157.1404\n","Estimator 211/500, Train metric: 156.9943\n","Estimator 212/500, Train metric: 156.7399\n","Estimator 213/500, Train metric: 156.4715\n","Estimator 214/500, Train metric: 156.3641\n","Estimator 215/500, Train metric: 156.1896\n","Estimator 216/500, Train metric: 155.9288\n","Estimator 217/500, Train metric: 155.7474\n","Estimator 218/500, Train metric: 155.5934\n","Estimator 219/500, Train metric: 155.4178\n","Estimator 220/500, Train metric: 155.2595\n","Estimator 221/500, Train metric: 155.0742\n","Estimator 222/500, Train metric: 154.9138\n","Estimator 223/500, Train metric: 154.7813\n","Estimator 224/500, Train metric: 154.6341\n","Estimator 225/500, Train metric: 154.4730\n","Estimator 226/500, Train metric: 154.2576\n","Estimator 227/500, Train metric: 154.0906\n","Estimator 228/500, Train metric: 153.9986\n","Estimator 229/500, Train metric: 153.8545\n","Estimator 230/500, Train metric: 153.7100\n","Estimator 231/500, Train metric: 153.5488\n","Estimator 232/500, Train metric: 153.4086\n","Estimator 233/500, Train metric: 153.2971\n","Estimator 234/500, Train metric: 153.1490\n","Estimator 235/500, Train metric: 152.9974\n","Estimator 236/500, Train metric: 152.7879\n","Estimator 237/500, Train metric: 152.6340\n","Estimator 238/500, Train metric: 152.4781\n","Estimator 239/500, Train metric: 152.3307\n","Estimator 240/500, Train metric: 152.2440\n","Estimator 241/500, Train metric: 152.1113\n","Estimator 242/500, Train metric: 151.9222\n","Estimator 243/500, Train metric: 151.7862\n","Estimator 244/500, Train metric: 151.6363\n","Estimator 245/500, Train metric: 151.4479\n","Estimator 246/500, Train metric: 151.3115\n","Estimator 247/500, Train metric: 151.2056\n","Estimator 248/500, Train metric: 151.1079\n","Estimator 249/500, Train metric: 150.9363\n","Estimator 250/500, Train metric: 150.7944\n","Estimator 251/500, Train metric: 150.6634\n","Estimator 252/500, Train metric: 150.5708\n","Estimator 253/500, Train metric: 150.3805\n","Estimator 254/500, Train metric: 150.2823\n","Estimator 255/500, Train metric: 150.1444\n","Estimator 256/500, Train metric: 150.0324\n","Estimator 257/500, Train metric: 149.9644\n","Estimator 258/500, Train metric: 149.7917\n","Estimator 259/500, Train metric: 149.6488\n","Estimator 260/500, Train metric: 149.5239\n","Estimator 261/500, Train metric: 149.3801\n","Estimator 262/500, Train metric: 149.1584\n","Estimator 263/500, Train metric: 149.0345\n","Estimator 264/500, Train metric: 148.8892\n","Estimator 265/500, Train metric: 148.7428\n","Estimator 266/500, Train metric: 148.6151\n","Estimator 267/500, Train metric: 148.5165\n","Estimator 268/500, Train metric: 148.4225\n","Estimator 269/500, Train metric: 148.3147\n","Estimator 270/500, Train metric: 148.2138\n","Estimator 271/500, Train metric: 148.1290\n","Estimator 272/500, Train metric: 148.0443\n","Estimator 273/500, Train metric: 147.9329\n","Estimator 274/500, Train metric: 147.8526\n","Estimator 275/500, Train metric: 147.7501\n","Estimator 276/500, Train metric: 147.6174\n","Estimator 277/500, Train metric: 147.2786\n","Estimator 278/500, Train metric: 147.1520\n","Estimator 279/500, Train metric: 147.0482\n","Estimator 280/500, Train metric: 146.9163\n","Estimator 281/500, Train metric: 146.8152\n","Estimator 282/500, Train metric: 146.6783\n","Estimator 283/500, Train metric: 146.5732\n","Estimator 284/500, Train metric: 146.4754\n","Estimator 285/500, Train metric: 146.3761\n","Estimator 286/500, Train metric: 146.2886\n","Estimator 287/500, Train metric: 146.1205\n","Estimator 288/500, Train metric: 146.0059\n","Estimator 289/500, Train metric: 145.6905\n","Estimator 290/500, Train metric: 145.5755\n","Estimator 291/500, Train metric: 145.3992\n","Estimator 292/500, Train metric: 145.3226\n","Estimator 293/500, Train metric: 145.2040\n","Estimator 294/500, Train metric: 145.0795\n","Estimator 295/500, Train metric: 144.9487\n","Estimator 296/500, Train metric: 144.8749\n","Estimator 297/500, Train metric: 144.7755\n","Estimator 298/500, Train metric: 144.6844\n","Estimator 299/500, Train metric: 144.5970\n","Estimator 300/500, Train metric: 144.4231\n","Estimator 301/500, Train metric: 144.3578\n","Estimator 302/500, Train metric: 144.2834\n","Estimator 303/500, Train metric: 144.2030\n","Estimator 304/500, Train metric: 144.1066\n","Estimator 305/500, Train metric: 143.9880\n","Estimator 306/500, Train metric: 143.8362\n","Estimator 307/500, Train metric: 143.6788\n","Estimator 308/500, Train metric: 143.6044\n","Estimator 309/500, Train metric: 143.5148\n","Estimator 310/500, Train metric: 143.4317\n","Estimator 311/500, Train metric: 143.3615\n","Estimator 312/500, Train metric: 143.2349\n","Estimator 313/500, Train metric: 143.1651\n","Estimator 314/500, Train metric: 143.0775\n","Estimator 315/500, Train metric: 143.0038\n","Estimator 316/500, Train metric: 142.9206\n","Estimator 317/500, Train metric: 142.7394\n","Estimator 318/500, Train metric: 142.6358\n","Estimator 319/500, Train metric: 142.5595\n","Estimator 320/500, Train metric: 142.4768\n","Estimator 321/500, Train metric: 142.2932\n","Estimator 322/500, Train metric: 142.1649\n","Estimator 323/500, Train metric: 142.0685\n","Estimator 324/500, Train metric: 142.0080\n","Estimator 325/500, Train metric: 141.9292\n","Estimator 326/500, Train metric: 141.8532\n","Estimator 327/500, Train metric: 141.7740\n","Estimator 328/500, Train metric: 141.5019\n","Estimator 329/500, Train metric: 141.3874\n","Estimator 330/500, Train metric: 141.1061\n","Estimator 331/500, Train metric: 141.0244\n","Estimator 332/500, Train metric: 140.8861\n","Estimator 333/500, Train metric: 140.5207\n","Estimator 334/500, Train metric: 140.4597\n","Estimator 335/500, Train metric: 140.3556\n","Estimator 336/500, Train metric: 140.2681\n","Estimator 337/500, Train metric: 140.1121\n","Estimator 338/500, Train metric: 139.9907\n","Estimator 339/500, Train metric: 139.9281\n","Estimator 340/500, Train metric: 139.8596\n","Estimator 341/500, Train metric: 139.4873\n","Estimator 342/500, Train metric: 139.3353\n","Estimator 343/500, Train metric: 139.1986\n","Estimator 344/500, Train metric: 139.0648\n","Estimator 345/500, Train metric: 138.9656\n","Estimator 346/500, Train metric: 138.8132\n","Estimator 347/500, Train metric: 138.7405\n","Estimator 348/500, Train metric: 138.4633\n","Estimator 349/500, Train metric: 138.3048\n","Estimator 350/500, Train metric: 138.2433\n","Estimator 351/500, Train metric: 138.1917\n","Estimator 352/500, Train metric: 138.0669\n","Estimator 353/500, Train metric: 137.9322\n","Estimator 354/500, Train metric: 137.8084\n","Estimator 355/500, Train metric: 137.6602\n","Estimator 356/500, Train metric: 137.5408\n","Estimator 357/500, Train metric: 137.3337\n","Estimator 358/500, Train metric: 137.1901\n","Estimator 359/500, Train metric: 137.1144\n","Estimator 360/500, Train metric: 136.8566\n","Estimator 361/500, Train metric: 136.7812\n","Estimator 362/500, Train metric: 136.4899\n","Estimator 363/500, Train metric: 136.4165\n","Estimator 364/500, Train metric: 136.0901\n","Estimator 365/500, Train metric: 135.9628\n","Estimator 366/500, Train metric: 135.8450\n","Estimator 367/500, Train metric: 135.8030\n","Estimator 368/500, Train metric: 135.6521\n","Estimator 369/500, Train metric: 135.5930\n","Estimator 370/500, Train metric: 135.5143\n","Estimator 371/500, Train metric: 135.3452\n","Estimator 372/500, Train metric: 135.0492\n","Estimator 373/500, Train metric: 134.9073\n","Estimator 374/500, Train metric: 134.8340\n","Estimator 375/500, Train metric: 134.7611\n","Estimator 376/500, Train metric: 134.5451\n","Estimator 377/500, Train metric: 134.2578\n","Estimator 378/500, Train metric: 134.0993\n","Estimator 379/500, Train metric: 133.8250\n","Estimator 380/500, Train metric: 133.6607\n","Estimator 381/500, Train metric: 133.5887\n","Estimator 382/500, Train metric: 133.4720\n","Estimator 383/500, Train metric: 133.3638\n","Estimator 384/500, Train metric: 133.3121\n","Estimator 385/500, Train metric: 133.2519\n","Estimator 386/500, Train metric: 133.1825\n","Estimator 387/500, Train metric: 132.9071\n","Estimator 388/500, Train metric: 132.8621\n","Estimator 389/500, Train metric: 132.7208\n","Estimator 390/500, Train metric: 132.4450\n","Estimator 391/500, Train metric: 132.1741\n","Estimator 392/500, Train metric: 132.1280\n","Estimator 393/500, Train metric: 131.9876\n","Estimator 394/500, Train metric: 131.7304\n","Estimator 395/500, Train metric: 131.4701\n","Estimator 396/500, Train metric: 131.4035\n","Estimator 397/500, Train metric: 131.2562\n","Estimator 398/500, Train metric: 131.1838\n","Estimator 399/500, Train metric: 131.1288\n","Estimator 400/500, Train metric: 131.0767\n","Estimator 401/500, Train metric: 131.0105\n","Estimator 402/500, Train metric: 130.9822\n","Estimator 403/500, Train metric: 130.8643\n","Estimator 404/500, Train metric: 130.6862\n","Estimator 405/500, Train metric: 130.6348\n","Estimator 406/500, Train metric: 130.5662\n","Estimator 407/500, Train metric: 130.4770\n","Estimator 408/500, Train metric: 130.3357\n","Estimator 409/500, Train metric: 130.2229\n","Estimator 410/500, Train metric: 130.1667\n","Estimator 411/500, Train metric: 130.0914\n","Estimator 412/500, Train metric: 129.8385\n","Estimator 413/500, Train metric: 129.7800\n","Estimator 414/500, Train metric: 129.5732\n","Estimator 415/500, Train metric: 129.5349\n","Estimator 416/500, Train metric: 129.3013\n","Estimator 417/500, Train metric: 129.2469\n","Estimator 418/500, Train metric: 129.1979\n","Estimator 419/500, Train metric: 129.1279\n","Estimator 420/500, Train metric: 129.0720\n","Estimator 421/500, Train metric: 128.9522\n","Estimator 422/500, Train metric: 128.7347\n","Estimator 423/500, Train metric: 128.6220\n","Estimator 424/500, Train metric: 128.3975\n","Estimator 425/500, Train metric: 128.2263\n","Estimator 426/500, Train metric: 128.1263\n","Estimator 427/500, Train metric: 128.0472\n","Estimator 428/500, Train metric: 127.9845\n","Estimator 429/500, Train metric: 127.8565\n","Estimator 430/500, Train metric: 127.7945\n","Estimator 431/500, Train metric: 127.5777\n","Estimator 432/500, Train metric: 127.3582\n","Estimator 433/500, Train metric: 127.1354\n","Estimator 434/500, Train metric: 127.0022\n","Estimator 435/500, Train metric: 126.8896\n","Estimator 436/500, Train metric: 126.8278\n","Estimator 437/500, Train metric: 126.7292\n","Estimator 438/500, Train metric: 126.5059\n","Estimator 439/500, Train metric: 126.4498\n","Estimator 440/500, Train metric: 126.3945\n","Estimator 441/500, Train metric: 126.2737\n","Estimator 442/500, Train metric: 126.1859\n","Estimator 443/500, Train metric: 126.0801\n","Estimator 444/500, Train metric: 125.9495\n","Estimator 445/500, Train metric: 125.7347\n","Estimator 446/500, Train metric: 125.5256\n","Estimator 447/500, Train metric: 125.3926\n","Estimator 448/500, Train metric: 125.1855\n","Estimator 449/500, Train metric: 125.1492\n","Estimator 450/500, Train metric: 124.9584\n","Estimator 451/500, Train metric: 124.8346\n","Estimator 452/500, Train metric: 124.7790\n","Estimator 453/500, Train metric: 124.6988\n","Estimator 454/500, Train metric: 124.6263\n","Estimator 455/500, Train metric: 124.5209\n","Estimator 456/500, Train metric: 124.3974\n","Estimator 457/500, Train metric: 124.1942\n","Estimator 458/500, Train metric: 124.0752\n","Estimator 459/500, Train metric: 124.0311\n","Estimator 460/500, Train metric: 123.9900\n","Estimator 461/500, Train metric: 123.9499\n","Estimator 462/500, Train metric: 123.9030\n","Estimator 463/500, Train metric: 123.8640\n","Estimator 464/500, Train metric: 123.6670\n","Estimator 465/500, Train metric: 123.6208\n","Estimator 466/500, Train metric: 123.4739\n","Estimator 467/500, Train metric: 123.4277\n","Estimator 468/500, Train metric: 123.3304\n","Estimator 469/500, Train metric: 123.2219\n","Estimator 470/500, Train metric: 123.1035\n","Estimator 471/500, Train metric: 122.9976\n","Estimator 472/500, Train metric: 122.8692\n","Estimator 473/500, Train metric: 122.6906\n","Estimator 474/500, Train metric: 122.4957\n","Estimator 475/500, Train metric: 122.3840\n","Estimator 476/500, Train metric: 122.2851\n","Estimator 477/500, Train metric: 122.1015\n","Estimator 478/500, Train metric: 122.0008\n","Estimator 479/500, Train metric: 121.8732\n","Estimator 480/500, Train metric: 121.7621\n","Estimator 481/500, Train metric: 121.6388\n","Estimator 482/500, Train metric: 121.4638\n","Estimator 483/500, Train metric: 121.3285\n","Estimator 484/500, Train metric: 121.2236\n","Estimator 485/500, Train metric: 121.0402\n","Estimator 486/500, Train metric: 120.9864\n","Estimator 487/500, Train metric: 120.9471\n","Estimator 488/500, Train metric: 120.8019\n","Estimator 489/500, Train metric: 120.6353\n","Estimator 490/500, Train metric: 120.4717\n","Estimator 491/500, Train metric: 120.3511\n","Estimator 492/500, Train metric: 120.2810\n","Estimator 493/500, Train metric: 120.1305\n","Estimator 494/500, Train metric: 120.0309\n","Estimator 495/500, Train metric: 119.9104\n","Estimator 496/500, Train metric: 119.7477\n","Estimator 497/500, Train metric: 119.6849\n","Estimator 498/500, Train metric: 119.6488\n","Estimator 499/500, Train metric: 119.5980\n","Best MSE for PGBM with NopPruner: 7935.705863759918 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'max_leaves': 50, 'min_split_gain': 0.5, 'reg_lambda': 1.0, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'tree_correlation': 0.2, 'min_data_in_leaf': 10, 'max_bin': 128, 'distribution': 'laplace'}\n","Best RMSE for PGBM with NopPruner: 89.08257890160073\n","Correlation Coefficient for PGBM with NopPruner: 0.8932170725368642\n","Running Optuna for PGBM with PatientPruner...\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Estimator 0/300, Train metric: 282.0879\n","Estimator 1/300, Train metric: 279.8289\n","Estimator 2/300, Train metric: 278.2864\n","Estimator 3/300, Train metric: 277.0394\n","Estimator 4/300, Train metric: 274.9732\n","Estimator 5/300, Train metric: 272.6784\n","Estimator 6/300, Train metric: 270.7197\n","Estimator 7/300, Train metric: 268.3668\n","Estimator 8/300, Train metric: 266.7731\n","Estimator 9/300, Train metric: 264.7304\n","Estimator 10/300, Train metric: 262.5108\n","Estimator 11/300, Train metric: 260.6197\n","Estimator 12/300, Train metric: 258.7881\n","Estimator 13/300, Train metric: 257.4232\n","Estimator 14/300, Train metric: 255.1190\n","Estimator 15/300, Train metric: 253.2872\n","Estimator 16/300, Train metric: 251.2900\n","Estimator 17/300, Train metric: 249.3364\n","Estimator 18/300, Train metric: 247.2919\n","Estimator 19/300, Train metric: 246.4284\n","Estimator 20/300, Train metric: 244.3657\n","Estimator 21/300, Train metric: 242.5655\n","Estimator 22/300, Train metric: 241.4949\n","Estimator 23/300, Train metric: 239.6941\n","Estimator 24/300, Train metric: 238.5791\n","Estimator 25/300, Train metric: 237.2830\n","Estimator 26/300, Train metric: 235.9565\n","Estimator 27/300, Train metric: 234.1498\n","Estimator 28/300, Train metric: 232.5585\n","Estimator 29/300, Train metric: 231.0100\n","Estimator 30/300, Train metric: 229.1918\n","Estimator 31/300, Train metric: 227.7442\n","Estimator 32/300, Train metric: 225.8721\n","Estimator 33/300, Train metric: 224.5793\n","Estimator 34/300, Train metric: 224.1823\n","Estimator 35/300, Train metric: 222.8103\n","Estimator 36/300, Train metric: 222.0056\n","Estimator 37/300, Train metric: 220.0602\n","Estimator 38/300, Train metric: 219.1826\n","Estimator 39/300, Train metric: 218.4252\n","Estimator 40/300, Train metric: 217.9641\n","Estimator 41/300, Train metric: 216.1090\n","Estimator 42/300, Train metric: 215.1601\n","Estimator 43/300, Train metric: 213.2915\n","Estimator 44/300, Train metric: 212.4584\n","Estimator 45/300, Train metric: 211.7207\n","Estimator 46/300, Train metric: 210.1851\n","Estimator 47/300, Train metric: 208.7092\n","Estimator 48/300, Train metric: 207.2079\n","Estimator 49/300, Train metric: 206.0768\n","Estimator 50/300, Train metric: 204.7174\n","Estimator 51/300, Train metric: 202.9321\n","Estimator 52/300, Train metric: 202.1340\n","Estimator 53/300, Train metric: 200.9151\n","Estimator 54/300, Train metric: 200.0612\n","Estimator 55/300, Train metric: 199.3337\n","Estimator 56/300, Train metric: 197.8991\n","Estimator 57/300, Train metric: 197.1959\n","Estimator 58/300, Train metric: 195.8478\n","Estimator 59/300, Train metric: 194.6137\n","Estimator 60/300, Train metric: 193.9840\n","Estimator 61/300, Train metric: 193.3539\n","Estimator 62/300, Train metric: 191.9638\n","Estimator 63/300, Train metric: 190.9811\n","Estimator 64/300, Train metric: 190.2200\n","Estimator 65/300, Train metric: 189.1275\n","Estimator 66/300, Train metric: 187.8243\n","Estimator 67/300, Train metric: 186.5762\n","Estimator 68/300, Train metric: 185.9452\n","Estimator 69/300, Train metric: 184.7641\n","Estimator 70/300, Train metric: 184.2771\n","Estimator 71/300, Train metric: 183.0382\n","Estimator 72/300, Train metric: 181.8579\n","Estimator 73/300, Train metric: 181.2379\n","Estimator 74/300, Train metric: 180.6920\n","Estimator 75/300, Train metric: 179.8251\n","Estimator 76/300, Train metric: 178.4650\n","Estimator 77/300, Train metric: 177.8050\n","Estimator 78/300, Train metric: 176.5578\n","Estimator 79/300, Train metric: 176.0331\n","Estimator 80/300, Train metric: 175.5924\n","Estimator 81/300, Train metric: 174.5875\n","Estimator 82/300, Train metric: 174.0645\n","Estimator 83/300, Train metric: 173.5672\n","Estimator 84/300, Train metric: 172.6295\n","Estimator 85/300, Train metric: 171.5969\n","Estimator 86/300, Train metric: 170.6430\n","Estimator 87/300, Train metric: 170.0141\n","Estimator 88/300, Train metric: 169.3896\n","Estimator 89/300, Train metric: 168.8724\n","Estimator 90/300, Train metric: 168.3174\n","Estimator 91/300, Train metric: 167.6990\n","Estimator 92/300, Train metric: 166.4754\n","Estimator 93/300, Train metric: 165.4741\n","Estimator 94/300, Train metric: 164.5345\n","Estimator 95/300, Train metric: 163.9499\n","Estimator 96/300, Train metric: 162.8870\n","Estimator 97/300, Train metric: 162.5956\n","Estimator 98/300, Train metric: 161.8620\n","Estimator 99/300, Train metric: 161.1634\n","Estimator 100/300, Train metric: 159.9584\n","Estimator 101/300, Train metric: 159.1791\n","Estimator 102/300, Train metric: 158.3331\n","Estimator 103/300, Train metric: 157.9407\n","Estimator 104/300, Train metric: 157.6402\n","Estimator 105/300, Train metric: 157.0131\n","Estimator 106/300, Train metric: 156.8687\n","Estimator 107/300, Train metric: 156.0550\n","Estimator 108/300, Train metric: 155.0009\n","Estimator 109/300, Train metric: 154.6065\n","Estimator 110/300, Train metric: 153.7875\n","Estimator 111/300, Train metric: 153.4809\n","Estimator 112/300, Train metric: 153.0132\n","Estimator 113/300, Train metric: 152.0968\n","Estimator 114/300, Train metric: 152.0169\n","Estimator 115/300, Train metric: 151.5674\n","Estimator 116/300, Train metric: 150.9646\n","Estimator 117/300, Train metric: 150.3975\n","Estimator 118/300, Train metric: 150.0504\n","Estimator 119/300, Train metric: 149.1492\n","Estimator 120/300, Train metric: 148.2636\n","Estimator 121/300, Train metric: 147.7807\n","Estimator 122/300, Train metric: 146.8118\n","Estimator 123/300, Train metric: 146.6503\n","Estimator 124/300, Train metric: 146.2448\n","Estimator 125/300, Train metric: 145.5855\n","Estimator 126/300, Train metric: 145.0321\n","Estimator 127/300, Train metric: 144.1719\n","Estimator 128/300, Train metric: 143.3982\n","Estimator 129/300, Train metric: 143.0535\n","Estimator 130/300, Train metric: 142.5845\n","Estimator 131/300, Train metric: 142.1598\n","Estimator 132/300, Train metric: 141.5157\n","Estimator 133/300, Train metric: 140.7440\n","Estimator 134/300, Train metric: 139.8669\n","Estimator 135/300, Train metric: 139.4283\n","Estimator 136/300, Train metric: 139.1162\n","Estimator 137/300, Train metric: 138.7000\n","Estimator 138/300, Train metric: 138.0478\n","Estimator 139/300, Train metric: 137.3692\n","Estimator 140/300, Train metric: 137.0073\n","Estimator 141/300, Train metric: 136.1382\n","Estimator 142/300, Train metric: 135.9412\n","Estimator 143/300, Train metric: 135.5603\n","Estimator 144/300, Train metric: 135.4472\n","Estimator 145/300, Train metric: 135.0465\n","Estimator 146/300, Train metric: 134.8389\n","Estimator 147/300, Train metric: 134.1186\n","Estimator 148/300, Train metric: 134.0089\n","Estimator 149/300, Train metric: 133.9847\n","Estimator 150/300, Train metric: 133.1961\n","Estimator 151/300, Train metric: 132.9082\n","Estimator 152/300, Train metric: 132.4612\n","Estimator 153/300, Train metric: 131.6622\n","Estimator 154/300, Train metric: 131.0006\n","Estimator 155/300, Train metric: 130.8274\n","Estimator 156/300, Train metric: 130.5676\n","Estimator 157/300, Train metric: 130.2408\n","Estimator 158/300, Train metric: 129.6384\n","Estimator 159/300, Train metric: 129.0515\n","Estimator 160/300, Train metric: 128.5374\n","Estimator 161/300, Train metric: 128.3464\n","Estimator 162/300, Train metric: 128.0383\n","Estimator 163/300, Train metric: 127.4290\n","Estimator 164/300, Train metric: 127.1691\n","Estimator 165/300, Train metric: 126.5610\n","Estimator 166/300, Train metric: 126.2794\n","Estimator 167/300, Train metric: 126.0449\n","Estimator 168/300, Train metric: 125.9239\n","Estimator 169/300, Train metric: 125.2348\n","Estimator 170/300, Train metric: 124.8229\n","Estimator 171/300, Train metric: 124.5499\n","Estimator 172/300, Train metric: 124.2697\n","Estimator 173/300, Train metric: 124.0786\n","Estimator 174/300, Train metric: 123.5390\n","Estimator 175/300, Train metric: 123.4346\n","Estimator 176/300, Train metric: 123.2689\n","Estimator 177/300, Train metric: 123.2193\n","Estimator 178/300, Train metric: 122.9451\n","Estimator 179/300, Train metric: 122.8463\n","Estimator 180/300, Train metric: 122.6873\n","Estimator 181/300, Train metric: 122.3923\n","Estimator 182/300, Train metric: 122.1912\n","Estimator 183/300, Train metric: 121.9641\n","Estimator 184/300, Train metric: 121.8079\n","Estimator 185/300, Train metric: 121.5726\n","Estimator 186/300, Train metric: 120.9478\n","Estimator 187/300, Train metric: 120.7913\n","Estimator 188/300, Train metric: 120.3500\n","Estimator 189/300, Train metric: 120.1729\n","Estimator 190/300, Train metric: 120.0550\n","Estimator 191/300, Train metric: 119.6556\n","Estimator 192/300, Train metric: 119.2752\n","Estimator 193/300, Train metric: 119.1537\n","Estimator 194/300, Train metric: 118.7786\n","Estimator 195/300, Train metric: 118.4411\n","Estimator 196/300, Train metric: 117.9038\n","Estimator 197/300, Train metric: 117.5907\n","Estimator 198/300, Train metric: 117.3457\n","Estimator 199/300, Train metric: 117.2922\n","Estimator 200/300, Train metric: 116.7763\n","Estimator 201/300, Train metric: 116.5751\n","Estimator 202/300, Train metric: 116.3903\n","Estimator 203/300, Train metric: 115.8997\n","Estimator 204/300, Train metric: 115.3339\n","Estimator 205/300, Train metric: 115.0282\n","Estimator 206/300, Train metric: 114.8473\n","Estimator 207/300, Train metric: 114.4438\n","Estimator 208/300, Train metric: 114.3024\n","Estimator 209/300, Train metric: 114.1382\n","Estimator 210/300, Train metric: 113.9605\n","Estimator 211/300, Train metric: 113.8458\n","Estimator 212/300, Train metric: 113.3609\n","Estimator 213/300, Train metric: 112.8634\n","Estimator 214/300, Train metric: 112.4591\n","Estimator 215/300, Train metric: 112.0142\n","Estimator 216/300, Train metric: 111.9220\n","Estimator 217/300, Train metric: 111.7957\n","Estimator 218/300, Train metric: 111.6727\n","Estimator 219/300, Train metric: 111.5750\n","Estimator 220/300, Train metric: 111.1822\n","Estimator 221/300, Train metric: 111.2063\n","Estimator 222/300, Train metric: 111.1803\n","Estimator 223/300, Train metric: 111.0954\n","Estimator 224/300, Train metric: 110.7831\n","Estimator 225/300, Train metric: 110.6200\n","Estimator 226/300, Train metric: 110.4575\n","Estimator 227/300, Train metric: 110.4318\n","Estimator 228/300, Train metric: 110.0983\n","Estimator 229/300, Train metric: 109.8953\n","Estimator 230/300, Train metric: 109.7908\n","Estimator 231/300, Train metric: 109.7809\n","Estimator 232/300, Train metric: 109.6569\n","Estimator 233/300, Train metric: 109.5286\n","Estimator 234/300, Train metric: 109.3806\n","Estimator 235/300, Train metric: 109.2808\n","Estimator 236/300, Train metric: 109.1552\n","Estimator 237/300, Train metric: 108.7751\n","Estimator 238/300, Train metric: 108.6999\n","Estimator 239/300, Train metric: 108.4237\n","Estimator 240/300, Train metric: 108.3201\n","Estimator 241/300, Train metric: 108.0854\n","Estimator 242/300, Train metric: 107.7792\n","Estimator 243/300, Train metric: 107.6947\n","Estimator 244/300, Train metric: 107.4751\n","Estimator 245/300, Train metric: 107.3900\n","Estimator 246/300, Train metric: 107.2224\n","Estimator 247/300, Train metric: 107.1245\n","Estimator 248/300, Train metric: 106.7355\n","Estimator 249/300, Train metric: 106.5651\n","Estimator 250/300, Train metric: 106.4184\n","Estimator 251/300, Train metric: 106.4076\n","Estimator 252/300, Train metric: 106.0540\n","Estimator 253/300, Train metric: 105.8309\n","Estimator 254/300, Train metric: 105.6375\n","Estimator 255/300, Train metric: 105.4184\n","Estimator 256/300, Train metric: 105.2096\n","Estimator 257/300, Train metric: 104.9705\n","Estimator 258/300, Train metric: 104.8538\n","Estimator 259/300, Train metric: 104.7152\n","Estimator 260/300, Train metric: 104.6018\n","Estimator 261/300, Train metric: 104.5548\n","Estimator 262/300, Train metric: 104.4208\n","Estimator 263/300, Train metric: 104.2422\n","Estimator 264/300, Train metric: 103.9508\n","Estimator 265/300, Train metric: 103.8164\n","Estimator 266/300, Train metric: 103.4930\n","Estimator 267/300, Train metric: 103.4403\n","Estimator 268/300, Train metric: 103.2669\n","Estimator 269/300, Train metric: 103.0338\n","Estimator 270/300, Train metric: 103.0615\n","Estimator 271/300, Train metric: 102.6915\n","Estimator 272/300, Train metric: 102.5906\n","Estimator 273/300, Train metric: 102.4658\n","Estimator 274/300, Train metric: 102.4512\n","Estimator 275/300, Train metric: 102.2343\n","Estimator 276/300, Train metric: 101.9850\n","Estimator 277/300, Train metric: 101.8475\n","Estimator 278/300, Train metric: 101.7112\n","Estimator 279/300, Train metric: 101.5239\n","Estimator 280/300, Train metric: 101.4557\n","Estimator 281/300, Train metric: 101.1332\n","Estimator 282/300, Train metric: 100.8939\n","Estimator 283/300, Train metric: 100.8496\n","Estimator 284/300, Train metric: 100.5667\n","Estimator 285/300, Train metric: 100.2835\n","Estimator 286/300, Train metric: 100.0848\n","Estimator 287/300, Train metric: 99.9909\n","Estimator 288/300, Train metric: 99.9714\n","Estimator 289/300, Train metric: 99.8056\n","Estimator 290/300, Train metric: 99.4987\n","Estimator 291/300, Train metric: 99.3233\n","Estimator 292/300, Train metric: 99.2788\n","Estimator 293/300, Train metric: 99.1979\n","Estimator 294/300, Train metric: 99.0064\n","Estimator 295/300, Train metric: 98.9646\n","Estimator 296/300, Train metric: 98.8805\n","Estimator 297/300, Train metric: 98.7480\n","Estimator 298/300, Train metric: 98.7610\n","Estimator 299/300, Train metric: 98.7423\n","Best MSE for PGBM with PatientPruner: 7229.537620264676 with params: {'n_estimators': 300, 'learning_rate': 0.01, 'max_leaves': 57, 'min_split_gain': 0.5, 'reg_lambda': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 0.5, 'tree_correlation': 0.1, 'min_data_in_leaf': 5, 'max_bin': 64, 'distribution': 'laplace'}\n","Best RMSE for PGBM with PatientPruner: 85.02668769430382\n","Correlation Coefficient for PGBM with PatientPruner: 0.9061196029012977\n","Running Optuna for PGBM with PercentilePruner...\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Estimator 0/100, Train metric: 283.2789\n","Estimator 1/100, Train metric: 274.7249\n","Estimator 2/100, Train metric: 266.6605\n","Estimator 3/100, Train metric: 264.5526\n","Estimator 4/100, Train metric: 256.4441\n","Estimator 5/100, Train metric: 249.7788\n","Estimator 6/100, Train metric: 243.1408\n","Estimator 7/100, Train metric: 237.0713\n","Estimator 8/100, Train metric: 231.1911\n","Estimator 9/100, Train metric: 226.7611\n","Estimator 10/100, Train metric: 220.8126\n","Estimator 11/100, Train metric: 216.9722\n","Estimator 12/100, Train metric: 212.5389\n","Estimator 13/100, Train metric: 208.0546\n","Estimator 14/100, Train metric: 207.0995\n","Estimator 15/100, Train metric: 204.8756\n","Estimator 16/100, Train metric: 201.1416\n","Estimator 17/100, Train metric: 197.7974\n","Estimator 18/100, Train metric: 196.0499\n","Estimator 19/100, Train metric: 194.7532\n","Estimator 20/100, Train metric: 191.7551\n","Estimator 21/100, Train metric: 190.5891\n","Estimator 22/100, Train metric: 188.1384\n","Estimator 23/100, Train metric: 186.9058\n","Estimator 24/100, Train metric: 185.9200\n","Estimator 25/100, Train metric: 184.3935\n","Estimator 26/100, Train metric: 181.9410\n","Estimator 27/100, Train metric: 178.9471\n","Estimator 28/100, Train metric: 176.6205\n","Estimator 29/100, Train metric: 174.4198\n","Estimator 30/100, Train metric: 172.2815\n","Estimator 31/100, Train metric: 171.6238\n","Estimator 32/100, Train metric: 169.8057\n","Estimator 33/100, Train metric: 168.6348\n","Estimator 34/100, Train metric: 166.7183\n","Estimator 35/100, Train metric: 165.0312\n","Estimator 36/100, Train metric: 163.6214\n","Estimator 37/100, Train metric: 162.4073\n","Estimator 38/100, Train metric: 161.8899\n","Estimator 39/100, Train metric: 160.4842\n","Estimator 40/100, Train metric: 159.7661\n","Estimator 41/100, Train metric: 158.6692\n","Estimator 42/100, Train metric: 157.6540\n","Estimator 43/100, Train metric: 156.9366\n","Estimator 44/100, Train metric: 156.2513\n","Estimator 45/100, Train metric: 155.2439\n","Estimator 46/100, Train metric: 154.6308\n","Estimator 47/100, Train metric: 153.6693\n","Estimator 48/100, Train metric: 152.9416\n","Estimator 49/100, Train metric: 152.3548\n","Estimator 50/100, Train metric: 151.7274\n","Estimator 51/100, Train metric: 150.8772\n","Estimator 52/100, Train metric: 149.9322\n","Estimator 53/100, Train metric: 149.3561\n","Estimator 54/100, Train metric: 148.8704\n","Estimator 55/100, Train metric: 147.2859\n","Estimator 56/100, Train metric: 146.7009\n","Estimator 57/100, Train metric: 146.1242\n","Estimator 58/100, Train metric: 145.3678\n","Estimator 59/100, Train metric: 144.8675\n","Estimator 60/100, Train metric: 143.9929\n","Estimator 61/100, Train metric: 143.3358\n","Estimator 62/100, Train metric: 142.9857\n","Estimator 63/100, Train metric: 142.4270\n","Estimator 64/100, Train metric: 141.7160\n","Estimator 65/100, Train metric: 140.9714\n","Estimator 66/100, Train metric: 140.4508\n","Estimator 67/100, Train metric: 139.8785\n","Estimator 68/100, Train metric: 139.3792\n","Estimator 69/100, Train metric: 137.9762\n","Estimator 70/100, Train metric: 136.7716\n","Estimator 71/100, Train metric: 136.0069\n","Estimator 72/100, Train metric: 135.5813\n","Estimator 73/100, Train metric: 135.2473\n","Estimator 74/100, Train metric: 134.7750\n","Estimator 75/100, Train metric: 134.3967\n","Estimator 76/100, Train metric: 134.0068\n","Estimator 77/100, Train metric: 133.4385\n","Estimator 78/100, Train metric: 131.9779\n","Estimator 79/100, Train metric: 131.6630\n","Estimator 80/100, Train metric: 131.3334\n","Estimator 81/100, Train metric: 129.9832\n","Estimator 82/100, Train metric: 129.7167\n","Estimator 83/100, Train metric: 129.4944\n","Estimator 84/100, Train metric: 129.2568\n","Estimator 85/100, Train metric: 128.4975\n","Estimator 86/100, Train metric: 127.8931\n","Estimator 87/100, Train metric: 127.5437\n","Estimator 88/100, Train metric: 127.3777\n","Estimator 89/100, Train metric: 126.8009\n","Estimator 90/100, Train metric: 126.4712\n","Estimator 91/100, Train metric: 125.3019\n","Estimator 92/100, Train metric: 124.7268\n","Estimator 93/100, Train metric: 123.6739\n","Estimator 94/100, Train metric: 123.4836\n","Estimator 95/100, Train metric: 122.8418\n","Estimator 96/100, Train metric: 122.3866\n","Estimator 97/100, Train metric: 121.9135\n","Estimator 98/100, Train metric: 121.3799\n","Estimator 99/100, Train metric: 120.9492\n","Best MSE for PGBM with PercentilePruner: 8429.764862044874 with params: {'n_estimators': 100, 'learning_rate': 0.05, 'max_leaves': 57, 'min_split_gain': 1.0, 'reg_lambda': 1.0, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'tree_correlation': 0.3, 'min_data_in_leaf': 10, 'max_bin': 256, 'distribution': 'laplace'}\n","Best RMSE for PGBM with PercentilePruner: 91.81375094202869\n","Correlation Coefficient for PGBM with PercentilePruner: 0.8856542783464043\n","Running Optuna for PGBM with SuccessiveHalvingPruner...\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Estimator 0/300, Train metric: 279.9619\n","Estimator 1/300, Train metric: 274.7384\n","Estimator 2/300, Train metric: 271.8337\n","Estimator 3/300, Train metric: 268.8358\n","Estimator 4/300, Train metric: 264.8341\n","Estimator 5/300, Train metric: 259.9226\n","Estimator 6/300, Train metric: 256.4999\n","Estimator 7/300, Train metric: 252.4423\n","Estimator 8/300, Train metric: 249.7546\n","Estimator 9/300, Train metric: 246.3731\n","Estimator 10/300, Train metric: 241.7144\n","Estimator 11/300, Train metric: 238.7111\n","Estimator 12/300, Train metric: 235.5328\n","Estimator 13/300, Train metric: 233.1211\n","Estimator 14/300, Train metric: 229.6362\n","Estimator 15/300, Train metric: 226.8521\n","Estimator 16/300, Train metric: 223.5696\n","Estimator 17/300, Train metric: 219.6199\n","Estimator 18/300, Train metric: 216.0102\n","Estimator 19/300, Train metric: 214.5082\n","Estimator 20/300, Train metric: 211.6191\n","Estimator 21/300, Train metric: 208.9080\n","Estimator 22/300, Train metric: 207.5595\n","Estimator 23/300, Train metric: 205.0532\n","Estimator 24/300, Train metric: 203.5386\n","Estimator 25/300, Train metric: 201.8900\n","Estimator 26/300, Train metric: 200.0427\n","Estimator 27/300, Train metric: 197.5806\n","Estimator 28/300, Train metric: 195.6293\n","Estimator 29/300, Train metric: 193.6650\n","Estimator 30/300, Train metric: 190.8386\n","Estimator 31/300, Train metric: 189.0392\n","Estimator 32/300, Train metric: 186.5940\n","Estimator 33/300, Train metric: 184.7677\n","Estimator 34/300, Train metric: 184.0822\n","Estimator 35/300, Train metric: 182.2728\n","Estimator 36/300, Train metric: 181.5375\n","Estimator 37/300, Train metric: 178.8132\n","Estimator 38/300, Train metric: 177.9230\n","Estimator 39/300, Train metric: 176.9558\n","Estimator 40/300, Train metric: 176.5746\n","Estimator 41/300, Train metric: 174.0765\n","Estimator 42/300, Train metric: 173.3421\n","Estimator 43/300, Train metric: 170.9989\n","Estimator 44/300, Train metric: 170.0527\n","Estimator 45/300, Train metric: 169.0391\n","Estimator 46/300, Train metric: 167.2996\n","Estimator 47/300, Train metric: 165.7046\n","Estimator 48/300, Train metric: 163.8659\n","Estimator 49/300, Train metric: 162.4042\n","Estimator 50/300, Train metric: 160.9574\n","Estimator 51/300, Train metric: 158.6681\n","Estimator 52/300, Train metric: 158.0405\n","Estimator 53/300, Train metric: 156.8488\n","Estimator 54/300, Train metric: 156.0850\n","Estimator 55/300, Train metric: 155.4551\n","Estimator 56/300, Train metric: 154.0211\n","Estimator 57/300, Train metric: 153.3604\n","Estimator 58/300, Train metric: 152.0319\n","Estimator 59/300, Train metric: 150.8240\n","Estimator 60/300, Train metric: 150.3880\n","Estimator 61/300, Train metric: 149.7825\n","Estimator 62/300, Train metric: 148.0535\n","Estimator 63/300, Train metric: 146.9610\n","Estimator 64/300, Train metric: 146.4982\n","Estimator 65/300, Train metric: 145.5202\n","Estimator 66/300, Train metric: 144.2307\n","Estimator 67/300, Train metric: 143.0471\n","Estimator 68/300, Train metric: 142.8805\n","Estimator 69/300, Train metric: 141.6198\n","Estimator 70/300, Train metric: 141.2429\n","Estimator 71/300, Train metric: 140.1187\n","Estimator 72/300, Train metric: 139.0294\n","Estimator 73/300, Train metric: 138.4606\n","Estimator 74/300, Train metric: 138.0101\n","Estimator 75/300, Train metric: 137.3899\n","Estimator 76/300, Train metric: 135.7552\n","Estimator 77/300, Train metric: 135.1880\n","Estimator 78/300, Train metric: 134.0192\n","Estimator 79/300, Train metric: 133.1860\n","Estimator 80/300, Train metric: 132.9558\n","Estimator 81/300, Train metric: 132.1821\n","Estimator 82/300, Train metric: 131.6737\n","Estimator 83/300, Train metric: 131.3086\n","Estimator 84/300, Train metric: 130.5647\n","Estimator 85/300, Train metric: 129.3861\n","Estimator 86/300, Train metric: 128.3781\n","Estimator 87/300, Train metric: 127.8971\n","Estimator 88/300, Train metric: 127.3102\n","Estimator 89/300, Train metric: 126.7716\n","Estimator 90/300, Train metric: 126.3263\n","Estimator 91/300, Train metric: 125.9933\n","Estimator 92/300, Train metric: 124.8279\n","Estimator 93/300, Train metric: 124.0155\n","Estimator 94/300, Train metric: 123.3494\n","Estimator 95/300, Train metric: 122.6819\n","Estimator 96/300, Train metric: 121.8075\n","Estimator 97/300, Train metric: 121.6182\n","Estimator 98/300, Train metric: 121.1258\n","Estimator 99/300, Train metric: 120.6092\n","Estimator 100/300, Train metric: 119.4306\n","Estimator 101/300, Train metric: 118.6516\n","Estimator 102/300, Train metric: 118.0881\n","Estimator 103/300, Train metric: 117.6645\n","Estimator 104/300, Train metric: 117.5236\n","Estimator 105/300, Train metric: 117.0548\n","Estimator 106/300, Train metric: 116.8763\n","Estimator 107/300, Train metric: 116.2769\n","Estimator 108/300, Train metric: 115.4728\n","Estimator 109/300, Train metric: 115.3549\n","Estimator 110/300, Train metric: 114.7175\n","Estimator 111/300, Train metric: 114.3844\n","Estimator 112/300, Train metric: 114.1398\n","Estimator 113/300, Train metric: 113.2355\n","Estimator 114/300, Train metric: 112.9888\n","Estimator 115/300, Train metric: 112.7270\n","Estimator 116/300, Train metric: 112.3553\n","Estimator 117/300, Train metric: 111.9833\n","Estimator 118/300, Train metric: 111.7338\n","Estimator 119/300, Train metric: 110.9315\n","Estimator 120/300, Train metric: 110.3201\n","Estimator 121/300, Train metric: 109.8102\n","Estimator 122/300, Train metric: 108.8628\n","Estimator 123/300, Train metric: 108.9452\n","Estimator 124/300, Train metric: 108.8312\n","Estimator 125/300, Train metric: 108.4637\n","Estimator 126/300, Train metric: 108.0985\n","Estimator 127/300, Train metric: 107.2853\n","Estimator 128/300, Train metric: 106.7923\n","Estimator 129/300, Train metric: 106.5634\n","Estimator 130/300, Train metric: 106.2925\n","Estimator 131/300, Train metric: 106.1284\n","Estimator 132/300, Train metric: 105.6913\n","Estimator 133/300, Train metric: 105.1231\n","Estimator 134/300, Train metric: 104.5027\n","Estimator 135/300, Train metric: 104.2582\n","Estimator 136/300, Train metric: 103.9887\n","Estimator 137/300, Train metric: 103.6573\n","Estimator 138/300, Train metric: 103.2981\n","Estimator 139/300, Train metric: 102.8938\n","Estimator 140/300, Train metric: 102.6195\n","Estimator 141/300, Train metric: 101.8636\n","Estimator 142/300, Train metric: 101.7625\n","Estimator 143/300, Train metric: 101.5084\n","Estimator 144/300, Train metric: 101.5104\n","Estimator 145/300, Train metric: 101.1454\n","Estimator 146/300, Train metric: 100.7753\n","Estimator 147/300, Train metric: 100.3778\n","Estimator 148/300, Train metric: 100.4577\n","Estimator 149/300, Train metric: 100.5576\n","Estimator 150/300, Train metric: 99.8969\n","Estimator 151/300, Train metric: 99.6668\n","Estimator 152/300, Train metric: 99.2904\n","Estimator 153/300, Train metric: 98.7203\n","Estimator 154/300, Train metric: 98.1704\n","Estimator 155/300, Train metric: 98.1762\n","Estimator 156/300, Train metric: 97.7662\n","Estimator 157/300, Train metric: 97.5153\n","Estimator 158/300, Train metric: 97.1852\n","Estimator 159/300, Train metric: 96.7433\n","Estimator 160/300, Train metric: 96.5394\n","Estimator 161/300, Train metric: 96.2020\n","Estimator 162/300, Train metric: 95.9367\n","Estimator 163/300, Train metric: 95.5746\n","Estimator 164/300, Train metric: 95.3304\n","Estimator 165/300, Train metric: 95.0171\n","Estimator 166/300, Train metric: 94.7346\n","Estimator 167/300, Train metric: 94.5862\n","Estimator 168/300, Train metric: 94.3168\n","Estimator 169/300, Train metric: 93.8027\n","Estimator 170/300, Train metric: 93.3941\n","Estimator 171/300, Train metric: 93.0795\n","Estimator 172/300, Train metric: 92.8726\n","Estimator 173/300, Train metric: 92.6431\n","Estimator 174/300, Train metric: 92.2958\n","Estimator 175/300, Train metric: 92.2750\n","Estimator 176/300, Train metric: 92.1604\n","Estimator 177/300, Train metric: 92.1624\n","Estimator 178/300, Train metric: 92.1320\n","Estimator 179/300, Train metric: 92.0540\n","Estimator 180/300, Train metric: 91.8234\n","Estimator 181/300, Train metric: 91.7611\n","Estimator 182/300, Train metric: 91.5491\n","Estimator 183/300, Train metric: 91.3999\n","Estimator 184/300, Train metric: 91.2969\n","Estimator 185/300, Train metric: 91.0270\n","Estimator 186/300, Train metric: 90.6645\n","Estimator 187/300, Train metric: 90.5254\n","Estimator 188/300, Train metric: 90.3334\n","Estimator 189/300, Train metric: 90.0291\n","Estimator 190/300, Train metric: 90.0124\n","Estimator 191/300, Train metric: 89.6456\n","Estimator 192/300, Train metric: 89.2435\n","Estimator 193/300, Train metric: 89.1458\n","Estimator 194/300, Train metric: 88.8967\n","Estimator 195/300, Train metric: 88.6161\n","Estimator 196/300, Train metric: 88.3828\n","Estimator 197/300, Train metric: 88.2602\n","Estimator 198/300, Train metric: 88.0332\n","Estimator 199/300, Train metric: 87.8703\n","Estimator 200/300, Train metric: 87.5957\n","Estimator 201/300, Train metric: 87.4133\n","Estimator 202/300, Train metric: 87.2090\n","Estimator 203/300, Train metric: 86.7707\n","Estimator 204/300, Train metric: 86.2567\n","Estimator 205/300, Train metric: 86.1091\n","Estimator 206/300, Train metric: 86.0659\n","Estimator 207/300, Train metric: 85.8450\n","Estimator 208/300, Train metric: 85.7540\n","Estimator 209/300, Train metric: 85.5508\n","Estimator 210/300, Train metric: 85.5134\n","Estimator 211/300, Train metric: 85.4953\n","Estimator 212/300, Train metric: 85.1753\n","Estimator 213/300, Train metric: 84.8438\n","Estimator 214/300, Train metric: 84.4307\n","Estimator 215/300, Train metric: 84.1192\n","Estimator 216/300, Train metric: 84.0796\n","Estimator 217/300, Train metric: 83.9605\n","Estimator 218/300, Train metric: 83.8323\n","Estimator 219/300, Train metric: 83.6573\n","Estimator 220/300, Train metric: 83.4022\n","Estimator 221/300, Train metric: 83.3642\n","Estimator 222/300, Train metric: 83.3134\n","Estimator 223/300, Train metric: 83.0938\n","Estimator 224/300, Train metric: 82.9410\n","Estimator 225/300, Train metric: 82.9129\n","Estimator 226/300, Train metric: 82.7547\n","Estimator 227/300, Train metric: 82.7020\n","Estimator 228/300, Train metric: 82.4656\n","Estimator 229/300, Train metric: 82.2544\n","Estimator 230/300, Train metric: 82.1446\n","Estimator 231/300, Train metric: 82.1087\n","Estimator 232/300, Train metric: 81.9048\n","Estimator 233/300, Train metric: 81.7001\n","Estimator 234/300, Train metric: 81.4223\n","Estimator 235/300, Train metric: 81.1986\n","Estimator 236/300, Train metric: 80.9470\n","Estimator 237/300, Train metric: 80.7253\n","Estimator 238/300, Train metric: 80.6810\n","Estimator 239/300, Train metric: 80.4709\n","Estimator 240/300, Train metric: 80.3965\n","Estimator 241/300, Train metric: 80.2391\n","Estimator 242/300, Train metric: 80.0606\n","Estimator 243/300, Train metric: 79.9091\n","Estimator 244/300, Train metric: 79.7543\n","Estimator 245/300, Train metric: 79.6146\n","Estimator 246/300, Train metric: 79.5010\n","Estimator 247/300, Train metric: 79.4242\n","Estimator 248/300, Train metric: 79.0796\n","Estimator 249/300, Train metric: 78.9897\n","Estimator 250/300, Train metric: 78.7402\n","Estimator 251/300, Train metric: 78.6573\n","Estimator 252/300, Train metric: 78.3828\n","Estimator 253/300, Train metric: 78.2258\n","Estimator 254/300, Train metric: 78.0131\n","Estimator 255/300, Train metric: 77.9138\n","Estimator 256/300, Train metric: 77.6741\n","Estimator 257/300, Train metric: 77.3964\n","Estimator 258/300, Train metric: 77.1024\n","Estimator 259/300, Train metric: 76.8568\n","Estimator 260/300, Train metric: 76.5960\n","Estimator 261/300, Train metric: 76.5245\n","Estimator 262/300, Train metric: 76.3994\n","Estimator 263/300, Train metric: 76.1866\n","Estimator 264/300, Train metric: 75.9612\n","Estimator 265/300, Train metric: 75.9162\n","Estimator 266/300, Train metric: 75.6572\n","Estimator 267/300, Train metric: 75.5806\n","Estimator 268/300, Train metric: 75.4002\n","Estimator 269/300, Train metric: 75.2959\n","Estimator 270/300, Train metric: 75.2877\n","Estimator 271/300, Train metric: 74.9918\n","Estimator 272/300, Train metric: 74.7979\n","Estimator 273/300, Train metric: 74.6625\n","Estimator 274/300, Train metric: 74.5772\n","Estimator 275/300, Train metric: 74.4817\n","Estimator 276/300, Train metric: 74.3850\n","Estimator 277/300, Train metric: 74.2045\n","Estimator 278/300, Train metric: 74.1323\n","Estimator 279/300, Train metric: 73.8885\n","Estimator 280/300, Train metric: 73.7862\n","Estimator 281/300, Train metric: 73.5141\n","Estimator 282/300, Train metric: 73.3311\n","Estimator 283/300, Train metric: 73.3014\n","Estimator 284/300, Train metric: 73.0907\n","Estimator 285/300, Train metric: 72.8547\n","Estimator 286/300, Train metric: 72.7413\n","Estimator 287/300, Train metric: 72.6992\n","Estimator 288/300, Train metric: 72.6804\n","Estimator 289/300, Train metric: 72.5562\n","Estimator 290/300, Train metric: 72.3188\n","Estimator 291/300, Train metric: 72.1840\n","Estimator 292/300, Train metric: 72.1273\n","Estimator 293/300, Train metric: 72.0112\n","Estimator 294/300, Train metric: 71.7897\n","Estimator 295/300, Train metric: 71.7186\n","Estimator 296/300, Train metric: 71.5148\n","Estimator 297/300, Train metric: 71.4116\n","Estimator 298/300, Train metric: 71.3343\n","Estimator 299/300, Train metric: 71.2811\n","Best MSE for PGBM with SuccessiveHalvingPruner: 7802.516625036883 with params: {'n_estimators': 300, 'learning_rate': 0.05, 'max_leaves': 22, 'min_split_gain': 1.0, 'reg_lambda': 10.0, 'feature_fraction': 1.0, 'bagging_fraction': 0.5, 'tree_correlation': 0.1, 'min_data_in_leaf': 5, 'max_bin': 128, 'distribution': 'laplace'}\n","Best RMSE for PGBM with SuccessiveHalvingPruner: 88.33185509790272\n","Correlation Coefficient for PGBM with SuccessiveHalvingPruner: 0.8940232734188226\n","Running Optuna for PGBM with HyperbandPruner...\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Estimator 0/300, Train metric: 277.9559\n","Estimator 1/300, Train metric: 270.9930\n","Estimator 2/300, Train metric: 266.9844\n","Estimator 3/300, Train metric: 263.1708\n","Estimator 4/300, Train metric: 257.6519\n","Estimator 5/300, Train metric: 251.1423\n","Estimator 6/300, Train metric: 246.5153\n","Estimator 7/300, Train metric: 241.0060\n","Estimator 8/300, Train metric: 237.3760\n","Estimator 9/300, Train metric: 232.8411\n","Estimator 10/300, Train metric: 227.0559\n","Estimator 11/300, Train metric: 223.0934\n","Estimator 12/300, Train metric: 219.0363\n","Estimator 13/300, Train metric: 216.0978\n","Estimator 14/300, Train metric: 211.5512\n","Estimator 15/300, Train metric: 208.0889\n","Estimator 16/300, Train metric: 204.0421\n","Estimator 17/300, Train metric: 199.3652\n","Estimator 18/300, Train metric: 195.1002\n","Estimator 19/300, Train metric: 193.3717\n","Estimator 20/300, Train metric: 189.7887\n","Estimator 21/300, Train metric: 186.5019\n","Estimator 22/300, Train metric: 184.9253\n","Estimator 23/300, Train metric: 181.9828\n","Estimator 24/300, Train metric: 180.2865\n","Estimator 25/300, Train metric: 178.5849\n","Estimator 26/300, Train metric: 176.6012\n","Estimator 27/300, Train metric: 173.7000\n","Estimator 28/300, Train metric: 171.5749\n","Estimator 29/300, Train metric: 169.4270\n","Estimator 30/300, Train metric: 166.4213\n","Estimator 31/300, Train metric: 164.3708\n","Estimator 32/300, Train metric: 161.6521\n","Estimator 33/300, Train metric: 159.8191\n","Estimator 34/300, Train metric: 159.1494\n","Estimator 35/300, Train metric: 157.2410\n","Estimator 36/300, Train metric: 156.6591\n","Estimator 37/300, Train metric: 153.5072\n","Estimator 38/300, Train metric: 152.7448\n","Estimator 39/300, Train metric: 151.8248\n","Estimator 40/300, Train metric: 151.3036\n","Estimator 41/300, Train metric: 148.5052\n","Estimator 42/300, Train metric: 147.2189\n","Estimator 43/300, Train metric: 144.7470\n","Estimator 44/300, Train metric: 143.7054\n","Estimator 45/300, Train metric: 142.5737\n","Estimator 46/300, Train metric: 140.7956\n","Estimator 47/300, Train metric: 139.2872\n","Estimator 48/300, Train metric: 137.4787\n","Estimator 49/300, Train metric: 136.0628\n","Estimator 50/300, Train metric: 134.7884\n","Estimator 51/300, Train metric: 132.5520\n","Estimator 52/300, Train metric: 132.2409\n","Estimator 53/300, Train metric: 131.2566\n","Estimator 54/300, Train metric: 130.6944\n","Estimator 55/300, Train metric: 130.1287\n","Estimator 56/300, Train metric: 128.8971\n","Estimator 57/300, Train metric: 128.5030\n","Estimator 58/300, Train metric: 127.4017\n","Estimator 59/300, Train metric: 126.3705\n","Estimator 60/300, Train metric: 125.9499\n","Estimator 61/300, Train metric: 125.3826\n","Estimator 62/300, Train metric: 123.7349\n","Estimator 63/300, Train metric: 122.8452\n","Estimator 64/300, Train metric: 122.5455\n","Estimator 65/300, Train metric: 121.7650\n","Estimator 66/300, Train metric: 120.5854\n","Estimator 67/300, Train metric: 119.5305\n","Estimator 68/300, Train metric: 119.3756\n","Estimator 69/300, Train metric: 118.2549\n","Estimator 70/300, Train metric: 118.0193\n","Estimator 71/300, Train metric: 117.0525\n","Estimator 72/300, Train metric: 116.1241\n","Estimator 73/300, Train metric: 115.7444\n","Estimator 74/300, Train metric: 115.4847\n","Estimator 75/300, Train metric: 114.8035\n","Estimator 76/300, Train metric: 113.3731\n","Estimator 77/300, Train metric: 113.0085\n","Estimator 78/300, Train metric: 111.9077\n","Estimator 79/300, Train metric: 111.2363\n","Estimator 80/300, Train metric: 110.9029\n","Estimator 81/300, Train metric: 110.3117\n","Estimator 82/300, Train metric: 110.1348\n","Estimator 83/300, Train metric: 109.6049\n","Estimator 84/300, Train metric: 109.0624\n","Estimator 85/300, Train metric: 107.9159\n","Estimator 86/300, Train metric: 107.0902\n","Estimator 87/300, Train metric: 106.7736\n","Estimator 88/300, Train metric: 106.3657\n","Estimator 89/300, Train metric: 105.7950\n","Estimator 90/300, Train metric: 105.3268\n","Estimator 91/300, Train metric: 105.0888\n","Estimator 92/300, Train metric: 104.2346\n","Estimator 93/300, Train metric: 103.6614\n","Estimator 94/300, Train metric: 103.1956\n","Estimator 95/300, Train metric: 102.5528\n","Estimator 96/300, Train metric: 101.9102\n","Estimator 97/300, Train metric: 101.7522\n","Estimator 98/300, Train metric: 101.4267\n","Estimator 99/300, Train metric: 101.1044\n","Estimator 100/300, Train metric: 100.2697\n","Estimator 101/300, Train metric: 99.5739\n","Estimator 102/300, Train metric: 99.2893\n","Estimator 103/300, Train metric: 98.7661\n","Estimator 104/300, Train metric: 98.6466\n","Estimator 105/300, Train metric: 98.4174\n","Estimator 106/300, Train metric: 98.3623\n","Estimator 107/300, Train metric: 97.9370\n","Estimator 108/300, Train metric: 97.3081\n","Estimator 109/300, Train metric: 97.2271\n","Estimator 110/300, Train metric: 96.9327\n","Estimator 111/300, Train metric: 96.7515\n","Estimator 112/300, Train metric: 96.7141\n","Estimator 113/300, Train metric: 95.8567\n","Estimator 114/300, Train metric: 95.8132\n","Estimator 115/300, Train metric: 95.6864\n","Estimator 116/300, Train metric: 95.4365\n","Estimator 117/300, Train metric: 95.1900\n","Estimator 118/300, Train metric: 95.0665\n","Estimator 119/300, Train metric: 94.4381\n","Estimator 120/300, Train metric: 94.0946\n","Estimator 121/300, Train metric: 93.8250\n","Estimator 122/300, Train metric: 93.0806\n","Estimator 123/300, Train metric: 93.1595\n","Estimator 124/300, Train metric: 93.0161\n","Estimator 125/300, Train metric: 92.4330\n","Estimator 126/300, Train metric: 92.0742\n","Estimator 127/300, Train metric: 91.4186\n","Estimator 128/300, Train metric: 91.1253\n","Estimator 129/300, Train metric: 90.7952\n","Estimator 130/300, Train metric: 90.5239\n","Estimator 131/300, Train metric: 90.2774\n","Estimator 132/300, Train metric: 89.9996\n","Estimator 133/300, Train metric: 89.6511\n","Estimator 134/300, Train metric: 89.1916\n","Estimator 135/300, Train metric: 89.0104\n","Estimator 136/300, Train metric: 88.8218\n","Estimator 137/300, Train metric: 88.6138\n","Estimator 138/300, Train metric: 88.4174\n","Estimator 139/300, Train metric: 87.9448\n","Estimator 140/300, Train metric: 87.7119\n","Estimator 141/300, Train metric: 87.1472\n","Estimator 142/300, Train metric: 87.1048\n","Estimator 143/300, Train metric: 86.9876\n","Estimator 144/300, Train metric: 86.9113\n","Estimator 145/300, Train metric: 86.5975\n","Estimator 146/300, Train metric: 86.4337\n","Estimator 147/300, Train metric: 86.2411\n","Estimator 148/300, Train metric: 86.2430\n","Estimator 149/300, Train metric: 86.2565\n","Estimator 150/300, Train metric: 85.7633\n","Estimator 151/300, Train metric: 85.6225\n","Estimator 152/300, Train metric: 85.2887\n","Estimator 153/300, Train metric: 84.9093\n","Estimator 154/300, Train metric: 84.5053\n","Estimator 155/300, Train metric: 84.4100\n","Estimator 156/300, Train metric: 84.0901\n","Estimator 157/300, Train metric: 83.9140\n","Estimator 158/300, Train metric: 83.7531\n","Estimator 159/300, Train metric: 83.4317\n","Estimator 160/300, Train metric: 83.0678\n","Estimator 161/300, Train metric: 82.6381\n","Estimator 162/300, Train metric: 82.4168\n","Estimator 163/300, Train metric: 82.2565\n","Estimator 164/300, Train metric: 82.0616\n","Estimator 165/300, Train metric: 81.8854\n","Estimator 166/300, Train metric: 81.6082\n","Estimator 167/300, Train metric: 81.5601\n","Estimator 168/300, Train metric: 81.3216\n","Estimator 169/300, Train metric: 80.9745\n","Estimator 170/300, Train metric: 80.6493\n","Estimator 171/300, Train metric: 80.2951\n","Estimator 172/300, Train metric: 80.0786\n","Estimator 173/300, Train metric: 79.8345\n","Estimator 174/300, Train metric: 79.7192\n","Estimator 175/300, Train metric: 79.4675\n","Estimator 176/300, Train metric: 79.3442\n","Estimator 177/300, Train metric: 79.2357\n","Estimator 178/300, Train metric: 79.1829\n","Estimator 179/300, Train metric: 79.0638\n","Estimator 180/300, Train metric: 78.7436\n","Estimator 181/300, Train metric: 78.6571\n","Estimator 182/300, Train metric: 78.5972\n","Estimator 183/300, Train metric: 78.5224\n","Estimator 184/300, Train metric: 78.4551\n","Estimator 185/300, Train metric: 78.1562\n","Estimator 186/300, Train metric: 77.9427\n","Estimator 187/300, Train metric: 77.8377\n","Estimator 188/300, Train metric: 77.7265\n","Estimator 189/300, Train metric: 77.3661\n","Estimator 190/300, Train metric: 77.2869\n","Estimator 191/300, Train metric: 76.7925\n","Estimator 192/300, Train metric: 76.5924\n","Estimator 193/300, Train metric: 76.5421\n","Estimator 194/300, Train metric: 76.3179\n","Estimator 195/300, Train metric: 76.1195\n","Estimator 196/300, Train metric: 75.9883\n","Estimator 197/300, Train metric: 75.8434\n","Estimator 198/300, Train metric: 75.6161\n","Estimator 199/300, Train metric: 75.4434\n","Estimator 200/300, Train metric: 75.2586\n","Estimator 201/300, Train metric: 75.0696\n","Estimator 202/300, Train metric: 74.8956\n","Estimator 203/300, Train metric: 74.4888\n","Estimator 204/300, Train metric: 73.9818\n","Estimator 205/300, Train metric: 73.9139\n","Estimator 206/300, Train metric: 73.8529\n","Estimator 207/300, Train metric: 73.5579\n","Estimator 208/300, Train metric: 73.4369\n","Estimator 209/300, Train metric: 73.2443\n","Estimator 210/300, Train metric: 73.1917\n","Estimator 211/300, Train metric: 73.1497\n","Estimator 212/300, Train metric: 72.9223\n","Estimator 213/300, Train metric: 72.7164\n","Estimator 214/300, Train metric: 72.3793\n","Estimator 215/300, Train metric: 72.1578\n","Estimator 216/300, Train metric: 72.1313\n","Estimator 217/300, Train metric: 71.9731\n","Estimator 218/300, Train metric: 71.8645\n","Estimator 219/300, Train metric: 71.6333\n","Estimator 220/300, Train metric: 71.4926\n","Estimator 221/300, Train metric: 71.4121\n","Estimator 222/300, Train metric: 71.2942\n","Estimator 223/300, Train metric: 71.1445\n","Estimator 224/300, Train metric: 70.9550\n","Estimator 225/300, Train metric: 70.8690\n","Estimator 226/300, Train metric: 70.7689\n","Estimator 227/300, Train metric: 70.6341\n","Estimator 228/300, Train metric: 70.3716\n","Estimator 229/300, Train metric: 70.1621\n","Estimator 230/300, Train metric: 69.9921\n","Estimator 231/300, Train metric: 69.9297\n","Estimator 232/300, Train metric: 69.9447\n","Estimator 233/300, Train metric: 69.8347\n","Estimator 234/300, Train metric: 69.6590\n","Estimator 235/300, Train metric: 69.5265\n","Estimator 236/300, Train metric: 69.2255\n","Estimator 237/300, Train metric: 69.0393\n","Estimator 238/300, Train metric: 69.0134\n","Estimator 239/300, Train metric: 68.7379\n","Estimator 240/300, Train metric: 68.6615\n","Estimator 241/300, Train metric: 68.5964\n","Estimator 242/300, Train metric: 68.3786\n","Estimator 243/300, Train metric: 68.4050\n","Estimator 244/300, Train metric: 68.2260\n","Estimator 245/300, Train metric: 68.0583\n","Estimator 246/300, Train metric: 67.8375\n","Estimator 247/300, Train metric: 67.7515\n","Estimator 248/300, Train metric: 67.4324\n","Estimator 249/300, Train metric: 67.3272\n","Estimator 250/300, Train metric: 67.0900\n","Estimator 251/300, Train metric: 66.9739\n","Estimator 252/300, Train metric: 66.7751\n","Estimator 253/300, Train metric: 66.6561\n","Estimator 254/300, Train metric: 66.4045\n","Estimator 255/300, Train metric: 66.3287\n","Estimator 256/300, Train metric: 65.9892\n","Estimator 257/300, Train metric: 65.6223\n","Estimator 258/300, Train metric: 65.3047\n","Estimator 259/300, Train metric: 65.0145\n","Estimator 260/300, Train metric: 64.6978\n","Estimator 261/300, Train metric: 64.5570\n","Estimator 262/300, Train metric: 64.4728\n","Estimator 263/300, Train metric: 64.2362\n","Estimator 264/300, Train metric: 64.0116\n","Estimator 265/300, Train metric: 63.9394\n","Estimator 266/300, Train metric: 63.7290\n","Estimator 267/300, Train metric: 63.6105\n","Estimator 268/300, Train metric: 63.4597\n","Estimator 269/300, Train metric: 63.3956\n","Estimator 270/300, Train metric: 63.3577\n","Estimator 271/300, Train metric: 63.1110\n","Estimator 272/300, Train metric: 63.0551\n","Estimator 273/300, Train metric: 62.9142\n","Estimator 274/300, Train metric: 62.8949\n","Estimator 275/300, Train metric: 62.6799\n","Estimator 276/300, Train metric: 62.6026\n","Estimator 277/300, Train metric: 62.3565\n","Estimator 278/300, Train metric: 62.3028\n","Estimator 279/300, Train metric: 62.1187\n","Estimator 280/300, Train metric: 62.1091\n","Estimator 281/300, Train metric: 61.8619\n","Estimator 282/300, Train metric: 61.6355\n","Estimator 283/300, Train metric: 61.5919\n","Estimator 284/300, Train metric: 61.4598\n","Estimator 285/300, Train metric: 61.1936\n","Estimator 286/300, Train metric: 61.1486\n","Estimator 287/300, Train metric: 61.0937\n","Estimator 288/300, Train metric: 61.0754\n","Estimator 289/300, Train metric: 60.8502\n","Estimator 290/300, Train metric: 60.5370\n","Estimator 291/300, Train metric: 60.3766\n","Estimator 292/300, Train metric: 60.3203\n","Estimator 293/300, Train metric: 60.2509\n","Estimator 294/300, Train metric: 60.1330\n","Estimator 295/300, Train metric: 60.0551\n","Estimator 296/300, Train metric: 59.9773\n","Estimator 297/300, Train metric: 59.9202\n","Estimator 298/300, Train metric: 59.8448\n","Estimator 299/300, Train metric: 59.7507\n","Best MSE for PGBM with HyperbandPruner: 8439.536940989694 with params: {'n_estimators': 300, 'learning_rate': 0.05, 'max_leaves': 24, 'min_split_gain': 1.0, 'reg_lambda': 5.0, 'feature_fraction': 1.0, 'bagging_fraction': 0.5, 'tree_correlation': 0.0, 'min_data_in_leaf': 5, 'max_bin': 128, 'distribution': 'normal'}\n","Best RMSE for PGBM with HyperbandPruner: 91.86695238762248\n","Correlation Coefficient for PGBM with HyperbandPruner: 0.8850543851091954\n","Running Optuna for PGBM with ThresholdPruner...\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Estimator 0/300, Train metric: 283.2903\n","Estimator 1/300, Train metric: 281.8180\n","Estimator 2/300, Train metric: 281.0196\n","Estimator 3/300, Train metric: 279.7445\n","Estimator 4/300, Train metric: 278.3856\n","Estimator 5/300, Train metric: 276.9957\n","Estimator 6/300, Train metric: 275.7682\n","Estimator 7/300, Train metric: 274.4552\n","Estimator 8/300, Train metric: 273.3411\n","Estimator 9/300, Train metric: 272.1361\n","Estimator 10/300, Train metric: 270.7701\n","Estimator 11/300, Train metric: 269.7248\n","Estimator 12/300, Train metric: 268.4653\n","Estimator 13/300, Train metric: 267.4487\n","Estimator 14/300, Train metric: 266.2794\n","Estimator 15/300, Train metric: 265.1877\n","Estimator 16/300, Train metric: 263.9799\n","Estimator 17/300, Train metric: 262.7773\n","Estimator 18/300, Train metric: 261.5918\n","Estimator 19/300, Train metric: 260.4967\n","Estimator 20/300, Train metric: 259.3357\n","Estimator 21/300, Train metric: 258.1822\n","Estimator 22/300, Train metric: 256.9536\n","Estimator 23/300, Train metric: 255.8274\n","Estimator 24/300, Train metric: 254.7091\n","Estimator 25/300, Train metric: 253.5385\n","Estimator 26/300, Train metric: 252.4292\n","Estimator 27/300, Train metric: 251.3806\n","Estimator 28/300, Train metric: 250.2639\n","Estimator 29/300, Train metric: 249.2123\n","Estimator 30/300, Train metric: 248.1074\n","Estimator 31/300, Train metric: 246.9572\n","Estimator 32/300, Train metric: 245.9323\n","Estimator 33/300, Train metric: 244.9256\n","Estimator 34/300, Train metric: 244.1045\n","Estimator 35/300, Train metric: 243.3355\n","Estimator 36/300, Train metric: 242.3350\n","Estimator 37/300, Train metric: 241.2887\n","Estimator 38/300, Train metric: 240.4135\n","Estimator 39/300, Train metric: 239.4506\n","Estimator 40/300, Train metric: 238.6180\n","Estimator 41/300, Train metric: 237.6781\n","Estimator 42/300, Train metric: 236.9165\n","Estimator 43/300, Train metric: 236.0254\n","Estimator 44/300, Train metric: 235.2798\n","Estimator 45/300, Train metric: 234.3833\n","Estimator 46/300, Train metric: 233.5228\n","Estimator 47/300, Train metric: 232.6420\n","Estimator 48/300, Train metric: 231.7233\n","Estimator 49/300, Train metric: 230.8662\n","Estimator 50/300, Train metric: 230.0565\n","Estimator 51/300, Train metric: 229.1893\n","Estimator 52/300, Train metric: 228.3546\n","Estimator 53/300, Train metric: 227.6356\n","Estimator 54/300, Train metric: 226.7666\n","Estimator 55/300, Train metric: 225.9507\n","Estimator 56/300, Train metric: 225.1584\n","Estimator 57/300, Train metric: 224.4104\n","Estimator 58/300, Train metric: 223.4975\n","Estimator 59/300, Train metric: 222.6013\n","Estimator 60/300, Train metric: 221.8451\n","Estimator 61/300, Train metric: 221.1916\n","Estimator 62/300, Train metric: 220.5361\n","Estimator 63/300, Train metric: 219.6942\n","Estimator 64/300, Train metric: 219.0974\n","Estimator 65/300, Train metric: 218.5231\n","Estimator 66/300, Train metric: 217.8168\n","Estimator 67/300, Train metric: 217.1142\n","Estimator 68/300, Train metric: 216.3931\n","Estimator 69/300, Train metric: 215.7229\n","Estimator 70/300, Train metric: 215.0561\n","Estimator 71/300, Train metric: 214.3810\n","Estimator 72/300, Train metric: 213.6903\n","Estimator 73/300, Train metric: 213.0356\n","Estimator 74/300, Train metric: 212.6010\n","Estimator 75/300, Train metric: 212.1421\n","Estimator 76/300, Train metric: 211.5295\n","Estimator 77/300, Train metric: 210.8963\n","Estimator 78/300, Train metric: 210.2802\n","Estimator 79/300, Train metric: 209.7245\n","Estimator 80/300, Train metric: 209.1667\n","Estimator 81/300, Train metric: 208.5408\n","Estimator 82/300, Train metric: 208.0366\n","Estimator 83/300, Train metric: 207.4992\n","Estimator 84/300, Train metric: 206.9350\n","Estimator 85/300, Train metric: 206.4073\n","Estimator 86/300, Train metric: 205.8552\n","Estimator 87/300, Train metric: 205.2410\n","Estimator 88/300, Train metric: 204.6986\n","Estimator 89/300, Train metric: 204.1225\n","Estimator 90/300, Train metric: 203.6687\n","Estimator 91/300, Train metric: 203.4298\n","Estimator 92/300, Train metric: 202.9012\n","Estimator 93/300, Train metric: 202.3964\n","Estimator 94/300, Train metric: 201.8707\n","Estimator 95/300, Train metric: 201.3350\n","Estimator 96/300, Train metric: 200.8849\n","Estimator 97/300, Train metric: 200.4714\n","Estimator 98/300, Train metric: 199.9845\n","Estimator 99/300, Train metric: 199.4857\n","Estimator 100/300, Train metric: 199.0073\n","Estimator 101/300, Train metric: 198.4773\n","Estimator 102/300, Train metric: 198.0013\n","Estimator 103/300, Train metric: 197.4257\n","Estimator 104/300, Train metric: 196.9425\n","Estimator 105/300, Train metric: 196.4807\n","Estimator 106/300, Train metric: 196.0944\n","Estimator 107/300, Train metric: 195.5993\n","Estimator 108/300, Train metric: 195.1444\n","Estimator 109/300, Train metric: 194.7820\n","Estimator 110/300, Train metric: 194.2431\n","Estimator 111/300, Train metric: 193.7984\n","Estimator 112/300, Train metric: 193.5103\n","Estimator 113/300, Train metric: 193.1570\n","Estimator 114/300, Train metric: 192.7432\n","Estimator 115/300, Train metric: 192.2733\n","Estimator 116/300, Train metric: 191.9210\n","Estimator 117/300, Train metric: 191.6098\n","Estimator 118/300, Train metric: 191.2229\n","Estimator 119/300, Train metric: 190.6954\n","Estimator 120/300, Train metric: 190.2603\n","Estimator 121/300, Train metric: 189.8531\n","Estimator 122/300, Train metric: 189.4364\n","Estimator 123/300, Train metric: 189.0773\n","Estimator 124/300, Train metric: 188.7351\n","Estimator 125/300, Train metric: 188.4021\n","Estimator 126/300, Train metric: 187.9573\n","Estimator 127/300, Train metric: 187.6521\n","Estimator 128/300, Train metric: 187.3618\n","Estimator 129/300, Train metric: 186.9975\n","Estimator 130/300, Train metric: 186.6991\n","Estimator 131/300, Train metric: 186.4046\n","Estimator 132/300, Train metric: 186.0362\n","Estimator 133/300, Train metric: 185.7616\n","Estimator 134/300, Train metric: 185.4085\n","Estimator 135/300, Train metric: 184.9036\n","Estimator 136/300, Train metric: 184.6134\n","Estimator 137/300, Train metric: 184.2563\n","Estimator 138/300, Train metric: 183.9863\n","Estimator 139/300, Train metric: 183.6342\n","Estimator 140/300, Train metric: 183.3434\n","Estimator 141/300, Train metric: 183.0365\n","Estimator 142/300, Train metric: 182.7339\n","Estimator 143/300, Train metric: 182.4659\n","Estimator 144/300, Train metric: 182.1601\n","Estimator 145/300, Train metric: 181.8737\n","Estimator 146/300, Train metric: 181.5453\n","Estimator 147/300, Train metric: 181.2692\n","Estimator 148/300, Train metric: 181.0225\n","Estimator 149/300, Train metric: 180.8008\n","Estimator 150/300, Train metric: 180.5634\n","Estimator 151/300, Train metric: 180.2582\n","Estimator 152/300, Train metric: 180.0310\n","Estimator 153/300, Train metric: 179.7987\n","Estimator 154/300, Train metric: 179.5473\n","Estimator 155/300, Train metric: 179.3172\n","Estimator 156/300, Train metric: 179.0598\n","Estimator 157/300, Train metric: 178.7969\n","Estimator 158/300, Train metric: 178.5587\n","Estimator 159/300, Train metric: 178.3015\n","Estimator 160/300, Train metric: 177.9790\n","Estimator 161/300, Train metric: 177.7281\n","Estimator 162/300, Train metric: 177.4404\n","Estimator 163/300, Train metric: 177.2216\n","Estimator 164/300, Train metric: 177.0037\n","Estimator 165/300, Train metric: 176.7455\n","Estimator 166/300, Train metric: 176.5289\n","Estimator 167/300, Train metric: 176.3147\n","Estimator 168/300, Train metric: 176.0952\n","Estimator 169/300, Train metric: 175.8401\n","Estimator 170/300, Train metric: 175.6180\n","Estimator 171/300, Train metric: 175.4085\n","Estimator 172/300, Train metric: 175.2646\n","Estimator 173/300, Train metric: 175.0950\n","Estimator 174/300, Train metric: 174.8629\n","Estimator 175/300, Train metric: 174.6739\n","Estimator 176/300, Train metric: 174.4974\n","Estimator 177/300, Train metric: 174.3487\n","Estimator 178/300, Train metric: 174.2123\n","Estimator 179/300, Train metric: 174.0387\n","Estimator 180/300, Train metric: 173.8703\n","Estimator 181/300, Train metric: 173.6999\n","Estimator 182/300, Train metric: 173.5306\n","Estimator 183/300, Train metric: 173.3216\n","Estimator 184/300, Train metric: 173.1270\n","Estimator 185/300, Train metric: 172.8067\n","Estimator 186/300, Train metric: 172.6255\n","Estimator 187/300, Train metric: 172.3797\n","Estimator 188/300, Train metric: 172.2054\n","Estimator 189/300, Train metric: 172.0324\n","Estimator 190/300, Train metric: 171.8227\n","Estimator 191/300, Train metric: 171.6603\n","Estimator 192/300, Train metric: 171.4441\n","Estimator 193/300, Train metric: 171.2516\n","Estimator 194/300, Train metric: 170.8924\n","Estimator 195/300, Train metric: 170.7411\n","Estimator 196/300, Train metric: 170.5678\n","Estimator 197/300, Train metric: 170.3564\n","Estimator 198/300, Train metric: 170.1655\n","Estimator 199/300, Train metric: 170.0427\n","Estimator 200/300, Train metric: 169.8628\n","Estimator 201/300, Train metric: 169.6827\n","Estimator 202/300, Train metric: 169.5393\n","Estimator 203/300, Train metric: 169.3626\n","Estimator 204/300, Train metric: 169.2229\n","Estimator 205/300, Train metric: 169.0456\n","Estimator 206/300, Train metric: 168.8742\n","Estimator 207/300, Train metric: 168.5164\n","Estimator 208/300, Train metric: 168.4155\n","Estimator 209/300, Train metric: 168.2696\n","Estimator 210/300, Train metric: 168.1049\n","Estimator 211/300, Train metric: 167.9590\n","Estimator 212/300, Train metric: 167.8329\n","Estimator 213/300, Train metric: 167.7029\n","Estimator 214/300, Train metric: 167.5786\n","Estimator 215/300, Train metric: 167.4475\n","Estimator 216/300, Train metric: 167.3330\n","Estimator 217/300, Train metric: 167.0398\n","Estimator 218/300, Train metric: 166.7129\n","Estimator 219/300, Train metric: 166.5832\n","Estimator 220/300, Train metric: 166.1365\n","Estimator 221/300, Train metric: 165.9724\n","Estimator 222/300, Train metric: 165.8646\n","Estimator 223/300, Train metric: 165.6492\n","Estimator 224/300, Train metric: 165.5416\n","Estimator 225/300, Train metric: 165.4088\n","Estimator 226/300, Train metric: 165.1862\n","Estimator 227/300, Train metric: 165.0466\n","Estimator 228/300, Train metric: 164.8429\n","Estimator 229/300, Train metric: 164.7199\n","Estimator 230/300, Train metric: 164.5480\n","Estimator 231/300, Train metric: 164.4285\n","Estimator 232/300, Train metric: 164.0177\n","Estimator 233/300, Train metric: 163.8959\n","Estimator 234/300, Train metric: 163.5394\n","Estimator 235/300, Train metric: 163.4427\n","Estimator 236/300, Train metric: 163.3355\n","Estimator 237/300, Train metric: 163.1776\n","Estimator 238/300, Train metric: 163.0456\n","Estimator 239/300, Train metric: 162.7341\n","Estimator 240/300, Train metric: 162.6194\n","Estimator 241/300, Train metric: 162.5178\n","Estimator 242/300, Train metric: 162.4210\n","Estimator 243/300, Train metric: 162.3245\n","Estimator 244/300, Train metric: 162.2179\n","Estimator 245/300, Train metric: 161.8553\n","Estimator 246/300, Train metric: 161.7645\n","Estimator 247/300, Train metric: 161.4213\n","Estimator 248/300, Train metric: 161.3240\n","Estimator 249/300, Train metric: 160.9810\n","Estimator 250/300, Train metric: 160.8620\n","Estimator 251/300, Train metric: 160.7650\n","Estimator 252/300, Train metric: 160.6526\n","Estimator 253/300, Train metric: 160.5540\n","Estimator 254/300, Train metric: 160.4581\n","Estimator 255/300, Train metric: 160.1312\n","Estimator 256/300, Train metric: 159.8180\n","Estimator 257/300, Train metric: 159.7149\n","Estimator 258/300, Train metric: 159.5964\n","Estimator 259/300, Train metric: 159.4647\n","Estimator 260/300, Train metric: 159.1609\n","Estimator 261/300, Train metric: 159.0504\n","Estimator 262/300, Train metric: 158.9596\n","Estimator 263/300, Train metric: 158.7033\n","Estimator 264/300, Train metric: 158.6003\n","Estimator 265/300, Train metric: 158.4494\n","Estimator 266/300, Train metric: 158.3543\n","Estimator 267/300, Train metric: 158.2603\n","Estimator 268/300, Train metric: 158.1453\n","Estimator 269/300, Train metric: 158.0540\n","Estimator 270/300, Train metric: 157.9553\n","Estimator 271/300, Train metric: 157.8418\n","Estimator 272/300, Train metric: 157.5329\n","Estimator 273/300, Train metric: 157.2817\n","Estimator 274/300, Train metric: 157.1509\n","Estimator 275/300, Train metric: 157.0748\n","Estimator 276/300, Train metric: 156.9701\n","Estimator 277/300, Train metric: 156.6665\n","Estimator 278/300, Train metric: 156.4445\n","Estimator 279/300, Train metric: 156.3466\n","Estimator 280/300, Train metric: 156.0511\n","Estimator 281/300, Train metric: 155.9379\n","Estimator 282/300, Train metric: 155.6368\n","Estimator 283/300, Train metric: 155.3469\n","Estimator 284/300, Train metric: 155.2612\n","Estimator 285/300, Train metric: 154.9686\n","Estimator 286/300, Train metric: 154.8613\n","Estimator 287/300, Train metric: 154.5651\n","Estimator 288/300, Train metric: 154.3454\n","Estimator 289/300, Train metric: 154.2471\n","Estimator 290/300, Train metric: 154.1539\n","Estimator 291/300, Train metric: 154.0633\n","Estimator 292/300, Train metric: 153.7661\n","Estimator 293/300, Train metric: 153.5914\n","Estimator 294/300, Train metric: 153.5229\n","Estimator 295/300, Train metric: 153.2599\n","Estimator 296/300, Train metric: 153.0456\n","Estimator 297/300, Train metric: 152.8501\n","Estimator 298/300, Train metric: 152.5665\n","Estimator 299/300, Train metric: 152.2924\n","Best MSE for PGBM with ThresholdPruner: 8068.997578574049 with params: {'n_estimators': 300, 'learning_rate': 0.01, 'max_leaves': 46, 'min_split_gain': 1.0, 'reg_lambda': 5.0, 'feature_fraction': 1.0, 'bagging_fraction': 0.9, 'tree_correlation': 0.3, 'min_data_in_leaf': 10, 'max_bin': 256, 'distribution': 'normal'}\n","Best RMSE for PGBM with ThresholdPruner: 89.82759920299578\n","Correlation Coefficient for PGBM with ThresholdPruner: 0.8982481810462295\n","Running Optuna for PGBM with WilcoxonPruner...\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Training on CPU\n","Estimator 0/500, Train metric: 282.4330\n","Estimator 1/500, Train metric: 280.3784\n","Estimator 2/500, Train metric: 279.0483\n","Estimator 3/500, Train metric: 277.8898\n","Estimator 4/500, Train metric: 276.0705\n","Estimator 5/500, Train metric: 274.0338\n","Estimator 6/500, Train metric: 272.3429\n","Estimator 7/500, Train metric: 270.3048\n","Estimator 8/500, Train metric: 268.9194\n","Estimator 9/500, Train metric: 267.1427\n","Estimator 10/500, Train metric: 265.1182\n","Estimator 11/500, Train metric: 263.4573\n","Estimator 12/500, Train metric: 261.8239\n","Estimator 13/500, Train metric: 260.5814\n","Estimator 14/500, Train metric: 258.5765\n","Estimator 15/500, Train metric: 256.9776\n","Estimator 16/500, Train metric: 255.1920\n","Estimator 17/500, Train metric: 253.3718\n","Estimator 18/500, Train metric: 251.5037\n","Estimator 19/500, Train metric: 250.7055\n","Estimator 20/500, Train metric: 248.8921\n","Estimator 21/500, Train metric: 247.2985\n","Estimator 22/500, Train metric: 246.3251\n","Estimator 23/500, Train metric: 244.7406\n","Estimator 24/500, Train metric: 243.7149\n","Estimator 25/500, Train metric: 242.5533\n","Estimator 26/500, Train metric: 241.3345\n","Estimator 27/500, Train metric: 239.7476\n","Estimator 28/500, Train metric: 238.3302\n","Estimator 29/500, Train metric: 236.9440\n","Estimator 30/500, Train metric: 235.2675\n","Estimator 31/500, Train metric: 233.9428\n","Estimator 32/500, Train metric: 232.2681\n","Estimator 33/500, Train metric: 231.0605\n","Estimator 34/500, Train metric: 230.6832\n","Estimator 35/500, Train metric: 229.4227\n","Estimator 36/500, Train metric: 228.6875\n","Estimator 37/500, Train metric: 226.9577\n","Estimator 38/500, Train metric: 226.1358\n","Estimator 39/500, Train metric: 225.4156\n","Estimator 40/500, Train metric: 224.9842\n","Estimator 41/500, Train metric: 223.3250\n","Estimator 42/500, Train metric: 222.4249\n","Estimator 43/500, Train metric: 220.7485\n","Estimator 44/500, Train metric: 219.9945\n","Estimator 45/500, Train metric: 219.2845\n","Estimator 46/500, Train metric: 217.8983\n","Estimator 47/500, Train metric: 216.5624\n","Estimator 48/500, Train metric: 215.1752\n","Estimator 49/500, Train metric: 214.1210\n","Estimator 50/500, Train metric: 212.8845\n","Estimator 51/500, Train metric: 211.2332\n","Estimator 52/500, Train metric: 210.4848\n","Estimator 53/500, Train metric: 209.3602\n","Estimator 54/500, Train metric: 208.5429\n","Estimator 55/500, Train metric: 207.8328\n","Estimator 56/500, Train metric: 206.5127\n","Estimator 57/500, Train metric: 205.8064\n","Estimator 58/500, Train metric: 204.5620\n","Estimator 59/500, Train metric: 203.4267\n","Estimator 60/500, Train metric: 202.8187\n","Estimator 61/500, Train metric: 202.1671\n","Estimator 62/500, Train metric: 200.8629\n","Estimator 63/500, Train metric: 199.9080\n","Estimator 64/500, Train metric: 199.1812\n","Estimator 65/500, Train metric: 198.1684\n","Estimator 66/500, Train metric: 196.9675\n","Estimator 67/500, Train metric: 195.8092\n","Estimator 68/500, Train metric: 195.1931\n","Estimator 69/500, Train metric: 194.0913\n","Estimator 70/500, Train metric: 193.5920\n","Estimator 71/500, Train metric: 192.4383\n","Estimator 72/500, Train metric: 191.3511\n","Estimator 73/500, Train metric: 190.7308\n","Estimator 74/500, Train metric: 190.1925\n","Estimator 75/500, Train metric: 189.3550\n","Estimator 76/500, Train metric: 188.0982\n","Estimator 77/500, Train metric: 187.4320\n","Estimator 78/500, Train metric: 186.2646\n","Estimator 79/500, Train metric: 185.7153\n","Estimator 80/500, Train metric: 185.2681\n","Estimator 81/500, Train metric: 184.3107\n","Estimator 82/500, Train metric: 183.7781\n","Estimator 83/500, Train metric: 183.2820\n","Estimator 84/500, Train metric: 182.3927\n","Estimator 85/500, Train metric: 181.3924\n","Estimator 86/500, Train metric: 180.4736\n","Estimator 87/500, Train metric: 179.8522\n","Estimator 88/500, Train metric: 179.2839\n","Estimator 89/500, Train metric: 178.7943\n","Estimator 90/500, Train metric: 178.2251\n","Estimator 91/500, Train metric: 177.6170\n","Estimator 92/500, Train metric: 176.4323\n","Estimator 93/500, Train metric: 175.4730\n","Estimator 94/500, Train metric: 174.5829\n","Estimator 95/500, Train metric: 173.9764\n","Estimator 96/500, Train metric: 172.9660\n","Estimator 97/500, Train metric: 172.6819\n","Estimator 98/500, Train metric: 171.9602\n","Estimator 99/500, Train metric: 171.2588\n","Estimator 100/500, Train metric: 170.0843\n","Estimator 101/500, Train metric: 169.3436\n","Estimator 102/500, Train metric: 168.5130\n","Estimator 103/500, Train metric: 168.1118\n","Estimator 104/500, Train metric: 167.7760\n","Estimator 105/500, Train metric: 167.1181\n","Estimator 106/500, Train metric: 166.9720\n","Estimator 107/500, Train metric: 166.1823\n","Estimator 108/500, Train metric: 165.1711\n","Estimator 109/500, Train metric: 164.7861\n","Estimator 110/500, Train metric: 163.9788\n","Estimator 111/500, Train metric: 163.6349\n","Estimator 112/500, Train metric: 163.1516\n","Estimator 113/500, Train metric: 162.2607\n","Estimator 114/500, Train metric: 162.1298\n","Estimator 115/500, Train metric: 161.6505\n","Estimator 116/500, Train metric: 161.0398\n","Estimator 117/500, Train metric: 160.4811\n","Estimator 118/500, Train metric: 160.1146\n","Estimator 119/500, Train metric: 159.2273\n","Estimator 120/500, Train metric: 158.3570\n","Estimator 121/500, Train metric: 157.8378\n","Estimator 122/500, Train metric: 156.8864\n","Estimator 123/500, Train metric: 156.6932\n","Estimator 124/500, Train metric: 156.2609\n","Estimator 125/500, Train metric: 155.6027\n","Estimator 126/500, Train metric: 155.0002\n","Estimator 127/500, Train metric: 154.1537\n","Estimator 128/500, Train metric: 153.3770\n","Estimator 129/500, Train metric: 152.9916\n","Estimator 130/500, Train metric: 152.4800\n","Estimator 131/500, Train metric: 152.0359\n","Estimator 132/500, Train metric: 151.4070\n","Estimator 133/500, Train metric: 150.6251\n","Estimator 134/500, Train metric: 149.7496\n","Estimator 135/500, Train metric: 149.2710\n","Estimator 136/500, Train metric: 148.9344\n","Estimator 137/500, Train metric: 148.4722\n","Estimator 138/500, Train metric: 147.8041\n","Estimator 139/500, Train metric: 147.1259\n","Estimator 140/500, Train metric: 146.7771\n","Estimator 141/500, Train metric: 145.9264\n","Estimator 142/500, Train metric: 145.7037\n","Estimator 143/500, Train metric: 145.2585\n","Estimator 144/500, Train metric: 145.1226\n","Estimator 145/500, Train metric: 144.7113\n","Estimator 146/500, Train metric: 144.4639\n","Estimator 147/500, Train metric: 143.7172\n","Estimator 148/500, Train metric: 143.6500\n","Estimator 149/500, Train metric: 143.5652\n","Estimator 150/500, Train metric: 142.7701\n","Estimator 151/500, Train metric: 142.4599\n","Estimator 152/500, Train metric: 141.9772\n","Estimator 153/500, Train metric: 141.1561\n","Estimator 154/500, Train metric: 140.4824\n","Estimator 155/500, Train metric: 140.3412\n","Estimator 156/500, Train metric: 140.0428\n","Estimator 157/500, Train metric: 139.6816\n","Estimator 158/500, Train metric: 139.0494\n","Estimator 159/500, Train metric: 138.4506\n","Estimator 160/500, Train metric: 137.9115\n","Estimator 161/500, Train metric: 137.6754\n","Estimator 162/500, Train metric: 137.3244\n","Estimator 163/500, Train metric: 136.6542\n","Estimator 164/500, Train metric: 136.3638\n","Estimator 165/500, Train metric: 135.7348\n","Estimator 166/500, Train metric: 135.4039\n","Estimator 167/500, Train metric: 135.1150\n","Estimator 168/500, Train metric: 134.9663\n","Estimator 169/500, Train metric: 134.2799\n","Estimator 170/500, Train metric: 133.8484\n","Estimator 171/500, Train metric: 133.5641\n","Estimator 172/500, Train metric: 133.2200\n","Estimator 173/500, Train metric: 132.9902\n","Estimator 174/500, Train metric: 132.3859\n","Estimator 175/500, Train metric: 132.2242\n","Estimator 176/500, Train metric: 132.0516\n","Estimator 177/500, Train metric: 131.9660\n","Estimator 178/500, Train metric: 131.6503\n","Estimator 179/500, Train metric: 131.5397\n","Estimator 180/500, Train metric: 131.3442\n","Estimator 181/500, Train metric: 131.0374\n","Estimator 182/500, Train metric: 130.7811\n","Estimator 183/500, Train metric: 130.5352\n","Estimator 184/500, Train metric: 130.4137\n","Estimator 185/500, Train metric: 130.1837\n","Estimator 186/500, Train metric: 129.5298\n","Estimator 187/500, Train metric: 129.4278\n","Estimator 188/500, Train metric: 128.9426\n","Estimator 189/500, Train metric: 128.7338\n","Estimator 190/500, Train metric: 128.5813\n","Estimator 191/500, Train metric: 128.1527\n","Estimator 192/500, Train metric: 127.7314\n","Estimator 193/500, Train metric: 127.5499\n","Estimator 194/500, Train metric: 127.4072\n","Estimator 195/500, Train metric: 127.0373\n","Estimator 196/500, Train metric: 126.4634\n","Estimator 197/500, Train metric: 126.0944\n","Estimator 198/500, Train metric: 125.8090\n","Estimator 199/500, Train metric: 125.7114\n","Estimator 200/500, Train metric: 125.1615\n","Estimator 201/500, Train metric: 124.9275\n","Estimator 202/500, Train metric: 124.7706\n","Estimator 203/500, Train metric: 124.2760\n","Estimator 204/500, Train metric: 123.7175\n","Estimator 205/500, Train metric: 123.3730\n","Estimator 206/500, Train metric: 123.1472\n","Estimator 207/500, Train metric: 122.7200\n","Estimator 208/500, Train metric: 122.6315\n","Estimator 209/500, Train metric: 122.4347\n","Estimator 210/500, Train metric: 122.2158\n","Estimator 211/500, Train metric: 122.0698\n","Estimator 212/500, Train metric: 121.5431\n","Estimator 213/500, Train metric: 121.0351\n","Estimator 214/500, Train metric: 120.5926\n","Estimator 215/500, Train metric: 120.1086\n","Estimator 216/500, Train metric: 120.0243\n","Estimator 217/500, Train metric: 119.8429\n","Estimator 218/500, Train metric: 119.6737\n","Estimator 219/500, Train metric: 119.5209\n","Estimator 220/500, Train metric: 119.0948\n","Estimator 221/500, Train metric: 119.0164\n","Estimator 222/500, Train metric: 118.8933\n","Estimator 223/500, Train metric: 118.7923\n","Estimator 224/500, Train metric: 118.4157\n","Estimator 225/500, Train metric: 118.2663\n","Estimator 226/500, Train metric: 118.0620\n","Estimator 227/500, Train metric: 118.0303\n","Estimator 228/500, Train metric: 117.6682\n","Estimator 229/500, Train metric: 117.4321\n","Estimator 230/500, Train metric: 117.1895\n","Estimator 231/500, Train metric: 117.1068\n","Estimator 232/500, Train metric: 116.9608\n","Estimator 233/500, Train metric: 116.8171\n","Estimator 234/500, Train metric: 116.6514\n","Estimator 235/500, Train metric: 116.5122\n","Estimator 236/500, Train metric: 116.3391\n","Estimator 237/500, Train metric: 115.9311\n","Estimator 238/500, Train metric: 115.8654\n","Estimator 239/500, Train metric: 115.5546\n","Estimator 240/500, Train metric: 115.4389\n","Estimator 241/500, Train metric: 115.2760\n","Estimator 242/500, Train metric: 114.9365\n","Estimator 243/500, Train metric: 114.8434\n","Estimator 244/500, Train metric: 114.5973\n","Estimator 245/500, Train metric: 114.5069\n","Estimator 246/500, Train metric: 114.3069\n","Estimator 247/500, Train metric: 114.2032\n","Estimator 248/500, Train metric: 113.7832\n","Estimator 249/500, Train metric: 113.5749\n","Estimator 250/500, Train metric: 113.4050\n","Estimator 251/500, Train metric: 113.3848\n","Estimator 252/500, Train metric: 112.9918\n","Estimator 253/500, Train metric: 112.7419\n","Estimator 254/500, Train metric: 112.5245\n","Estimator 255/500, Train metric: 112.2527\n","Estimator 256/500, Train metric: 112.1503\n","Estimator 257/500, Train metric: 111.8818\n","Estimator 258/500, Train metric: 111.7404\n","Estimator 259/500, Train metric: 111.5872\n","Estimator 260/500, Train metric: 111.4536\n","Estimator 261/500, Train metric: 111.3967\n","Estimator 262/500, Train metric: 111.2309\n","Estimator 263/500, Train metric: 111.1125\n","Estimator 264/500, Train metric: 110.7797\n","Estimator 265/500, Train metric: 110.6202\n","Estimator 266/500, Train metric: 110.2640\n","Estimator 267/500, Train metric: 110.2276\n","Estimator 268/500, Train metric: 110.0215\n","Estimator 269/500, Train metric: 109.7319\n","Estimator 270/500, Train metric: 109.7559\n","Estimator 271/500, Train metric: 109.3537\n","Estimator 272/500, Train metric: 109.2379\n","Estimator 273/500, Train metric: 109.1052\n","Estimator 274/500, Train metric: 109.0913\n","Estimator 275/500, Train metric: 108.8458\n","Estimator 276/500, Train metric: 108.5574\n","Estimator 277/500, Train metric: 108.4176\n","Estimator 278/500, Train metric: 108.2484\n","Estimator 279/500, Train metric: 108.0387\n","Estimator 280/500, Train metric: 107.9407\n","Estimator 281/500, Train metric: 107.5830\n","Estimator 282/500, Train metric: 107.3237\n","Estimator 283/500, Train metric: 107.2518\n","Estimator 284/500, Train metric: 106.9192\n","Estimator 285/500, Train metric: 106.6260\n","Estimator 286/500, Train metric: 106.3979\n","Estimator 287/500, Train metric: 106.2911\n","Estimator 288/500, Train metric: 106.1136\n","Estimator 289/500, Train metric: 105.9602\n","Estimator 290/500, Train metric: 105.6371\n","Estimator 291/500, Train metric: 105.4601\n","Estimator 292/500, Train metric: 105.4382\n","Estimator 293/500, Train metric: 105.3689\n","Estimator 294/500, Train metric: 105.1346\n","Estimator 295/500, Train metric: 105.0797\n","Estimator 296/500, Train metric: 104.9787\n","Estimator 297/500, Train metric: 104.8050\n","Estimator 298/500, Train metric: 104.8224\n","Estimator 299/500, Train metric: 104.8131\n","Estimator 300/500, Train metric: 104.4149\n","Estimator 301/500, Train metric: 104.2273\n","Estimator 302/500, Train metric: 104.1762\n","Estimator 303/500, Train metric: 103.8057\n","Estimator 304/500, Train metric: 103.5980\n","Estimator 305/500, Train metric: 103.4880\n","Estimator 306/500, Train metric: 103.3169\n","Estimator 307/500, Train metric: 103.1658\n","Estimator 308/500, Train metric: 103.0193\n","Estimator 309/500, Train metric: 102.9965\n","Estimator 310/500, Train metric: 102.9870\n","Estimator 311/500, Train metric: 102.7016\n","Estimator 312/500, Train metric: 102.5278\n","Estimator 313/500, Train metric: 102.1558\n","Estimator 314/500, Train metric: 102.0784\n","Estimator 315/500, Train metric: 102.0050\n","Estimator 316/500, Train metric: 101.7731\n","Estimator 317/500, Train metric: 101.5378\n","Estimator 318/500, Train metric: 101.3456\n","Estimator 319/500, Train metric: 101.2242\n","Estimator 320/500, Train metric: 101.1253\n","Estimator 321/500, Train metric: 100.9254\n","Estimator 322/500, Train metric: 100.7872\n","Estimator 323/500, Train metric: 100.7429\n","Estimator 324/500, Train metric: 100.4574\n","Estimator 325/500, Train metric: 100.4414\n","Estimator 326/500, Train metric: 100.2828\n","Estimator 327/500, Train metric: 100.1765\n","Estimator 328/500, Train metric: 99.9313\n","Estimator 329/500, Train metric: 99.8453\n","Estimator 330/500, Train metric: 99.6920\n","Estimator 331/500, Train metric: 99.6247\n","Estimator 332/500, Train metric: 99.6027\n","Estimator 333/500, Train metric: 99.3749\n","Estimator 334/500, Train metric: 99.1678\n","Estimator 335/500, Train metric: 99.1071\n","Estimator 336/500, Train metric: 99.0248\n","Estimator 337/500, Train metric: 98.9400\n","Estimator 338/500, Train metric: 98.8697\n","Estimator 339/500, Train metric: 98.6500\n","Estimator 340/500, Train metric: 98.3881\n","Estimator 341/500, Train metric: 98.3202\n","Estimator 342/500, Train metric: 98.2049\n","Estimator 343/500, Train metric: 98.1895\n","Estimator 344/500, Train metric: 98.0954\n","Estimator 345/500, Train metric: 97.8871\n","Estimator 346/500, Train metric: 97.6830\n","Estimator 347/500, Train metric: 97.5312\n","Estimator 348/500, Train metric: 97.3905\n","Estimator 349/500, Train metric: 97.2106\n","Estimator 350/500, Train metric: 96.9650\n","Estimator 351/500, Train metric: 96.8599\n","Estimator 352/500, Train metric: 96.8381\n","Estimator 353/500, Train metric: 96.8464\n","Estimator 354/500, Train metric: 96.7982\n","Estimator 355/500, Train metric: 96.7352\n","Estimator 356/500, Train metric: 96.6025\n","Estimator 357/500, Train metric: 96.5274\n","Estimator 358/500, Train metric: 96.3213\n","Estimator 359/500, Train metric: 96.2245\n","Estimator 360/500, Train metric: 96.1337\n","Estimator 361/500, Train metric: 96.0494\n","Estimator 362/500, Train metric: 96.0274\n","Estimator 363/500, Train metric: 96.0436\n","Estimator 364/500, Train metric: 95.9081\n","Estimator 365/500, Train metric: 95.8159\n","Estimator 366/500, Train metric: 95.6782\n","Estimator 367/500, Train metric: 95.4813\n","Estimator 368/500, Train metric: 95.4865\n","Estimator 369/500, Train metric: 95.2513\n","Estimator 370/500, Train metric: 95.1719\n","Estimator 371/500, Train metric: 95.1402\n","Estimator 372/500, Train metric: 95.0243\n","Estimator 373/500, Train metric: 94.9399\n","Estimator 374/500, Train metric: 94.7148\n","Estimator 375/500, Train metric: 94.6577\n","Estimator 376/500, Train metric: 94.6209\n","Estimator 377/500, Train metric: 94.6107\n","Estimator 378/500, Train metric: 94.6188\n","Estimator 379/500, Train metric: 94.5301\n","Estimator 380/500, Train metric: 94.5091\n","Estimator 381/500, Train metric: 94.4735\n","Estimator 382/500, Train metric: 94.3849\n","Estimator 383/500, Train metric: 94.2005\n","Estimator 384/500, Train metric: 94.1267\n","Estimator 385/500, Train metric: 93.8896\n","Estimator 386/500, Train metric: 93.9083\n","Estimator 387/500, Train metric: 93.8072\n","Estimator 388/500, Train metric: 93.7467\n","Estimator 389/500, Train metric: 93.7135\n","Estimator 390/500, Train metric: 93.6030\n","Estimator 391/500, Train metric: 93.5118\n","Estimator 392/500, Train metric: 93.4628\n","Estimator 393/500, Train metric: 93.2597\n","Estimator 394/500, Train metric: 93.2077\n","Estimator 395/500, Train metric: 93.0698\n","Estimator 396/500, Train metric: 93.0704\n","Estimator 397/500, Train metric: 92.9792\n","Estimator 398/500, Train metric: 92.9759\n","Estimator 399/500, Train metric: 92.9422\n","Estimator 400/500, Train metric: 92.8927\n","Estimator 401/500, Train metric: 92.8400\n","Estimator 402/500, Train metric: 92.7847\n","Estimator 403/500, Train metric: 92.7263\n","Estimator 404/500, Train metric: 92.6563\n","Estimator 405/500, Train metric: 92.6427\n","Estimator 406/500, Train metric: 92.5428\n","Estimator 407/500, Train metric: 92.4688\n","Estimator 408/500, Train metric: 92.4322\n","Estimator 409/500, Train metric: 92.2632\n","Estimator 410/500, Train metric: 92.2869\n","Estimator 411/500, Train metric: 92.0953\n","Estimator 412/500, Train metric: 92.0438\n","Estimator 413/500, Train metric: 91.9660\n","Estimator 414/500, Train metric: 91.9279\n","Estimator 415/500, Train metric: 91.8385\n","Estimator 416/500, Train metric: 91.8298\n","Estimator 417/500, Train metric: 91.6786\n","Estimator 418/500, Train metric: 91.5165\n","Estimator 419/500, Train metric: 91.3414\n","Estimator 420/500, Train metric: 91.3097\n","Estimator 421/500, Train metric: 91.2730\n","Estimator 422/500, Train metric: 91.2165\n","Estimator 423/500, Train metric: 91.1070\n","Estimator 424/500, Train metric: 91.0048\n","Estimator 425/500, Train metric: 90.8498\n","Estimator 426/500, Train metric: 90.8383\n","Estimator 427/500, Train metric: 90.7321\n","Estimator 428/500, Train metric: 90.6546\n","Estimator 429/500, Train metric: 90.5978\n","Estimator 430/500, Train metric: 90.4437\n","Estimator 431/500, Train metric: 90.3700\n","Estimator 432/500, Train metric: 90.2933\n","Estimator 433/500, Train metric: 90.1945\n","Estimator 434/500, Train metric: 90.1669\n","Estimator 435/500, Train metric: 89.9200\n","Estimator 436/500, Train metric: 89.7795\n","Estimator 437/500, Train metric: 89.6690\n","Estimator 438/500, Train metric: 89.5490\n","Estimator 439/500, Train metric: 89.4818\n","Estimator 440/500, Train metric: 89.4629\n","Estimator 441/500, Train metric: 89.3473\n","Estimator 442/500, Train metric: 89.3123\n","Estimator 443/500, Train metric: 89.3063\n","Estimator 444/500, Train metric: 89.2720\n","Estimator 445/500, Train metric: 89.2321\n","Estimator 446/500, Train metric: 89.1592\n","Estimator 447/500, Train metric: 89.0795\n","Estimator 448/500, Train metric: 89.0662\n","Estimator 449/500, Train metric: 88.8717\n","Estimator 450/500, Train metric: 88.8632\n","Estimator 451/500, Train metric: 88.7989\n","Estimator 452/500, Train metric: 88.7173\n","Estimator 453/500, Train metric: 88.5569\n","Estimator 454/500, Train metric: 88.4159\n","Estimator 455/500, Train metric: 88.4035\n","Estimator 456/500, Train metric: 88.3860\n","Estimator 457/500, Train metric: 88.3593\n","Estimator 458/500, Train metric: 88.2844\n","Estimator 459/500, Train metric: 88.0725\n","Estimator 460/500, Train metric: 88.0505\n","Estimator 461/500, Train metric: 87.9150\n","Estimator 462/500, Train metric: 87.8898\n","Estimator 463/500, Train metric: 87.8234\n","Estimator 464/500, Train metric: 87.7764\n","Estimator 465/500, Train metric: 87.7074\n","Estimator 466/500, Train metric: 87.5411\n","Estimator 467/500, Train metric: 87.4730\n","Estimator 468/500, Train metric: 87.4313\n","Estimator 469/500, Train metric: 87.3511\n","Estimator 470/500, Train metric: 87.3048\n","Estimator 471/500, Train metric: 87.1545\n","Estimator 472/500, Train metric: 87.0946\n","Estimator 473/500, Train metric: 86.9918\n","Estimator 474/500, Train metric: 86.9690\n","Estimator 475/500, Train metric: 86.8463\n","Estimator 476/500, Train metric: 86.6445\n","Estimator 477/500, Train metric: 86.5438\n","Estimator 478/500, Train metric: 86.5226\n","Estimator 479/500, Train metric: 86.3716\n","Estimator 480/500, Train metric: 86.3674\n","Estimator 481/500, Train metric: 86.2415\n","Estimator 482/500, Train metric: 86.1104\n","Estimator 483/500, Train metric: 86.0212\n","Estimator 484/500, Train metric: 85.9810\n","Estimator 485/500, Train metric: 85.9519\n","Estimator 486/500, Train metric: 85.9170\n","Estimator 487/500, Train metric: 85.8290\n","Estimator 488/500, Train metric: 85.8167\n","Estimator 489/500, Train metric: 85.7461\n","Estimator 490/500, Train metric: 85.6029\n","Estimator 491/500, Train metric: 85.5831\n","Estimator 492/500, Train metric: 85.5341\n","Estimator 493/500, Train metric: 85.4489\n","Estimator 494/500, Train metric: 85.3531\n","Estimator 495/500, Train metric: 85.3272\n","Estimator 496/500, Train metric: 85.2743\n","Estimator 497/500, Train metric: 85.0770\n","Estimator 498/500, Train metric: 85.0247\n","Estimator 499/500, Train metric: 84.9697\n","Best MSE for PGBM with WilcoxonPruner: 7272.343466656127 with params: {'n_estimators': 500, 'learning_rate': 0.01, 'max_leaves': 40, 'min_split_gain': 0.5, 'reg_lambda': 1.0, 'feature_fraction': 1.0, 'bagging_fraction': 0.5, 'tree_correlation': 0.0, 'min_data_in_leaf': 5, 'max_bin': 64, 'distribution': 'studentt'}\n","Best RMSE for PGBM with WilcoxonPruner: 85.2780362499989\n","Correlation Coefficient for PGBM with WilcoxonPruner: 0.9014289079671919\n","Running Optuna for TabNet with MedianPruner...\n","\n","Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_mse = 38147.47656\n","\n","Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_mse = 22781.25\n","\n","Early stopping occurred at epoch 91 with best_epoch = 81 and best_val_0_mse = 12691.05957\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 54426.12891\n","\n","Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_mse = 30455.22461\n","\n","Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_mse = 26556.88086\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 32700.39648\n","\n","Early stopping occurred at epoch 91 with best_epoch = 81 and best_val_0_mse = 19904.54297\n","\n","Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_mse = 32754.59961\n","Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_val_0_mse = 15444.8252\n","\n","Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_mse = 16327.05469\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 14782.55957\n","\n","Early stopping occurred at epoch 85 with best_epoch = 75 and best_val_0_mse = 10446.55273\n","\n","Early stopping occurred at epoch 88 with best_epoch = 78 and best_val_0_mse = 12126.03027\n","\n","Early stopping occurred at epoch 88 with best_epoch = 78 and best_val_0_mse = 12126.03027\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 32903.37891\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 13014.93262\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 12076.47656\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 55431.67188\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 24537.22656\n","Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_mse = 26306.97656\n","\n","Early stopping occurred at epoch 79 with best_epoch = 69 and best_val_0_mse = 12914.01953\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 22942.94336\n","\n","Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_mse = 27158.31445\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 74567.05469\n","\n","Early stopping occurred at epoch 61 with best_epoch = 51 and best_val_0_mse = 14495.86328\n","Stop training because you reached max_epochs = 100 with best_epoch = 92 and best_val_0_mse = 17101.1582\n","\n","Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_mse = 24627.75781\n","Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_mse = 73597.41406\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 28620.12695\n","\n","Early stopping occurred at epoch 85 with best_epoch = 75 and best_val_0_mse = 11039.5459\n","\n","Early stopping occurred at epoch 85 with best_epoch = 75 and best_val_0_mse = 11039.5459\n","Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_val_0_mse = 13400.92871\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 31786.81055\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 72996.90625\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 21040.25586\n","\n","Early stopping occurred at epoch 85 with best_epoch = 75 and best_val_0_mse = 10446.55273\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 19960.77148\n","\n","Early stopping occurred at epoch 75 with best_epoch = 65 and best_val_0_mse = 19066.69336\n","Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_val_0_mse = 9295.72266\n","Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_mse = 14146.11328\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 22876.09375\n","\n","Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_mse = 22252.93164\n","\n","Early stopping occurred at epoch 85 with best_epoch = 75 and best_val_0_mse = 10446.55273\n","\n","Early stopping occurred at epoch 85 with best_epoch = 75 and best_val_0_mse = 10446.55273\n","\n","Early stopping occurred at epoch 85 with best_epoch = 75 and best_val_0_mse = 10446.55273\n","\n","Early stopping occurred at epoch 96 with best_epoch = 86 and best_val_0_mse = 12859.30566\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 10687.9375\n","\n","Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_mse = 46963.33203\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 69443.8125\n","Best MSE for TabNet with MedianPruner: 25581.421875 with params: {'n_d': 8, 'n_a': 16, 'n_steps': 10, 'gamma': 2.0, 'lambda_sparse': 0.001, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'sparsemax', 'n_shared': 1, 'n_independent': 3, 'scheduler_params': {'step_size': 10, 'gamma': 0.9}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'seed': 42, 'verbose': 0}\n","Best RMSE for TabNet with MedianPruner: 159.94193282250905\n","Correlation Coefficient for TabNet with MedianPruner: 0.8295406580371981\n","Running Optuna for TabNet with NopPruner...\n","Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_val_0_mse = 51353.95703\n","\n","Early stopping occurred at epoch 54 with best_epoch = 44 and best_val_0_mse = 10965.81348\n","\n","Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_mse = 61079.33984\n","\n","Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_mse = 23808.88867\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 49544.16016\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 29624.54102\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 47175.875\n","\n","Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_mse = 23841.36133\n","\n","Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_mse = 31288.09961\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 23357.66602\n","\n","Early stopping occurred at epoch 82 with best_epoch = 72 and best_val_0_mse = 13966.1377\n","\n","Early stopping occurred at epoch 82 with best_epoch = 72 and best_val_0_mse = 13966.1377\n","\n","Early stopping occurred at epoch 70 with best_epoch = 60 and best_val_0_mse = 9053.49512\n","\n","Early stopping occurred at epoch 65 with best_epoch = 55 and best_val_0_mse = 12873.3125\n","\n","Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_mse = 22701.88477\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 55381.04297\n","\n","Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_mse = 20395.0293\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 16460.26367\n","\n","Early stopping occurred at epoch 64 with best_epoch = 54 and best_val_0_mse = 12317.30078\n","\n","Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_mse = 24306.26758\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 20392.04492\n","\n","Early stopping occurred at epoch 64 with best_epoch = 54 and best_val_0_mse = 12317.30078\n","\n","Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_mse = 26254.52344\n","\n","Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_mse = 18950.84375\n","\n","Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_mse = 20777.20508\n","\n","Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_mse = 20439.07812\n","\n","Early stopping occurred at epoch 57 with best_epoch = 47 and best_val_0_mse = 67346.41406\n","\n","Early stopping occurred at epoch 54 with best_epoch = 44 and best_val_0_mse = 15699.02832\n","\n","Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_mse = 14014.32715\n","\n","Early stopping occurred at epoch 56 with best_epoch = 46 and best_val_0_mse = 15147.5459\n","\n","Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_mse = 18528.81445\n","\n","Early stopping occurred at epoch 64 with best_epoch = 54 and best_val_0_mse = 12317.30078\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 30185.59375\n","\n","Early stopping occurred at epoch 42 with best_epoch = 32 and best_val_0_mse = 16646.53711\n","\n","Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_mse = 25909.5957\n","\n","Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_mse = 27492.44531\n","\n","Early stopping occurred at epoch 77 with best_epoch = 67 and best_val_0_mse = 10188.95312\n","\n","Early stopping occurred at epoch 71 with best_epoch = 61 and best_val_0_mse = 12166.84375\n","\n","Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_mse = 36328.34766\n","\n","Early stopping occurred at epoch 71 with best_epoch = 61 and best_val_0_mse = 12166.84375\n","\n","Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mse = 25269.86133\n","\n","Early stopping occurred at epoch 77 with best_epoch = 67 and best_val_0_mse = 10188.95312\n","\n","Early stopping occurred at epoch 87 with best_epoch = 77 and best_val_0_mse = 11375.63672\n","\n","Early stopping occurred at epoch 61 with best_epoch = 51 and best_val_0_mse = 9619.11426\n","\n","Early stopping occurred at epoch 84 with best_epoch = 74 and best_val_0_mse = 19009.11523\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 37934.30859\n","\n","Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_mse = 10091.11719\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 14987.44141\n","\n","Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_mse = 22283.84375\n","\n","Early stopping occurred at epoch 68 with best_epoch = 58 and best_val_0_mse = 14685.14551\n","Best MSE for TabNet with NopPruner: 15667.0458984375 with params: {'n_d': 32, 'n_a': 16, 'n_steps': 10, 'gamma': 2.0, 'lambda_sparse': 0.001, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'entmax', 'n_shared': 1, 'n_independent': 2, 'scheduler_params': {'step_size': 10, 'gamma': 0.9}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'seed': 42, 'verbose': 0}\n","Best RMSE for TabNet with NopPruner: 125.16807060283985\n","Correlation Coefficient for TabNet with NopPruner: 0.8310132382622455\n","Running Optuna for TabNet with PatientPruner...\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 12400.96191\n","\n","Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_mse = 22640.46094\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 22338.78711\n","\n","Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_mse = 26958.67188\n","\n","Early stopping occurred at epoch 51 with best_epoch = 41 and best_val_0_mse = 12310.41016\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 21420.66406\n","\n","Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mse = 26011.4043\n","\n","Early stopping occurred at epoch 91 with best_epoch = 81 and best_val_0_mse = 14177.22949\n","\n","Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_mse = 19501.23828\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 81383.5625\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 17219.0293\n","\n","Early stopping occurred at epoch 52 with best_epoch = 42 and best_val_0_mse = 8723.93262\n","\n","Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_mse = 28218.17188\n","Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_mse = 11764.31641\n","\n","Early stopping occurred at epoch 87 with best_epoch = 77 and best_val_0_mse = 19970.18359\n","\n","Early stopping occurred at epoch 85 with best_epoch = 75 and best_val_0_mse = 8687.13184\n","\n","Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_mse = 13666.97656\n","\n","Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_mse = 19062.50195\n","\n","Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_mse = 25353.90625\n","\n","Early stopping occurred at epoch 56 with best_epoch = 46 and best_val_0_mse = 12669.40723\n","\n","Early stopping occurred at epoch 65 with best_epoch = 55 and best_val_0_mse = 11834.28516\n","\n","Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_mse = 19182.76953\n","\n","Early stopping occurred at epoch 59 with best_epoch = 49 and best_val_0_mse = 12056.15332\n","\n","Early stopping occurred at epoch 77 with best_epoch = 67 and best_val_0_mse = 14928.14551\n","Stop training because you reached max_epochs = 100 with best_epoch = 91 and best_val_0_mse = 10641.44727\n","Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_val_0_mse = 17491.91992\n","\n","Early stopping occurred at epoch 64 with best_epoch = 54 and best_val_0_mse = 19314.72852\n","\n","Early stopping occurred at epoch 54 with best_epoch = 44 and best_val_0_mse = 12864.34668\n","\n","Early stopping occurred at epoch 99 with best_epoch = 89 and best_val_0_mse = 15519.21973\n","\n","Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_mse = 54513.28125\n","\n","Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_mse = 22026.52148\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 29385.42188\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 56351.47266\n","\n","Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_mse = 12557.96777\n","\n","Early stopping occurred at epoch 84 with best_epoch = 74 and best_val_0_mse = 10612.83496\n","\n","Early stopping occurred at epoch 87 with best_epoch = 77 and best_val_0_mse = 8684.42969\n","\n","Early stopping occurred at epoch 87 with best_epoch = 77 and best_val_0_mse = 8684.42969\n","\n","Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_mse = 15838.59473\n","\n","Early stopping occurred at epoch 67 with best_epoch = 57 and best_val_0_mse = 14849.54199\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 30363.84375\n","\n","Early stopping occurred at epoch 59 with best_epoch = 49 and best_val_0_mse = 16350.43359\n","\n","Early stopping occurred at epoch 97 with best_epoch = 87 and best_val_0_mse = 9664.24219\n","\n","Early stopping occurred at epoch 60 with best_epoch = 50 and best_val_0_mse = 13925.3291\n","\n","Early stopping occurred at epoch 98 with best_epoch = 88 and best_val_0_mse = 12217.30859\n","Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_val_0_mse = 11222.69531\n","\n","Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_mse = 14000.125\n","\n","Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_mse = 18253.04492\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 19219.58594\n","\n","Early stopping occurred at epoch 84 with best_epoch = 74 and best_val_0_mse = 35700.72266\n","\n","Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_mse = 24030.25586\n","Best MSE for TabNet with PatientPruner: 13300.8408203125 with params: {'n_d': 16, 'n_a': 8, 'n_steps': 7, 'gamma': 2.0, 'lambda_sparse': 0.001, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'entmax', 'n_shared': 1, 'n_independent': 3, 'scheduler_params': {'step_size': 10, 'gamma': 0.9}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'seed': 42, 'verbose': 0}\n","Best RMSE for TabNet with PatientPruner: 115.32927130747207\n","Correlation Coefficient for TabNet with PatientPruner: 0.8445435050943526\n","Running Optuna for TabNet with PercentilePruner...\n","\n","Early stopping occurred at epoch 61 with best_epoch = 51 and best_val_0_mse = 16958.4043\n","\n","Early stopping occurred at epoch 93 with best_epoch = 83 and best_val_0_mse = 66816.10156\n","\n","Early stopping occurred at epoch 70 with best_epoch = 60 and best_val_0_mse = 17838.89258\n","\n","Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_mse = 80512.32031\n","\n","Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_mse = 23833.57227\n","\n","Early stopping occurred at epoch 61 with best_epoch = 51 and best_val_0_mse = 11415.20215\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 53873.65625\n","\n","Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_mse = 31012.54688\n","\n","Early stopping occurred at epoch 56 with best_epoch = 46 and best_val_0_mse = 23133.30273\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 51861.23047\n","\n","Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_mse = 29919.02344\n","\n","Early stopping occurred at epoch 78 with best_epoch = 68 and best_val_0_mse = 33978.82422\n","\n","Early stopping occurred at epoch 59 with best_epoch = 49 and best_val_0_mse = 16463.75586\n","\n","Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_mse = 29198.27344\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 19738.24805\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 19082.44336\n","\n","Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_mse = 30677.79492\n","\n","Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_mse = 36122.39453\n","Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_mse = 12331.72363\n","Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_mse = 12331.72363\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 14812.14551\n","Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_mse = 12331.72363\n","\n","Early stopping occurred at epoch 96 with best_epoch = 86 and best_val_0_mse = 17668.14453\n","\n","Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_mse = 28612.19336\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 17681.6543\n","Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_val_0_mse = 20227.50977\n","\n","Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_mse = 9053.71191\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 9132.05957\n","\n","Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_mse = 9053.71191\n","\n","Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_mse = 9053.71191\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 16631.16211\n","\n","Early stopping occurred at epoch 86 with best_epoch = 76 and best_val_0_mse = 13064.03223\n","Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_mse = 28217.43555\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 52770.20312\n","Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_val_0_mse = 9620.79004\n","\n","Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_mse = 35916.63672\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 14476.34473\n","\n","Early stopping occurred at epoch 69 with best_epoch = 59 and best_val_0_mse = 10025.5459\n","Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_mse = 16208.12109\n","\n","Early stopping occurred at epoch 82 with best_epoch = 72 and best_val_0_mse = 36018.96484\n","\n","Early stopping occurred at epoch 65 with best_epoch = 55 and best_val_0_mse = 14029.30469\n","Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_val_0_mse = 9620.79004\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 12538.09082\n","\n","Early stopping occurred at epoch 84 with best_epoch = 74 and best_val_0_mse = 10838.60645\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 24107.76758\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 8645.19043\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 8645.19043\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 11371.21777\n","\n","Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_mse = 23674.37305\n","\n","Early stopping occurred at epoch 68 with best_epoch = 58 and best_val_0_mse = 11722.51172\n","Best MSE for TabNet with PercentilePruner: 11346.50390625 with params: {'n_d': 16, 'n_a': 64, 'n_steps': 10, 'gamma': 1.5, 'lambda_sparse': 0.01, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'sparsemax', 'n_shared': 2, 'n_independent': 1, 'scheduler_params': {'step_size': 10, 'gamma': 0.9}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'seed': 42, 'verbose': 0}\n","Best RMSE for TabNet with PercentilePruner: 106.51996951863063\n","Correlation Coefficient for TabNet with PercentilePruner: 0.8408986077476054\n","Running Optuna for TabNet with SuccessiveHalvingPruner...\n","\n","Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_mse = 26298.10938\n","\n","Early stopping occurred at epoch 84 with best_epoch = 74 and best_val_0_mse = 12476.07031\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 16590.23633\n","\n","Early stopping occurred at epoch 62 with best_epoch = 52 and best_val_0_mse = 19735.68555\n","\n","Early stopping occurred at epoch 87 with best_epoch = 77 and best_val_0_mse = 14947.85254\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 30586.14648\n","\n","Early stopping occurred at epoch 64 with best_epoch = 54 and best_val_0_mse = 78899.89062\n","\n","Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_mse = 78443.92188\n","\n","Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_mse = 17603.24414\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 18997.25195\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 41391.55859\n","\n","Early stopping occurred at epoch 78 with best_epoch = 68 and best_val_0_mse = 22016.89258\n","Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_val_0_mse = 27253.11523\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 30151.59961\n","\n","Early stopping occurred at epoch 45 with best_epoch = 35 and best_val_0_mse = 22577.5332\n","\n","Early stopping occurred at epoch 94 with best_epoch = 84 and best_val_0_mse = 18535.05664\n","\n","Early stopping occurred at epoch 50 with best_epoch = 40 and best_val_0_mse = 17242.94336\n","\n","Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_mse = 17464.96484\n","\n","Early stopping occurred at epoch 60 with best_epoch = 50 and best_val_0_mse = 37641.625\n","\n","Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_mse = 61400.34375\n","\n","Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_mse = 28758.81836\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 16590.23633\n","Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_mse = 11236.74707\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 38105.60938\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 18890.41406\n","Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_mse = 12150.51953\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 22209.57617\n","Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_mse = 12150.51953\n","\n","Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mse = 26579.20898\n","\n","Early stopping occurred at epoch 82 with best_epoch = 72 and best_val_0_mse = 19217.35352\n","\n","Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_mse = 30526.50977\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 12496.32422\n","\n","Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_mse = 36856.17578\n","Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_val_0_mse = 30275.54883\n","\n","Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mse = 22973.93945\n","\n","Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_mse = 19816.70508\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 18760.74414\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 12146.00488\n","\n","Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_mse = 17300.58203\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 36498.79297\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 15902.48828\n","\n","Early stopping occurred at epoch 78 with best_epoch = 68 and best_val_0_mse = 19575.3457\n","\n","Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_mse = 14601.84375\n","\n","Early stopping occurred at epoch 99 with best_epoch = 89 and best_val_0_mse = 19276.74023\n","\n","Early stopping occurred at epoch 84 with best_epoch = 74 and best_val_0_mse = 12476.07031\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 11176.71777\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 11176.71777\n","\n","Early stopping occurred at epoch 66 with best_epoch = 56 and best_val_0_mse = 21949.75\n","\n","Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_mse = 29523.34375\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 14327.63574\n","Best MSE for TabNet with SuccessiveHalvingPruner: 15933.2294921875 with params: {'n_d': 16, 'n_a': 16, 'n_steps': 10, 'gamma': 1.5, 'lambda_sparse': 0.0001, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'entmax', 'n_shared': 2, 'n_independent': 2, 'scheduler_params': {'step_size': 10, 'gamma': 0.9}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'seed': 42, 'verbose': 0}\n","Best RMSE for TabNet with SuccessiveHalvingPruner: 126.22689686507982\n","Correlation Coefficient for TabNet with SuccessiveHalvingPruner: 0.7867959767153399\n","Running Optuna for TabNet with HyperbandPruner...\n","\n","Early stopping occurred at epoch 60 with best_epoch = 50 and best_val_0_mse = 18738.48633\n","\n","Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_mse = 30818.29102\n","\n","Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_mse = 18345.75195\n","\n","Early stopping occurred at epoch 62 with best_epoch = 52 and best_val_0_mse = 16686.12695\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 37934.30859\n","\n","Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_mse = 47814.45703\n","\n","Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_mse = 22673.22461\n","Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_mse = 27136.92188\n","\n","Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_mse = 25858.0332\n","\n","Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_mse = 81819.5625\n","\n","Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_mse = 69470.35156\n","\n","Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mse = 22973.93945\n","\n","Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_mse = 14895.22949\n","\n","Early stopping occurred at epoch 50 with best_epoch = 40 and best_val_0_mse = 16533.17773\n","\n","Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_mse = 30954.83398\n","\n","Early stopping occurred at epoch 75 with best_epoch = 65 and best_val_0_mse = 13261.0752\n","\n","Early stopping occurred at epoch 56 with best_epoch = 46 and best_val_0_mse = 24765.11914\n","\n","Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_mse = 9273.40137\n","\n","Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_mse = 20584.41797\n","Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_val_0_mse = 14677.82031\n","\n","Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_mse = 16827.34961\n","\n","Early stopping occurred at epoch 55 with best_epoch = 45 and best_val_0_mse = 17599.17383\n","\n","Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_mse = 27357.99023\n","\n","Early stopping occurred at epoch 95 with best_epoch = 85 and best_val_0_mse = 21730.9668\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 34858.32031\n","\n","Early stopping occurred at epoch 75 with best_epoch = 65 and best_val_0_mse = 14411.78906\n","\n","Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_mse = 19804.03125\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 14324.65918\n","Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_mse = 18627.08203\n","\n","Early stopping occurred at epoch 50 with best_epoch = 40 and best_val_0_mse = 28709.07617\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 69829.39062\n","\n","Early stopping occurred at epoch 90 with best_epoch = 80 and best_val_0_mse = 19099.32227\n","\n","Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_mse = 28428.58398\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 19869.41602\n","\n","Early stopping occurred at epoch 62 with best_epoch = 52 and best_val_0_mse = 19515.76562\n","\n","Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_mse = 18527.90625\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 35153.79297\n","\n","Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_mse = 22632.84375\n","\n","Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_mse = 27996.98438\n","\n","Early stopping occurred at epoch 64 with best_epoch = 54 and best_val_0_mse = 17594.25391\n","\n","Early stopping occurred at epoch 82 with best_epoch = 72 and best_val_0_mse = 14934.5166\n","\n","Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_mse = 25430.00586\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 18562.01758\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 37300.33594\n","\n","Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_mse = 29841.76562\n","\n","Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_mse = 47820.59766\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 17265.37695\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 13097.95996\n","Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_val_0_mse = 18507.45508\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 13097.95996\n","Best MSE for TabNet with HyperbandPruner: 13764.0029296875 with params: {'n_d': 64, 'n_a': 64, 'n_steps': 7, 'gamma': 2.0, 'lambda_sparse': 0.001, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'sparsemax', 'n_shared': 2, 'n_independent': 1, 'scheduler_params': {'step_size': 10, 'gamma': 0.9}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'seed': 42, 'verbose': 0}\n","Best RMSE for TabNet with HyperbandPruner: 117.3200874943737\n","Correlation Coefficient for TabNet with HyperbandPruner: 0.8452309365722418\n","Running Optuna for TabNet with ThresholdPruner...\n","\n","Early stopping occurred at epoch 62 with best_epoch = 52 and best_val_0_mse = 25899.10938\n","\n","Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_mse = 19931.18945\n","\n","Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_mse = 56728.50391\n","\n","Early stopping occurred at epoch 55 with best_epoch = 45 and best_val_0_mse = 14669.66113\n","\n","Early stopping occurred at epoch 51 with best_epoch = 41 and best_val_0_mse = 18196.00977\n","\n","Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_mse = 74968.76562\n","\n","Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_mse = 34248.66797\n","\n","Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_mse = 15839.52734\n","\n","Early stopping occurred at epoch 68 with best_epoch = 58 and best_val_0_mse = 20581.82617\n","Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_mse = 14741.87207\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 23381.00977\n","\n","Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_mse = 28117.00195\n","Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_mse = 12135.91016\n","\n","Early stopping occurred at epoch 73 with best_epoch = 63 and best_val_0_mse = 45970.89062\n","\n","Early stopping occurred at epoch 99 with best_epoch = 89 and best_val_0_mse = 11365.59863\n","\n","Early stopping occurred at epoch 61 with best_epoch = 51 and best_val_0_mse = 11237.71094\n","\n","Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_mse = 24957.68164\n","\n","Early stopping occurred at epoch 82 with best_epoch = 72 and best_val_0_mse = 12328.6084\n","Stop training because you reached max_epochs = 100 with best_epoch = 92 and best_val_0_mse = 13038.00098\n","\n","Early stopping occurred at epoch 81 with best_epoch = 71 and best_val_0_mse = 23387.10938\n","\n","Early stopping occurred at epoch 65 with best_epoch = 55 and best_val_0_mse = 15421.33691\n","Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_val_0_mse = 21568.91406\n","\n","Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_mse = 30615.94531\n","\n","Early stopping occurred at epoch 76 with best_epoch = 66 and best_val_0_mse = 50495.81641\n","\n","Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_mse = 12327.05371\n","\n","Early stopping occurred at epoch 88 with best_epoch = 78 and best_val_0_mse = 9951.15234\n","\n","Early stopping occurred at epoch 87 with best_epoch = 77 and best_val_0_mse = 13155.70605\n","\n","Early stopping occurred at epoch 94 with best_epoch = 84 and best_val_0_mse = 28191.67969\n","\n","Early stopping occurred at epoch 95 with best_epoch = 85 and best_val_0_mse = 13529.95801\n","\n","Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_mse = 46932.29688\n","\n","Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_mse = 24952.83008\n","\n","Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_mse = 41807.43359\n","Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_mse = 13975.97559\n","\n","Early stopping occurred at epoch 61 with best_epoch = 51 and best_val_0_mse = 15740.84766\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 19780.98242\n","\n","Early stopping occurred at epoch 71 with best_epoch = 61 and best_val_0_mse = 14249.63281\n","\n","Early stopping occurred at epoch 97 with best_epoch = 87 and best_val_0_mse = 11846.38965\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 24806.57617\n","\n","Early stopping occurred at epoch 97 with best_epoch = 87 and best_val_0_mse = 11846.38965\n","\n","Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_mse = 22510.54883\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 32073.94336\n","Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_0_mse = 10496.09082\n","\n","Early stopping occurred at epoch 99 with best_epoch = 89 and best_val_0_mse = 33369.75391\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 67272.82031\n","\n","Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_mse = 22836.70117\n","\n","Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_mse = 11345.27441\n","\n","Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_mse = 11345.27441\n","\n","Early stopping occurred at epoch 88 with best_epoch = 78 and best_val_0_mse = 29419.31836\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 14310.53418\n","\n","Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_mse = 11345.27441\n","Best MSE for TabNet with ThresholdPruner: 28922.509765625 with params: {'n_d': 16, 'n_a': 16, 'n_steps': 10, 'gamma': 2.0, 'lambda_sparse': 0.0001, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'sparsemax', 'n_shared': 1, 'n_independent': 3, 'scheduler_params': {'step_size': 10, 'gamma': 0.9}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'seed': 42, 'verbose': 0}\n","Best RMSE for TabNet with ThresholdPruner: 170.06619230648107\n","Correlation Coefficient for TabNet with ThresholdPruner: 0.7791160590982565\n","Running Optuna for TabNet with WilcoxonPruner...\n","\n","Early stopping occurred at epoch 63 with best_epoch = 53 and best_val_0_mse = 14929.44922\n","\n","Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mse = 26579.20898\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 20394.36523\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 25222.18164\n","\n","Early stopping occurred at epoch 93 with best_epoch = 83 and best_val_0_mse = 26680.74219\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 78161.71094\n","\n","Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_mse = 43303.67578\n","\n","Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_mse = 24728.49414\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 16746.45898\n","\n","Early stopping occurred at epoch 80 with best_epoch = 70 and best_val_0_mse = 8561.96387\n","\n","Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mse = 16795.00586\n","\n","Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_mse = 22643.1543\n","\n","Early stopping occurred at epoch 63 with best_epoch = 53 and best_val_0_mse = 14929.44922\n","\n","Early stopping occurred at epoch 80 with best_epoch = 70 and best_val_0_mse = 13638.62402\n","\n","Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_mse = 34193.69531\n","\n","Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_mse = 19390.30469\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 70468.39062\n","\n","Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_mse = 28897.6582\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 13679.00879\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 70770.57031\n","\n","Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_mse = 35439.96875\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 13679.00879\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 28661.74219\n","\n","Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_mse = 12624.40918\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 20229.57812\n","\n","Early stopping occurred at epoch 66 with best_epoch = 56 and best_val_0_mse = 14524.68359\n","\n","Early stopping occurred at epoch 64 with best_epoch = 54 and best_val_0_mse = 16593.62109\n","\n","Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_mse = 18378.98242\n","\n","Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_mse = 27488.24414\n","\n","Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_mse = 26719.29883\n","\n","Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_mse = 42491.40234\n","\n","Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_mse = 36139.69531\n","\n","Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_mse = 16108.82715\n","\n","Early stopping occurred at epoch 92 with best_epoch = 82 and best_val_0_mse = 13766.94531\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 36754.25\n","\n","Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_mse = 48079.77734\n","\n","Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_mse = 29798.68164\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 10915.49414\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 10915.49414\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 10915.49414\n","\n","Early stopping occurred at epoch 93 with best_epoch = 83 and best_val_0_mse = 44489.34766\n","\n","Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_mse = 10915.49414\n","\n","Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_mse = 19584.3457\n","\n","Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_mse = 15583.16309\n","\n","Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_mse = 23804.91992\n","\n","Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_mse = 28100.55469\n","\n","Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_mse = 14781.05176\n","Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_0_mse = 18521.13477\n","Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_mse = 9390.97949\n","\n","Early stopping occurred at epoch 97 with best_epoch = 87 and best_val_0_mse = 14239.05371\n","Best MSE for TabNet with WilcoxonPruner: 13662.20703125 with params: {'n_d': 64, 'n_a': 16, 'n_steps': 5, 'gamma': 1.3, 'lambda_sparse': 0.0001, 'optimizer_params': {'lr': 0.02}, 'mask_type': 'sparsemax', 'n_shared': 2, 'n_independent': 1, 'scheduler_params': {'step_size': 10, 'gamma': 0.9}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'seed': 42, 'verbose': 0}\n","Best RMSE for TabNet with WilcoxonPruner: 116.88544405207178\n","Correlation Coefficient for TabNet with WilcoxonPruner: 0.8216805723051812\n","\n","Best model on test data: PGBM with PatientPruner\n","Test MSE: 7229.537620264676\n","Test RMSE: 85.02668769430382\n","Correlation Coefficient: 0.9061196029012977\n","Best Parameters: {'n_estimators': 300, 'learning_rate': 0.01, 'max_leaves': 57, 'min_split_gain': 0.5, 'reg_lambda': 0.1, 'feature_fraction': 1.0, 'bagging_fraction': 0.5, 'tree_correlation': 0.1, 'min_data_in_leaf': 5, 'max_bin': 64, 'distribution': 'laplace'}\n","Pruner Used: PatientPruner\n"]}]},{"cell_type":"code","source":["best_scores_autosampler"],"metadata":{"id":"uxFhi1w4LHZX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762771196859,"user_tz":-330,"elapsed":100,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"}},"outputId":"ba8c25cf-e2ad-4159-b032-edcf79e02e57"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{('Random Forest', 'MedianPruner'): {'best_score': 11899.272160644423,\n","  'best_params': {'n_estimators': 200,\n","   'criterion': 'poisson',\n","   'max_depth': None,\n","   'min_samples_split': 2,\n","   'min_samples_leaf': 0.01,\n","   'min_weight_fraction_leaf': 0.01,\n","   'max_features': 'sqrt',\n","   'max_leaf_nodes': 200,\n","   'min_impurity_decrease': 0.2,\n","   'n_jobs': -1,\n","   'random_state': 42,\n","   'verbose': 0,\n","   'warm_start': False,\n","   'ccp_alpha': 0.001},\n","  'test_mse': 11899.272160644423,\n","  'test_rmse': 109.08378504912828,\n","  'test_corr_coef': 0.8477575383559034,\n","  'pruner': 'MedianPruner'},\n"," ('Random Forest', 'NopPruner'): {'best_score': 12448.972212788854,\n","  'best_params': {'n_estimators': 300,\n","   'criterion': 'poisson',\n","   'max_depth': 10,\n","   'min_samples_split': 5,\n","   'min_samples_leaf': 0.01,\n","   'min_weight_fraction_leaf': 0.01,\n","   'max_features': 'sqrt',\n","   'max_leaf_nodes': 50,\n","   'min_impurity_decrease': 0.2,\n","   'n_jobs': -1,\n","   'random_state': 42,\n","   'verbose': 0,\n","   'warm_start': False,\n","   'ccp_alpha': 0.05},\n","  'test_mse': 12448.972212788854,\n","  'test_rmse': 111.5749623024308,\n","  'test_corr_coef': 0.844618446645913,\n","  'pruner': 'NopPruner'},\n"," ('Random Forest', 'PatientPruner'): {'best_score': 11575.713346171628,\n","  'best_params': {'n_estimators': 200,\n","   'criterion': 'poisson',\n","   'max_depth': 10,\n","   'min_samples_split': 0.01,\n","   'min_samples_leaf': 0.01,\n","   'min_weight_fraction_leaf': 0.0,\n","   'max_features': 'sqrt',\n","   'max_leaf_nodes': None,\n","   'min_impurity_decrease': 0.01,\n","   'n_jobs': -1,\n","   'random_state': 42,\n","   'verbose': 0,\n","   'warm_start': False,\n","   'ccp_alpha': 0.05},\n","  'test_mse': 11575.713346171628,\n","  'test_rmse': 107.59048910648016,\n","  'test_corr_coef': 0.8546567655496594,\n","  'pruner': 'PatientPruner'},\n"," ('Random Forest', 'PercentilePruner'): {'best_score': 11915.042116962484,\n","  'best_params': {'n_estimators': 200,\n","   'criterion': 'friedman_mse',\n","   'max_depth': None,\n","   'min_samples_split': 10,\n","   'min_samples_leaf': 5,\n","   'min_weight_fraction_leaf': 0.0,\n","   'max_features': 1.0,\n","   'max_leaf_nodes': None,\n","   'min_impurity_decrease': 0.01,\n","   'n_jobs': -1,\n","   'random_state': 42,\n","   'verbose': 0,\n","   'warm_start': False,\n","   'ccp_alpha': 0.001},\n","  'test_mse': 11915.042116962484,\n","  'test_rmse': 109.15604480266992,\n","  'test_corr_coef': 0.8400541584020661,\n","  'pruner': 'PercentilePruner'},\n"," ('Random Forest',\n","  'SuccessiveHalvingPruner'): {'best_score': 11796.492013891693, 'best_params': {'n_estimators': 200,\n","   'criterion': 'poisson',\n","   'max_depth': 40,\n","   'min_samples_split': 2,\n","   'min_samples_leaf': 0.01,\n","   'min_weight_fraction_leaf': 0.0,\n","   'max_features': 'log2',\n","   'max_leaf_nodes': None,\n","   'min_impurity_decrease': 0.01,\n","   'n_jobs': -1,\n","   'random_state': 42,\n","   'verbose': 0,\n","   'warm_start': False,\n","   'ccp_alpha': 0.001}, 'test_mse': 11796.492013891693, 'test_rmse': 108.61165689690814, 'test_corr_coef': 0.8507910192885686, 'pruner': 'SuccessiveHalvingPruner'},\n"," ('Random Forest', 'HyperbandPruner'): {'best_score': 11915.068920540138,\n","  'best_params': {'n_estimators': 200,\n","   'criterion': 'squared_error',\n","   'max_depth': 40,\n","   'min_samples_split': 10,\n","   'min_samples_leaf': 5,\n","   'min_weight_fraction_leaf': 0.01,\n","   'max_features': 1.0,\n","   'max_leaf_nodes': None,\n","   'min_impurity_decrease': 0.2,\n","   'n_jobs': -1,\n","   'random_state': 42,\n","   'verbose': 0,\n","   'warm_start': False,\n","   'ccp_alpha': 0.05},\n","  'test_mse': 11915.068920540138,\n","  'test_rmse': 109.15616757902477,\n","  'test_corr_coef': 0.8400536224944962,\n","  'pruner': 'HyperbandPruner'},\n"," ('Random Forest', 'ThresholdPruner'): {'best_score': 11396.714504124559,\n","  'best_params': {'n_estimators': 200,\n","   'criterion': 'poisson',\n","   'max_depth': 10,\n","   'min_samples_split': 5,\n","   'min_samples_leaf': 5,\n","   'min_weight_fraction_leaf': 0.0,\n","   'max_features': 1.0,\n","   'max_leaf_nodes': 50,\n","   'min_impurity_decrease': 0.2,\n","   'n_jobs': -1,\n","   'random_state': 42,\n","   'verbose': 0,\n","   'warm_start': False,\n","   'ccp_alpha': 0.01},\n","  'test_mse': 11396.714504124559,\n","  'test_rmse': 106.75539566750038,\n","  'test_corr_coef': 0.8545175710230446,\n","  'pruner': 'ThresholdPruner'},\n"," ('Random Forest', 'WilcoxonPruner'): {'best_score': 11899.27216064442,\n","  'best_params': {'n_estimators': 200,\n","   'criterion': 'poisson',\n","   'max_depth': None,\n","   'min_samples_split': 2,\n","   'min_samples_leaf': 0.01,\n","   'min_weight_fraction_leaf': 0.0,\n","   'max_features': 'log2',\n","   'max_leaf_nodes': 100,\n","   'min_impurity_decrease': 0.2,\n","   'n_jobs': -1,\n","   'random_state': 42,\n","   'verbose': 0,\n","   'warm_start': False,\n","   'ccp_alpha': 0.0},\n","  'test_mse': 11899.27216064442,\n","  'test_rmse': 109.08378504912827,\n","  'test_corr_coef': 0.8477575383559033,\n","  'pruner': 'WilcoxonPruner'},\n"," ('Gradient Boosting', 'MedianPruner'): {'best_score': 7970.963664616332,\n","  'best_params': {'loss': 'squared_error',\n","   'learning_rate': 0.1,\n","   'n_estimators': 300,\n","   'subsample': 1.0,\n","   'criterion': 'friedman_mse',\n","   'min_samples_split': 0.01,\n","   'min_samples_leaf': 3,\n","   'min_weight_fraction_leaf': 0.05,\n","   'max_depth': 7,\n","   'min_impurity_decrease': 0.01,\n","   'init': None,\n","   'random_state': 42,\n","   'max_features': None,\n","   'alpha': 0.9,\n","   'verbose': 0,\n","   'max_leaf_nodes': 10,\n","   'warm_start': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 10,\n","   'tol': 0.001,\n","   'ccp_alpha': 0.001},\n","  'test_mse': 7970.963664616332,\n","  'test_rmse': 89.28025349771545,\n","  'test_corr_coef': 0.8929816720083197,\n","  'pruner': 'MedianPruner'},\n"," ('Gradient Boosting', 'NopPruner'): {'best_score': 8095.507840454411,\n","  'best_params': {'loss': 'squared_error',\n","   'learning_rate': 0.01,\n","   'n_estimators': 500,\n","   'subsample': 1.0,\n","   'criterion': 'friedman_mse',\n","   'min_samples_split': 5,\n","   'min_samples_leaf': 3,\n","   'min_weight_fraction_leaf': 0.05,\n","   'max_depth': 10,\n","   'min_impurity_decrease': 0.1,\n","   'init': None,\n","   'random_state': 42,\n","   'max_features': 'log2',\n","   'alpha': 0.9,\n","   'verbose': 0,\n","   'max_leaf_nodes': 10,\n","   'warm_start': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 20,\n","   'tol': 0.001,\n","   'ccp_alpha': 0.0},\n","  'test_mse': 8095.507840454411,\n","  'test_rmse': 89.97504009698696,\n","  'test_corr_coef': 0.8972665861442432,\n","  'pruner': 'NopPruner'},\n"," ('Gradient Boosting', 'PatientPruner'): {'best_score': 7939.9986575438825,\n","  'best_params': {'loss': 'squared_error',\n","   'learning_rate': 0.05,\n","   'n_estimators': 500,\n","   'subsample': 0.9,\n","   'criterion': 'squared_error',\n","   'min_samples_split': 10,\n","   'min_samples_leaf': 3,\n","   'min_weight_fraction_leaf': 0.05,\n","   'max_depth': 5,\n","   'min_impurity_decrease': 0.1,\n","   'init': None,\n","   'random_state': 42,\n","   'max_features': 'log2',\n","   'alpha': 0.9,\n","   'verbose': 0,\n","   'max_leaf_nodes': 30,\n","   'warm_start': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 10,\n","   'tol': 0.001,\n","   'ccp_alpha': 0.01},\n","  'test_mse': 7939.9986575438825,\n","  'test_rmse': 89.10667010692231,\n","  'test_corr_coef': 0.8990304260652475,\n","  'pruner': 'PatientPruner'},\n"," ('Gradient Boosting', 'PercentilePruner'): {'best_score': 7895.175496459338,\n","  'best_params': {'loss': 'squared_error',\n","   'learning_rate': 0.05,\n","   'n_estimators': 700,\n","   'subsample': 1.0,\n","   'criterion': 'squared_error',\n","   'min_samples_split': 0.01,\n","   'min_samples_leaf': 0.01,\n","   'min_weight_fraction_leaf': 0.05,\n","   'max_depth': 3,\n","   'min_impurity_decrease': 0.01,\n","   'init': None,\n","   'random_state': 42,\n","   'max_features': None,\n","   'alpha': 0.9,\n","   'verbose': 0,\n","   'max_leaf_nodes': 30,\n","   'warm_start': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 10,\n","   'tol': 0.001,\n","   'ccp_alpha': 0.001},\n","  'test_mse': 7895.175496459338,\n","  'test_rmse': 88.85480007551274,\n","  'test_corr_coef': 0.906381821876527,\n","  'pruner': 'PercentilePruner'},\n"," ('Gradient Boosting',\n","  'SuccessiveHalvingPruner'): {'best_score': 8814.538009628499, 'best_params': {'loss': 'squared_error',\n","   'learning_rate': 0.05,\n","   'n_estimators': 300,\n","   'subsample': 0.5,\n","   'criterion': 'squared_error',\n","   'min_samples_split': 10,\n","   'min_samples_leaf': 5,\n","   'min_weight_fraction_leaf': 0.0,\n","   'max_depth': 5,\n","   'min_impurity_decrease': 0.0,\n","   'init': None,\n","   'random_state': 42,\n","   'max_features': 'sqrt',\n","   'alpha': 0.5,\n","   'verbose': 0,\n","   'max_leaf_nodes': None,\n","   'warm_start': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 20,\n","   'tol': 0.001,\n","   'ccp_alpha': 0.01}, 'test_mse': 8814.538009628499, 'test_rmse': 93.8857710711719, 'test_corr_coef': 0.8848669675879104, 'pruner': 'SuccessiveHalvingPruner'},\n"," ('Gradient Boosting', 'HyperbandPruner'): {'best_score': 8069.03802734413,\n","  'best_params': {'loss': 'squared_error',\n","   'learning_rate': 0.1,\n","   'n_estimators': 700,\n","   'subsample': 0.9,\n","   'criterion': 'squared_error',\n","   'min_samples_split': 5,\n","   'min_samples_leaf': 0.01,\n","   'min_weight_fraction_leaf': 0.05,\n","   'max_depth': 5,\n","   'min_impurity_decrease': 0.01,\n","   'init': None,\n","   'random_state': 42,\n","   'max_features': 'log2',\n","   'alpha': 0.5,\n","   'verbose': 0,\n","   'max_leaf_nodes': 10,\n","   'warm_start': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 10,\n","   'tol': 0.0001,\n","   'ccp_alpha': 0.001},\n","  'test_mse': 8069.03802734413,\n","  'test_rmse': 89.82782434938592,\n","  'test_corr_coef': 0.8971614193988803,\n","  'pruner': 'HyperbandPruner'},\n"," ('Gradient Boosting', 'ThresholdPruner'): {'best_score': 8315.221560527307,\n","  'best_params': {'loss': 'huber',\n","   'learning_rate': 0.05,\n","   'n_estimators': 100,\n","   'subsample': 1.0,\n","   'criterion': 'squared_error',\n","   'min_samples_split': 0.01,\n","   'min_samples_leaf': 3,\n","   'min_weight_fraction_leaf': 0.05,\n","   'max_depth': 5,\n","   'min_impurity_decrease': 0.1,\n","   'init': None,\n","   'random_state': 42,\n","   'max_features': None,\n","   'alpha': 0.9,\n","   'verbose': 0,\n","   'max_leaf_nodes': 10,\n","   'warm_start': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 20,\n","   'tol': 0.0001,\n","   'ccp_alpha': 0.01},\n","  'test_mse': 8315.221560527307,\n","  'test_rmse': 91.18783669178312,\n","  'test_corr_coef': 0.8872156687967951,\n","  'pruner': 'ThresholdPruner'},\n"," ('Gradient Boosting', 'WilcoxonPruner'): {'best_score': 11155.980059064212,\n","  'best_params': {'loss': 'huber',\n","   'learning_rate': 0.1,\n","   'n_estimators': 100,\n","   'subsample': 0.7,\n","   'criterion': 'squared_error',\n","   'min_samples_split': 5,\n","   'min_samples_leaf': 0.01,\n","   'min_weight_fraction_leaf': 0.01,\n","   'max_depth': 5,\n","   'min_impurity_decrease': 0.0,\n","   'init': None,\n","   'random_state': 42,\n","   'max_features': 'log2',\n","   'alpha': 0.1,\n","   'verbose': 0,\n","   'max_leaf_nodes': None,\n","   'warm_start': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 20,\n","   'tol': 0.001,\n","   'ccp_alpha': 0.0},\n","  'test_mse': 11155.980059064212,\n","  'test_rmse': 105.62187301437241,\n","  'test_corr_coef': 0.8533098826130289,\n","  'pruner': 'WilcoxonPruner'},\n"," ('XGBoost', 'MedianPruner'): {'best_score': 9549.828125,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'max_depth': 5,\n","   'min_child_weight': 5,\n","   'gamma': 0.1,\n","   'subsample': 0.5,\n","   'colsample_bytree': 0.9,\n","   'colsample_bylevel': 0.7,\n","   'reg_alpha': 0.01,\n","   'reg_lambda': 0.1,\n","   'objective': 'reg:squarederror',\n","   'random_state': 42,\n","   'n_jobs': -1},\n","  'test_mse': 9549.828125,\n","  'test_rmse': 97.72322203550188,\n","  'test_corr_coef': 0.8852101651430149,\n","  'pruner': 'MedianPruner'},\n"," ('XGBoost', 'NopPruner'): {'best_score': 10584.6494140625,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.05,\n","   'max_depth': 3,\n","   'min_child_weight': 5,\n","   'gamma': 1,\n","   'subsample': 0.5,\n","   'colsample_bytree': 0.7,\n","   'colsample_bylevel': 0.7,\n","   'reg_alpha': 0,\n","   'reg_lambda': 5,\n","   'objective': 'reg:squarederror',\n","   'random_state': 42,\n","   'n_jobs': -1},\n","  'test_mse': 10584.6494140625,\n","  'test_rmse': 102.88172536491842,\n","  'test_corr_coef': 0.8555665428449333,\n","  'pruner': 'NopPruner'},\n"," ('XGBoost', 'PatientPruner'): {'best_score': 9592.75,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.05,\n","   'max_depth': 7,\n","   'min_child_weight': 5,\n","   'gamma': 0,\n","   'subsample': 0.5,\n","   'colsample_bytree': 0.9,\n","   'colsample_bylevel': 0.9,\n","   'reg_alpha': 0.1,\n","   'reg_lambda': 5,\n","   'objective': 'reg:squarederror',\n","   'random_state': 42,\n","   'n_jobs': -1},\n","  'test_mse': 9592.75,\n","  'test_rmse': 97.94258522215962,\n","  'test_corr_coef': 0.8744618085762377,\n","  'pruner': 'PatientPruner'},\n"," ('XGBoost', 'PercentilePruner'): {'best_score': 11422.75,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.1,\n","   'max_depth': 3,\n","   'min_child_weight': 5,\n","   'gamma': 1,\n","   'subsample': 0.5,\n","   'colsample_bytree': 0.5,\n","   'colsample_bylevel': 0.9,\n","   'reg_alpha': 0.1,\n","   'reg_lambda': 5,\n","   'objective': 'reg:squarederror',\n","   'random_state': 42,\n","   'n_jobs': -1},\n","  'test_mse': 11422.75,\n","  'test_rmse': 106.87726605784786,\n","  'test_corr_coef': 0.8392123989874094,\n","  'pruner': 'PercentilePruner'},\n"," ('XGBoost', 'SuccessiveHalvingPruner'): {'best_score': 9704.0751953125,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.05,\n","   'max_depth': 5,\n","   'min_child_weight': 5,\n","   'gamma': 0.5,\n","   'subsample': 0.6,\n","   'colsample_bytree': 0.9,\n","   'colsample_bylevel': 0.9,\n","   'reg_alpha': 0.1,\n","   'reg_lambda': 5,\n","   'objective': 'reg:squarederror',\n","   'random_state': 42,\n","   'n_jobs': -1},\n","  'test_mse': 9704.0751953125,\n","  'test_rmse': 98.5092645151333,\n","  'test_corr_coef': 0.8768954297682986,\n","  'pruner': 'SuccessiveHalvingPruner'},\n"," ('XGBoost', 'HyperbandPruner'): {'best_score': 10142.8193359375,\n","  'best_params': {'n_estimators': 100,\n","   'learning_rate': 0.1,\n","   'max_depth': 3,\n","   'min_child_weight': 1,\n","   'gamma': 0,\n","   'subsample': 0.6,\n","   'colsample_bytree': 0.9,\n","   'colsample_bylevel': 0.7,\n","   'reg_alpha': 1,\n","   'reg_lambda': 5,\n","   'objective': 'reg:squarederror',\n","   'random_state': 42,\n","   'n_jobs': -1},\n","  'test_mse': 10142.8193359375,\n","  'test_rmse': 100.71156505554613,\n","  'test_corr_coef': 0.8631087766411594,\n","  'pruner': 'HyperbandPruner'},\n"," ('XGBoost', 'ThresholdPruner'): {'best_score': 9715.6533203125,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.05,\n","   'max_depth': 5,\n","   'min_child_weight': 5,\n","   'gamma': 1,\n","   'subsample': 0.6,\n","   'colsample_bytree': 0.9,\n","   'colsample_bylevel': 0.7,\n","   'reg_alpha': 0,\n","   'reg_lambda': 5,\n","   'objective': 'reg:squarederror',\n","   'random_state': 42,\n","   'n_jobs': -1},\n","  'test_mse': 9715.6533203125,\n","  'test_rmse': 98.56801367742226,\n","  'test_corr_coef': 0.8766330625314297,\n","  'pruner': 'ThresholdPruner'},\n"," ('XGBoost', 'WilcoxonPruner'): {'best_score': 9901.001953125,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.05,\n","   'max_depth': 3,\n","   'min_child_weight': 5,\n","   'gamma': 0.1,\n","   'subsample': 0.6,\n","   'colsample_bytree': 0.9,\n","   'colsample_bylevel': 0.9,\n","   'reg_alpha': 0.01,\n","   'reg_lambda': 5,\n","   'objective': 'reg:squarederror',\n","   'random_state': 42,\n","   'n_jobs': -1},\n","  'test_mse': 9901.001953125,\n","  'test_rmse': 99.50377858717225,\n","  'test_corr_coef': 0.8735680563612124,\n","  'pruner': 'WilcoxonPruner'},\n"," ('LightGBM', 'MedianPruner'): {'best_score': 7758.7734195111625,\n","  'best_params': {'n_estimators': 100,\n","   'learning_rate': 0.05,\n","   'num_leaves': 31,\n","   'max_depth': 7,\n","   'min_child_samples': 10,\n","   'subsample': 0.9,\n","   'colsample_bytree': 0.9,\n","   'reg_alpha': 0,\n","   'reg_lambda': 10,\n","   'min_child_weight': 1e-05,\n","   'bagging_freq': 5,\n","   'objective': 'regression',\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7758.7734195111625,\n","  'test_rmse': 88.08389988818139,\n","  'test_corr_coef': 0.8949937460822153,\n","  'pruner': 'MedianPruner'},\n"," ('LightGBM', 'NopPruner'): {'best_score': 7733.385730568155,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'num_leaves': 31,\n","   'max_depth': 3,\n","   'min_child_samples': 10,\n","   'subsample': 0.8,\n","   'colsample_bytree': 1.0,\n","   'reg_alpha': 1,\n","   'reg_lambda': 0,\n","   'min_child_weight': 1e-05,\n","   'bagging_freq': 1,\n","   'objective': 'regression',\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7733.385730568155,\n","  'test_rmse': 87.93967097145722,\n","  'test_corr_coef': 0.8950374906287867,\n","  'pruner': 'NopPruner'},\n"," ('LightGBM', 'PatientPruner'): {'best_score': 8340.646563999011,\n","  'best_params': {'n_estimators': 100,\n","   'learning_rate': 0.1,\n","   'num_leaves': 31,\n","   'max_depth': 3,\n","   'min_child_samples': 10,\n","   'subsample': 0.7,\n","   'colsample_bytree': 0.9,\n","   'reg_alpha': 1,\n","   'reg_lambda': 10,\n","   'min_child_weight': 0.01,\n","   'bagging_freq': 1,\n","   'objective': 'regression',\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 8340.646563999011,\n","  'test_rmse': 91.3271403472101,\n","  'test_corr_coef': 0.8855045388683417,\n","  'pruner': 'PatientPruner'},\n"," ('LightGBM', 'PercentilePruner'): {'best_score': 7999.619783578978,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'num_leaves': 15,\n","   'max_depth': 5,\n","   'min_child_samples': 10,\n","   'subsample': 0.8,\n","   'colsample_bytree': 0.9,\n","   'reg_alpha': 1,\n","   'reg_lambda': 10,\n","   'min_child_weight': 0.01,\n","   'bagging_freq': 5,\n","   'objective': 'regression',\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7999.619783578978,\n","  'test_rmse': 89.44059360032769,\n","  'test_corr_coef': 0.8942225188076984,\n","  'pruner': 'PercentilePruner'},\n"," ('LightGBM', 'SuccessiveHalvingPruner'): {'best_score': 8048.868195004266,\n","  'best_params': {'n_estimators': 100,\n","   'learning_rate': 0.05,\n","   'num_leaves': 15,\n","   'max_depth': 7,\n","   'min_child_samples': 10,\n","   'subsample': 0.7,\n","   'colsample_bytree': 1.0,\n","   'reg_alpha': 0.1,\n","   'reg_lambda': 0,\n","   'min_child_weight': 0.001,\n","   'bagging_freq': 1,\n","   'objective': 'regression',\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 8048.868195004266,\n","  'test_rmse': 89.71548470026937,\n","  'test_corr_coef': 0.8896934012654839,\n","  'pruner': 'SuccessiveHalvingPruner'},\n"," ('LightGBM', 'HyperbandPruner'): {'best_score': 8485.506620601334,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.05,\n","   'num_leaves': 15,\n","   'max_depth': 3,\n","   'min_child_samples': 10,\n","   'subsample': 0.7,\n","   'colsample_bytree': 0.9,\n","   'reg_alpha': 0,\n","   'reg_lambda': 0.1,\n","   'min_child_weight': 0.01,\n","   'bagging_freq': 5,\n","   'objective': 'regression',\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 8485.506620601334,\n","  'test_rmse': 92.11680965275195,\n","  'test_corr_coef': 0.8844658090762936,\n","  'pruner': 'HyperbandPruner'},\n"," ('LightGBM', 'ThresholdPruner'): {'best_score': 8070.687217418167,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.01,\n","   'num_leaves': 63,\n","   'max_depth': 5,\n","   'min_child_samples': 10,\n","   'subsample': 0.9,\n","   'colsample_bytree': 0.9,\n","   'reg_alpha': 0.1,\n","   'reg_lambda': 0,\n","   'min_child_weight': 1e-05,\n","   'bagging_freq': 5,\n","   'objective': 'regression',\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 8070.687217418167,\n","  'test_rmse': 89.83700360885912,\n","  'test_corr_coef': 0.9053014098785445,\n","  'pruner': 'ThresholdPruner'},\n"," ('LightGBM', 'WilcoxonPruner'): {'best_score': 8113.147255773668,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.01,\n","   'num_leaves': 31,\n","   'max_depth': 3,\n","   'min_child_samples': 10,\n","   'subsample': 0.8,\n","   'colsample_bytree': 0.9,\n","   'reg_alpha': 0,\n","   'reg_lambda': 0,\n","   'min_child_weight': 1e-05,\n","   'bagging_freq': 0,\n","   'objective': 'regression',\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 8113.147255773668,\n","  'test_rmse': 90.07301069562219,\n","  'test_corr_coef': 0.9026229998778147,\n","  'pruner': 'WilcoxonPruner'},\n"," ('GPBoost', 'MedianPruner'): {'best_score': 8176.113487642123,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.01,\n","   'max_depth': 5,\n","   'num_leaves': 15,\n","   'min_child_samples': 10,\n","   'subsample': 0.6,\n","   'colsample_bytree': 0.9,\n","   'reg_alpha': 0.5,\n","   'reg_lambda': 0,\n","   'min_child_weight': 0.1,\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 8176.113487642123,\n","  'test_rmse': 90.42186399119475,\n","  'test_corr_coef': 0.8994771459169217,\n","  'pruner': 'MedianPruner'},\n"," ('GPBoost', 'NopPruner'): {'best_score': 7561.740338130348,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.01,\n","   'max_depth': 7,\n","   'num_leaves': 31,\n","   'min_child_samples': 10,\n","   'subsample': 0.9,\n","   'colsample_bytree': 0.5,\n","   'reg_alpha': 1.0,\n","   'reg_lambda': 0.1,\n","   'min_child_weight': 0.1,\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7561.740338130348,\n","  'test_rmse': 86.9582677962846,\n","  'test_corr_coef': 0.8988928982684636,\n","  'pruner': 'NopPruner'},\n"," ('GPBoost', 'PatientPruner'): {'best_score': 7556.738487349329,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.01,\n","   'max_depth': 5,\n","   'num_leaves': 15,\n","   'min_child_samples': 10,\n","   'subsample': 0.8,\n","   'colsample_bytree': 0.5,\n","   'reg_alpha': 0,\n","   'reg_lambda': 1.0,\n","   'min_child_weight': 0.01,\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7556.738487349329,\n","  'test_rmse': 86.92950297424534,\n","  'test_corr_coef': 0.8998878429456466,\n","  'pruner': 'PatientPruner'},\n"," ('GPBoost', 'PercentilePruner'): {'best_score': 7695.892984238559,\n","  'best_params': {'n_estimators': 400,\n","   'learning_rate': 0.01,\n","   'max_depth': 3,\n","   'num_leaves': 31,\n","   'min_child_samples': 10,\n","   'subsample': 0.6,\n","   'colsample_bytree': 0.5,\n","   'reg_alpha': 0.5,\n","   'reg_lambda': 1.0,\n","   'min_child_weight': 0.001,\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7695.892984238559,\n","  'test_rmse': 87.7262388583858,\n","  'test_corr_coef': 0.8952597138838548,\n","  'pruner': 'PercentilePruner'},\n"," ('GPBoost', 'SuccessiveHalvingPruner'): {'best_score': 7719.798660513351,\n","  'best_params': {'n_estimators': 400,\n","   'learning_rate': 0.01,\n","   'max_depth': 3,\n","   'num_leaves': 15,\n","   'min_child_samples': 10,\n","   'subsample': 0.7,\n","   'colsample_bytree': 0.5,\n","   'reg_alpha': 0.1,\n","   'reg_lambda': 0.5,\n","   'min_child_weight': 0.001,\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7719.798660513351,\n","  'test_rmse': 87.86238478731016,\n","  'test_corr_coef': 0.8947212110299436,\n","  'pruner': 'SuccessiveHalvingPruner'},\n"," ('GPBoost', 'HyperbandPruner'): {'best_score': 7390.497112033827,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.01,\n","   'max_depth': 3,\n","   'num_leaves': 63,\n","   'min_child_samples': 10,\n","   'subsample': 0.5,\n","   'colsample_bytree': 0.5,\n","   'reg_alpha': 0.5,\n","   'reg_lambda': 0,\n","   'min_child_weight': 0.01,\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7390.497112033827,\n","  'test_rmse': 85.9680005120151,\n","  'test_corr_coef': 0.9021764089273122,\n","  'pruner': 'HyperbandPruner'},\n"," ('GPBoost', 'ThresholdPruner'): {'best_score': 7752.413923785924,\n","  'best_params': {'n_estimators': 400,\n","   'learning_rate': 0.01,\n","   'max_depth': 3,\n","   'num_leaves': 15,\n","   'min_child_samples': 10,\n","   'subsample': 0.9,\n","   'colsample_bytree': 0.5,\n","   'reg_alpha': 0.1,\n","   'reg_lambda': 0.1,\n","   'min_child_weight': 0.001,\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7752.413923785924,\n","  'test_rmse': 88.04779340668297,\n","  'test_corr_coef': 0.8941771471912344,\n","  'pruner': 'ThresholdPruner'},\n"," ('GPBoost', 'WilcoxonPruner'): {'best_score': 7415.173781485694,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.01,\n","   'max_depth': 3,\n","   'num_leaves': 31,\n","   'min_child_samples': 10,\n","   'subsample': 0.9,\n","   'colsample_bytree': 0.5,\n","   'reg_alpha': 0,\n","   'reg_lambda': 0.1,\n","   'min_child_weight': 0.1,\n","   'random_state': 42,\n","   'n_jobs': -1,\n","   'verbose': -1},\n","  'test_mse': 7415.173781485694,\n","  'test_rmse': 86.11140331852509,\n","  'test_corr_coef': 0.9019365138758428,\n","  'pruner': 'WilcoxonPruner'},\n"," ('CatBoost', 'MedianPruner'): {'best_score': 12376.263521855422,\n","  'best_params': {'iterations': 1000,\n","   'learning_rate': 0.01,\n","   'depth': 4,\n","   'l2_leaf_reg': 5,\n","   'border_count': 64,\n","   'min_data_in_leaf': 20,\n","   'rsm': 0.6,\n","   'bagging_temperature': 0,\n","   'random_seed': 42,\n","   'verbose': 0},\n","  'test_mse': 12376.263521855422,\n","  'test_rmse': 111.2486562698868,\n","  'test_corr_coef': 0.8292568399631975,\n","  'pruner': 'MedianPruner'},\n"," ('CatBoost', 'NopPruner'): {'best_score': 12295.893812610366,\n","  'best_params': {'iterations': 1000,\n","   'learning_rate': 0.01,\n","   'depth': 4,\n","   'l2_leaf_reg': 5,\n","   'border_count': 32,\n","   'min_data_in_leaf': 20,\n","   'rsm': 0.6,\n","   'bagging_temperature': 1,\n","   'random_seed': 42,\n","   'verbose': 0},\n","  'test_mse': 12295.893812610366,\n","  'test_rmse': 110.88685139641385,\n","  'test_corr_coef': 0.8292402939039899,\n","  'pruner': 'NopPruner'},\n"," ('CatBoost', 'PatientPruner'): {'best_score': 12238.78020635146,\n","  'best_params': {'iterations': 200,\n","   'learning_rate': 0.03,\n","   'depth': 4,\n","   'l2_leaf_reg': 1,\n","   'border_count': 32,\n","   'min_data_in_leaf': 5,\n","   'rsm': 0.6,\n","   'bagging_temperature': 10,\n","   'random_seed': 42,\n","   'verbose': 0},\n","  'test_mse': 12238.78020635146,\n","  'test_rmse': 110.62902063360887,\n","  'test_corr_coef': 0.8361744446633028,\n","  'pruner': 'PatientPruner'},\n"," ('CatBoost', 'PercentilePruner'): {'best_score': 11828.352768700503,\n","  'best_params': {'iterations': 200,\n","   'learning_rate': 0.03,\n","   'depth': 4,\n","   'l2_leaf_reg': 1,\n","   'border_count': 32,\n","   'min_data_in_leaf': 20,\n","   'rsm': 1.0,\n","   'bagging_temperature': 0,\n","   'random_seed': 42,\n","   'verbose': 0},\n","  'test_mse': 11828.352768700503,\n","  'test_rmse': 108.75823080898522,\n","  'test_corr_coef': 0.8402340352771763,\n","  'pruner': 'PercentilePruner'},\n"," ('CatBoost', 'SuccessiveHalvingPruner'): {'best_score': 12104.45825994089,\n","  'best_params': {'iterations': 500,\n","   'learning_rate': 0.01,\n","   'depth': 4,\n","   'l2_leaf_reg': 1,\n","   'border_count': 64,\n","   'min_data_in_leaf': 1,\n","   'rsm': 0.6,\n","   'bagging_temperature': 1,\n","   'random_seed': 42,\n","   'verbose': 0},\n","  'test_mse': 12104.45825994089,\n","  'test_rmse': 110.02026295160765,\n","  'test_corr_coef': 0.8375757155776365,\n","  'pruner': 'SuccessiveHalvingPruner'},\n"," ('CatBoost', 'HyperbandPruner'): {'best_score': 12143.915733308544,\n","  'best_params': {'iterations': 500,\n","   'learning_rate': 0.01,\n","   'depth': 4,\n","   'l2_leaf_reg': 1,\n","   'border_count': 32,\n","   'min_data_in_leaf': 10,\n","   'rsm': 1.0,\n","   'bagging_temperature': 0,\n","   'random_seed': 42,\n","   'verbose': 0},\n","  'test_mse': 12143.915733308544,\n","  'test_rmse': 110.19943617509367,\n","  'test_corr_coef': 0.8341941766725451,\n","  'pruner': 'HyperbandPruner'},\n"," ('CatBoost', 'ThresholdPruner'): {'best_score': 12059.717630039499,\n","  'best_params': {'iterations': 200,\n","   'learning_rate': 0.05,\n","   'depth': 4,\n","   'l2_leaf_reg': 5,\n","   'border_count': 64,\n","   'min_data_in_leaf': 10,\n","   'rsm': 1.0,\n","   'bagging_temperature': 1,\n","   'random_seed': 42,\n","   'verbose': 0},\n","  'test_mse': 12059.717630039499,\n","  'test_rmse': 109.81674567223115,\n","  'test_corr_coef': 0.834682576760058,\n","  'pruner': 'ThresholdPruner'},\n"," ('CatBoost', 'WilcoxonPruner'): {'best_score': 12469.34371865179,\n","  'best_params': {'iterations': 500,\n","   'learning_rate': 0.03,\n","   'depth': 4,\n","   'l2_leaf_reg': 9,\n","   'border_count': 32,\n","   'min_data_in_leaf': 5,\n","   'rsm': 0.6,\n","   'bagging_temperature': 10,\n","   'random_seed': 42,\n","   'verbose': 0},\n","  'test_mse': 12469.34371865179,\n","  'test_rmse': 111.66621565474398,\n","  'test_corr_coef': 0.8262490043890625,\n","  'pruner': 'WilcoxonPruner'},\n"," ('NGBoost', 'MedianPruner'): {'best_score': 12350.956487191177,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'natural_gradient': True,\n","   'minibatch_frac': 0.5,\n","   'col_sample': 0.5,\n","   'Dist': ngboost.distns.normal.Normal,\n","   'Score': ngboost.scores.LogScore,\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 12350.956487191177,\n","  'test_rmse': 111.1348572104683,\n","  'test_corr_coef': 0.8240968482667025,\n","  'pruner': 'MedianPruner'},\n"," ('NGBoost', 'NopPruner'): {'best_score': 12516.277127218891,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.01,\n","   'natural_gradient': True,\n","   'minibatch_frac': 0.7,\n","   'col_sample': 1.0,\n","   'Dist': ngboost.distns.normal.Normal,\n","   'Score': ngboost.scores.LogScore,\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 12516.277127218891,\n","  'test_rmse': 111.87616871889604,\n","  'test_corr_coef': 0.8229323362566604,\n","  'pruner': 'NopPruner'},\n"," ('NGBoost', 'PatientPruner'): {'best_score': 12407.823695288273,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'natural_gradient': True,\n","   'minibatch_frac': 0.5,\n","   'col_sample': 0.5,\n","   'Dist': ngboost.distns.normal.Normal,\n","   'Score': ngboost.scores.LogScore,\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 12407.823695288273,\n","  'test_rmse': 111.39041114605993,\n","  'test_corr_coef': 0.8236012244709813,\n","  'pruner': 'PatientPruner'},\n"," ('NGBoost', 'PercentilePruner'): {'best_score': 11891.196521152962,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.01,\n","   'natural_gradient': True,\n","   'minibatch_frac': 0.5,\n","   'col_sample': 0.9,\n","   'Dist': ngboost.distns.normal.Normal,\n","   'Score': ngboost.scores.LogScore,\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 11891.196521152962,\n","  'test_rmse': 109.0467630017185,\n","  'test_corr_coef': 0.8320328778774883,\n","  'pruner': 'PercentilePruner'},\n"," ('NGBoost', 'SuccessiveHalvingPruner'): {'best_score': 12081.228944892368,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'natural_gradient': True,\n","   'minibatch_frac': 0.5,\n","   'col_sample': 0.7,\n","   'Dist': ngboost.distns.normal.Normal,\n","   'Score': ngboost.scores.LogScore,\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 12081.228944892368,\n","  'test_rmse': 109.9146439055887,\n","  'test_corr_coef': 0.8282018510834986,\n","  'pruner': 'SuccessiveHalvingPruner'},\n"," ('NGBoost', 'HyperbandPruner'): {'best_score': 12186.4936254796,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'natural_gradient': True,\n","   'minibatch_frac': 0.5,\n","   'col_sample': 0.5,\n","   'Dist': ngboost.distns.normal.Normal,\n","   'Score': ngboost.scores.LogScore,\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 12186.4936254796,\n","  'test_rmse': 110.3924527559724,\n","  'test_corr_coef': 0.8265603520840819,\n","  'pruner': 'HyperbandPruner'},\n"," ('NGBoost', 'ThresholdPruner'): {'best_score': 12267.687603954357,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.01,\n","   'natural_gradient': True,\n","   'minibatch_frac': 0.7,\n","   'col_sample': 0.9,\n","   'Dist': ngboost.distns.normal.Normal,\n","   'Score': ngboost.scores.LogScore,\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 12267.687603954357,\n","  'test_rmse': 110.75959373324893,\n","  'test_corr_coef': 0.8284245213996452,\n","  'pruner': 'ThresholdPruner'},\n"," ('NGBoost', 'WilcoxonPruner'): {'best_score': 12801.060181481735,\n","  'best_params': {'n_estimators': 200,\n","   'learning_rate': 0.01,\n","   'natural_gradient': True,\n","   'minibatch_frac': 0.7,\n","   'col_sample': 0.9,\n","   'Dist': ngboost.distns.normal.Normal,\n","   'Score': ngboost.scores.LogScore,\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 12801.060181481735,\n","  'test_rmse': 113.1417702773018,\n","  'test_corr_coef': 0.8190521037461904,\n","  'pruner': 'WilcoxonPruner'},\n"," ('HistGradientBoosting', 'MedianPruner'): {'best_score': 9140.53355576915,\n","  'best_params': {'learning_rate': 0.05,\n","   'max_iter': 100,\n","   'max_depth': 7,\n","   'min_samples_leaf': 10,\n","   'max_leaf_nodes': 63,\n","   'l2_regularization': 0.0,\n","   'max_bins': 128,\n","   'early_stopping': False,\n","   'validation_fraction': 0.2,\n","   'n_iter_no_change': 5,\n","   'loss': 'squared_error',\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 9140.53355576915,\n","  'test_rmse': 95.60613764695837,\n","  'test_corr_coef': 0.8754575749315937,\n","  'pruner': 'MedianPruner'},\n"," ('HistGradientBoosting', 'NopPruner'): {'best_score': 8220.483111237245,\n","  'best_params': {'learning_rate': 0.01,\n","   'max_iter': 200,\n","   'max_depth': 3,\n","   'min_samples_leaf': 10,\n","   'max_leaf_nodes': 63,\n","   'l2_regularization': 0.5,\n","   'max_bins': 255,\n","   'early_stopping': False,\n","   'validation_fraction': 0.2,\n","   'n_iter_no_change': 10,\n","   'loss': 'squared_error',\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 8220.483111237245,\n","  'test_rmse': 90.66687990240563,\n","  'test_corr_coef': 0.9031505169182412,\n","  'pruner': 'NopPruner'},\n"," ('HistGradientBoosting', 'PatientPruner'): {'best_score': 8079.834507712777,\n","  'best_params': {'learning_rate': 0.01,\n","   'max_iter': 200,\n","   'max_depth': 3,\n","   'min_samples_leaf': 10,\n","   'max_leaf_nodes': 31,\n","   'l2_regularization': 0.0,\n","   'max_bins': 255,\n","   'early_stopping': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 15,\n","   'loss': 'squared_error',\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 8079.834507712777,\n","  'test_rmse': 89.88789967349764,\n","  'test_corr_coef': 0.9032771484501396,\n","  'pruner': 'PatientPruner'},\n"," ('HistGradientBoosting',\n","  'PercentilePruner'): {'best_score': 8317.046438364536, 'best_params': {'learning_rate': 0.01,\n","   'max_iter': 300,\n","   'max_depth': 3,\n","   'min_samples_leaf': 10,\n","   'max_leaf_nodes': None,\n","   'l2_regularization': 1.0,\n","   'max_bins': 128,\n","   'early_stopping': False,\n","   'validation_fraction': 0.2,\n","   'n_iter_no_change': 5,\n","   'loss': 'squared_error',\n","   'random_state': 42,\n","   'verbose': 0}, 'test_mse': 8317.046438364536, 'test_rmse': 91.1978422900703, 'test_corr_coef': 0.886267177095813, 'pruner': 'PercentilePruner'},\n"," ('HistGradientBoosting',\n","  'SuccessiveHalvingPruner'): {'best_score': 13963.238482618499, 'best_params': {'learning_rate': 0.01,\n","   'max_iter': 200,\n","   'max_depth': 3,\n","   'min_samples_leaf': 5,\n","   'max_leaf_nodes': 63,\n","   'l2_regularization': 0.0,\n","   'max_bins': 64,\n","   'early_stopping': True,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 15,\n","   'loss': 'squared_error',\n","   'random_state': 42,\n","   'verbose': 0}, 'test_mse': 13963.238482618499, 'test_rmse': 118.16614778615109, 'test_corr_coef': 0.8092102479436297, 'pruner': 'SuccessiveHalvingPruner'},\n"," ('HistGradientBoosting', 'HyperbandPruner'): {'best_score': 8317.046438364536,\n","  'best_params': {'learning_rate': 0.01,\n","   'max_iter': 300,\n","   'max_depth': 3,\n","   'min_samples_leaf': 10,\n","   'max_leaf_nodes': 15,\n","   'l2_regularization': 1.0,\n","   'max_bins': 255,\n","   'early_stopping': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 5,\n","   'loss': 'squared_error',\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 8317.046438364536,\n","  'test_rmse': 91.1978422900703,\n","  'test_corr_coef': 0.886267177095813,\n","  'pruner': 'HyperbandPruner'},\n"," ('HistGradientBoosting', 'ThresholdPruner'): {'best_score': 9029.10721281066,\n","  'best_params': {'learning_rate': 0.05,\n","   'max_iter': 100,\n","   'max_depth': 7,\n","   'min_samples_leaf': 10,\n","   'max_leaf_nodes': None,\n","   'l2_regularization': 0.5,\n","   'max_bins': 64,\n","   'early_stopping': False,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 15,\n","   'loss': 'squared_error',\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 9029.10721281066,\n","  'test_rmse': 95.02161445066412,\n","  'test_corr_coef': 0.8773438467284207,\n","  'pruner': 'ThresholdPruner'},\n"," ('HistGradientBoosting', 'WilcoxonPruner'): {'best_score': 14087.668901725148,\n","  'best_params': {'learning_rate': 0.01,\n","   'max_iter': 500,\n","   'max_depth': 3,\n","   'min_samples_leaf': 5,\n","   'max_leaf_nodes': 15,\n","   'l2_regularization': 0.5,\n","   'max_bins': 64,\n","   'early_stopping': True,\n","   'validation_fraction': 0.1,\n","   'n_iter_no_change': 5,\n","   'loss': 'squared_error',\n","   'random_state': 42,\n","   'verbose': 0},\n","  'test_mse': 14087.668901725148,\n","  'test_rmse': 118.69148622258105,\n","  'test_corr_coef': 0.7977046077554625,\n","  'pruner': 'WilcoxonPruner'},\n"," ('PGBM', 'MedianPruner'): {'best_score': 8211.289334879875,\n","  'best_params': {'n_estimators': 100,\n","   'learning_rate': 0.05,\n","   'max_leaves': 29,\n","   'min_split_gain': 0.1,\n","   'reg_lambda': 0.1,\n","   'feature_fraction': 0.9,\n","   'bagging_fraction': 0.9,\n","   'tree_correlation': 0.1,\n","   'min_data_in_leaf': 10,\n","   'max_bin': 64,\n","   'distribution': 'laplace'},\n","  'test_mse': 8211.289334879875,\n","  'test_rmse': 90.61616486521527,\n","  'test_corr_coef': 0.8883623538234999,\n","  'pruner': 'MedianPruner'},\n"," ('PGBM', 'NopPruner'): {'best_score': 7935.705863759918,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'max_leaves': 50,\n","   'min_split_gain': 0.5,\n","   'reg_lambda': 1.0,\n","   'feature_fraction': 0.9,\n","   'bagging_fraction': 0.9,\n","   'tree_correlation': 0.2,\n","   'min_data_in_leaf': 10,\n","   'max_bin': 128,\n","   'distribution': 'laplace'},\n","  'test_mse': 7935.705863759918,\n","  'test_rmse': 89.08257890160073,\n","  'test_corr_coef': 0.8932170725368642,\n","  'pruner': 'NopPruner'},\n"," ('PGBM', 'PatientPruner'): {'best_score': 7229.537620264676,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.01,\n","   'max_leaves': 57,\n","   'min_split_gain': 0.5,\n","   'reg_lambda': 0.1,\n","   'feature_fraction': 1.0,\n","   'bagging_fraction': 0.5,\n","   'tree_correlation': 0.1,\n","   'min_data_in_leaf': 5,\n","   'max_bin': 64,\n","   'distribution': 'laplace'},\n","  'test_mse': 7229.537620264676,\n","  'test_rmse': 85.02668769430382,\n","  'test_corr_coef': 0.9061196029012977,\n","  'pruner': 'PatientPruner'},\n"," ('PGBM', 'PercentilePruner'): {'best_score': 8429.764862044874,\n","  'best_params': {'n_estimators': 100,\n","   'learning_rate': 0.05,\n","   'max_leaves': 57,\n","   'min_split_gain': 1.0,\n","   'reg_lambda': 1.0,\n","   'feature_fraction': 0.9,\n","   'bagging_fraction': 0.9,\n","   'tree_correlation': 0.3,\n","   'min_data_in_leaf': 10,\n","   'max_bin': 256,\n","   'distribution': 'laplace'},\n","  'test_mse': 8429.764862044874,\n","  'test_rmse': 91.81375094202869,\n","  'test_corr_coef': 0.8856542783464043,\n","  'pruner': 'PercentilePruner'},\n"," ('PGBM', 'SuccessiveHalvingPruner'): {'best_score': 7802.516625036883,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.05,\n","   'max_leaves': 22,\n","   'min_split_gain': 1.0,\n","   'reg_lambda': 10.0,\n","   'feature_fraction': 1.0,\n","   'bagging_fraction': 0.5,\n","   'tree_correlation': 0.1,\n","   'min_data_in_leaf': 5,\n","   'max_bin': 128,\n","   'distribution': 'laplace'},\n","  'test_mse': 7802.516625036883,\n","  'test_rmse': 88.33185509790272,\n","  'test_corr_coef': 0.8940232734188226,\n","  'pruner': 'SuccessiveHalvingPruner'},\n"," ('PGBM', 'HyperbandPruner'): {'best_score': 8439.536940989694,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.05,\n","   'max_leaves': 24,\n","   'min_split_gain': 1.0,\n","   'reg_lambda': 5.0,\n","   'feature_fraction': 1.0,\n","   'bagging_fraction': 0.5,\n","   'tree_correlation': 0.0,\n","   'min_data_in_leaf': 5,\n","   'max_bin': 128,\n","   'distribution': 'normal'},\n","  'test_mse': 8439.536940989694,\n","  'test_rmse': 91.86695238762248,\n","  'test_corr_coef': 0.8850543851091954,\n","  'pruner': 'HyperbandPruner'},\n"," ('PGBM', 'ThresholdPruner'): {'best_score': 8068.997578574049,\n","  'best_params': {'n_estimators': 300,\n","   'learning_rate': 0.01,\n","   'max_leaves': 46,\n","   'min_split_gain': 1.0,\n","   'reg_lambda': 5.0,\n","   'feature_fraction': 1.0,\n","   'bagging_fraction': 0.9,\n","   'tree_correlation': 0.3,\n","   'min_data_in_leaf': 10,\n","   'max_bin': 256,\n","   'distribution': 'normal'},\n","  'test_mse': 8068.997578574049,\n","  'test_rmse': 89.82759920299578,\n","  'test_corr_coef': 0.8982481810462295,\n","  'pruner': 'ThresholdPruner'},\n"," ('PGBM', 'WilcoxonPruner'): {'best_score': 7272.343466656127,\n","  'best_params': {'n_estimators': 500,\n","   'learning_rate': 0.01,\n","   'max_leaves': 40,\n","   'min_split_gain': 0.5,\n","   'reg_lambda': 1.0,\n","   'feature_fraction': 1.0,\n","   'bagging_fraction': 0.5,\n","   'tree_correlation': 0.0,\n","   'min_data_in_leaf': 5,\n","   'max_bin': 64,\n","   'distribution': 'studentt'},\n","  'test_mse': 7272.343466656127,\n","  'test_rmse': 85.2780362499989,\n","  'test_corr_coef': 0.9014289079671919,\n","  'pruner': 'WilcoxonPruner'},\n"," ('TabNet', 'MedianPruner'): {'best_score': 25581.421875,\n","  'best_params': {'n_d': 8,\n","   'n_a': 16,\n","   'n_steps': 10,\n","   'gamma': 2.0,\n","   'lambda_sparse': 0.001,\n","   'optimizer_params': {'lr': 0.02},\n","   'mask_type': 'sparsemax',\n","   'n_shared': 1,\n","   'n_independent': 3,\n","   'scheduler_params': {'step_size': 10, 'gamma': 0.9},\n","   'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n","   'seed': 42,\n","   'verbose': 0},\n","  'test_mse': 25581.421875,\n","  'test_rmse': 159.94193282250905,\n","  'test_corr_coef': 0.8295406580371981,\n","  'pruner': 'MedianPruner'},\n"," ('TabNet', 'NopPruner'): {'best_score': 15667.0458984375,\n","  'best_params': {'n_d': 32,\n","   'n_a': 16,\n","   'n_steps': 10,\n","   'gamma': 2.0,\n","   'lambda_sparse': 0.001,\n","   'optimizer_params': {'lr': 0.02},\n","   'mask_type': 'entmax',\n","   'n_shared': 1,\n","   'n_independent': 2,\n","   'scheduler_params': {'step_size': 10, 'gamma': 0.9},\n","   'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n","   'seed': 42,\n","   'verbose': 0},\n","  'test_mse': 15667.0458984375,\n","  'test_rmse': 125.16807060283985,\n","  'test_corr_coef': 0.8310132382622455,\n","  'pruner': 'NopPruner'},\n"," ('TabNet', 'PatientPruner'): {'best_score': 13300.8408203125,\n","  'best_params': {'n_d': 16,\n","   'n_a': 8,\n","   'n_steps': 7,\n","   'gamma': 2.0,\n","   'lambda_sparse': 0.001,\n","   'optimizer_params': {'lr': 0.02},\n","   'mask_type': 'entmax',\n","   'n_shared': 1,\n","   'n_independent': 3,\n","   'scheduler_params': {'step_size': 10, 'gamma': 0.9},\n","   'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n","   'seed': 42,\n","   'verbose': 0},\n","  'test_mse': 13300.8408203125,\n","  'test_rmse': 115.32927130747207,\n","  'test_corr_coef': 0.8445435050943526,\n","  'pruner': 'PatientPruner'},\n"," ('TabNet', 'PercentilePruner'): {'best_score': 11346.50390625,\n","  'best_params': {'n_d': 16,\n","   'n_a': 64,\n","   'n_steps': 10,\n","   'gamma': 1.5,\n","   'lambda_sparse': 0.01,\n","   'optimizer_params': {'lr': 0.02},\n","   'mask_type': 'sparsemax',\n","   'n_shared': 2,\n","   'n_independent': 1,\n","   'scheduler_params': {'step_size': 10, 'gamma': 0.9},\n","   'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n","   'seed': 42,\n","   'verbose': 0},\n","  'test_mse': 11346.50390625,\n","  'test_rmse': 106.51996951863063,\n","  'test_corr_coef': 0.8408986077476054,\n","  'pruner': 'PercentilePruner'},\n"," ('TabNet', 'SuccessiveHalvingPruner'): {'best_score': 15933.2294921875,\n","  'best_params': {'n_d': 16,\n","   'n_a': 16,\n","   'n_steps': 10,\n","   'gamma': 1.5,\n","   'lambda_sparse': 0.0001,\n","   'optimizer_params': {'lr': 0.02},\n","   'mask_type': 'entmax',\n","   'n_shared': 2,\n","   'n_independent': 2,\n","   'scheduler_params': {'step_size': 10, 'gamma': 0.9},\n","   'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n","   'seed': 42,\n","   'verbose': 0},\n","  'test_mse': 15933.2294921875,\n","  'test_rmse': 126.22689686507982,\n","  'test_corr_coef': 0.7867959767153399,\n","  'pruner': 'SuccessiveHalvingPruner'},\n"," ('TabNet', 'HyperbandPruner'): {'best_score': 13764.0029296875,\n","  'best_params': {'n_d': 64,\n","   'n_a': 64,\n","   'n_steps': 7,\n","   'gamma': 2.0,\n","   'lambda_sparse': 0.001,\n","   'optimizer_params': {'lr': 0.02},\n","   'mask_type': 'sparsemax',\n","   'n_shared': 2,\n","   'n_independent': 1,\n","   'scheduler_params': {'step_size': 10, 'gamma': 0.9},\n","   'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n","   'seed': 42,\n","   'verbose': 0},\n","  'test_mse': 13764.0029296875,\n","  'test_rmse': 117.3200874943737,\n","  'test_corr_coef': 0.8452309365722418,\n","  'pruner': 'HyperbandPruner'},\n"," ('TabNet', 'ThresholdPruner'): {'best_score': 28922.509765625,\n","  'best_params': {'n_d': 16,\n","   'n_a': 16,\n","   'n_steps': 10,\n","   'gamma': 2.0,\n","   'lambda_sparse': 0.0001,\n","   'optimizer_params': {'lr': 0.02},\n","   'mask_type': 'sparsemax',\n","   'n_shared': 1,\n","   'n_independent': 3,\n","   'scheduler_params': {'step_size': 10, 'gamma': 0.9},\n","   'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n","   'seed': 42,\n","   'verbose': 0},\n","  'test_mse': 28922.509765625,\n","  'test_rmse': 170.06619230648107,\n","  'test_corr_coef': 0.7791160590982565,\n","  'pruner': 'ThresholdPruner'},\n"," ('TabNet', 'WilcoxonPruner'): {'best_score': 13662.20703125,\n","  'best_params': {'n_d': 64,\n","   'n_a': 16,\n","   'n_steps': 5,\n","   'gamma': 1.3,\n","   'lambda_sparse': 0.0001,\n","   'optimizer_params': {'lr': 0.02},\n","   'mask_type': 'sparsemax',\n","   'n_shared': 2,\n","   'n_independent': 1,\n","   'scheduler_params': {'step_size': 10, 'gamma': 0.9},\n","   'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n","   'seed': 42,\n","   'verbose': 0},\n","  'test_mse': 13662.20703125,\n","  'test_rmse': 116.88544405207178,\n","  'test_corr_coef': 0.8216805723051812,\n","  'pruner': 'WilcoxonPruner'}}"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"N9uWqgUD3pKn"},"source":["# **Best Model Analysis**"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"QifO8ipK2ZdZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762771216678,"user_tz":-330,"elapsed":19817,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"}},"outputId":"cf5bf35f-889c-435e-f757-7f43e8ac78aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["0:\tlearn: 279.2325090\ttotal: 5.04ms\tremaining: 1s\n","1:\tlearn: 274.6177976\ttotal: 5.6ms\tremaining: 555ms\n","2:\tlearn: 269.0627888\ttotal: 5.8ms\tremaining: 381ms\n","3:\tlearn: 265.0639962\ttotal: 5.95ms\tremaining: 292ms\n","4:\tlearn: 260.0592385\ttotal: 6.09ms\tremaining: 238ms\n","5:\tlearn: 255.1984039\ttotal: 6.24ms\tremaining: 202ms\n","6:\tlearn: 250.4520269\ttotal: 6.42ms\tremaining: 177ms\n","7:\tlearn: 245.5294657\ttotal: 6.56ms\tremaining: 157ms\n","8:\tlearn: 241.2354853\ttotal: 6.71ms\tremaining: 142ms\n","9:\tlearn: 237.2383014\ttotal: 6.84ms\tremaining: 130ms\n","10:\tlearn: 233.5596286\ttotal: 6.98ms\tremaining: 120ms\n","11:\tlearn: 229.7039990\ttotal: 7.11ms\tremaining: 111ms\n","12:\tlearn: 225.7110338\ttotal: 7.26ms\tremaining: 104ms\n","13:\tlearn: 222.2194395\ttotal: 7.39ms\tremaining: 98.2ms\n","14:\tlearn: 218.2656562\ttotal: 7.52ms\tremaining: 92.7ms\n","15:\tlearn: 214.2830025\ttotal: 7.69ms\tremaining: 88.4ms\n","16:\tlearn: 210.7511383\ttotal: 7.83ms\tremaining: 84.3ms\n","17:\tlearn: 207.4566555\ttotal: 7.96ms\tremaining: 80.5ms\n","18:\tlearn: 204.3293467\ttotal: 8.09ms\tremaining: 77ms\n","19:\tlearn: 200.8126031\ttotal: 8.23ms\tremaining: 74.1ms\n","20:\tlearn: 197.7260954\ttotal: 8.5ms\tremaining: 72.4ms\n","21:\tlearn: 194.9578757\ttotal: 8.66ms\tremaining: 70.1ms\n","22:\tlearn: 191.2265397\ttotal: 8.79ms\tremaining: 67.7ms\n","23:\tlearn: 188.3175120\ttotal: 8.94ms\tremaining: 65.6ms\n","24:\tlearn: 185.2514743\ttotal: 9.09ms\tremaining: 63.6ms\n","25:\tlearn: 182.2682096\ttotal: 9.22ms\tremaining: 61.7ms\n","26:\tlearn: 179.7704807\ttotal: 9.35ms\tremaining: 59.9ms\n","27:\tlearn: 176.9992505\ttotal: 9.48ms\tremaining: 58.2ms\n","28:\tlearn: 173.9239207\ttotal: 9.61ms\tremaining: 56.7ms\n","29:\tlearn: 171.2724363\ttotal: 9.8ms\tremaining: 55.6ms\n","30:\tlearn: 168.7186787\ttotal: 9.92ms\tremaining: 54.1ms\n","31:\tlearn: 166.5579529\ttotal: 10ms\tremaining: 52.8ms\n","32:\tlearn: 164.2872638\ttotal: 10.2ms\tremaining: 51.6ms\n","33:\tlearn: 162.0611535\ttotal: 10.3ms\tremaining: 50.4ms\n","34:\tlearn: 159.4896535\ttotal: 10.4ms\tremaining: 49.3ms\n","35:\tlearn: 157.4829504\ttotal: 10.6ms\tremaining: 48.2ms\n","36:\tlearn: 155.2659298\ttotal: 10.7ms\tremaining: 47.2ms\n","37:\tlearn: 153.2577217\ttotal: 10.8ms\tremaining: 46.3ms\n","38:\tlearn: 151.4477440\ttotal: 11ms\tremaining: 45.3ms\n","39:\tlearn: 149.5424530\ttotal: 11.1ms\tremaining: 44.3ms\n","40:\tlearn: 147.3464779\ttotal: 11.2ms\tremaining: 43.4ms\n","41:\tlearn: 145.3444990\ttotal: 11.3ms\tremaining: 42.6ms\n","42:\tlearn: 143.5996352\ttotal: 11.5ms\tremaining: 41.9ms\n","43:\tlearn: 141.7709452\ttotal: 11.6ms\tremaining: 41.1ms\n","44:\tlearn: 140.0347739\ttotal: 11.7ms\tremaining: 40.4ms\n","45:\tlearn: 138.7140919\ttotal: 11.8ms\tremaining: 39.7ms\n","46:\tlearn: 137.3274205\ttotal: 12ms\tremaining: 39ms\n","47:\tlearn: 135.9469128\ttotal: 12.1ms\tremaining: 38.3ms\n","48:\tlearn: 134.3849329\ttotal: 12.3ms\tremaining: 37.8ms\n","49:\tlearn: 132.9090994\ttotal: 12.4ms\tremaining: 37.2ms\n","50:\tlearn: 131.5274879\ttotal: 12.5ms\tremaining: 36.6ms\n","51:\tlearn: 130.3404814\ttotal: 12.6ms\tremaining: 36ms\n","52:\tlearn: 129.0537554\ttotal: 12.8ms\tremaining: 35.4ms\n","53:\tlearn: 127.7616230\ttotal: 12.9ms\tremaining: 34.8ms\n","54:\tlearn: 126.2750356\ttotal: 13ms\tremaining: 34.4ms\n","55:\tlearn: 124.8653052\ttotal: 13.2ms\tremaining: 33.9ms\n","56:\tlearn: 123.5309990\ttotal: 13.3ms\tremaining: 33.4ms\n","57:\tlearn: 122.3165099\ttotal: 13.4ms\tremaining: 32.9ms\n","58:\tlearn: 121.0037133\ttotal: 13.5ms\tremaining: 32.4ms\n","59:\tlearn: 120.1271536\ttotal: 13.7ms\tremaining: 31.9ms\n","60:\tlearn: 119.1940080\ttotal: 13.8ms\tremaining: 31.5ms\n","61:\tlearn: 118.1756174\ttotal: 14ms\tremaining: 31.1ms\n","62:\tlearn: 117.3128845\ttotal: 14.1ms\tremaining: 30.6ms\n","63:\tlearn: 116.2805608\ttotal: 14.2ms\tremaining: 30.2ms\n","64:\tlearn: 115.3727145\ttotal: 14.7ms\tremaining: 30.4ms\n","65:\tlearn: 114.6025011\ttotal: 14.8ms\tremaining: 30.1ms\n","66:\tlearn: 113.8728351\ttotal: 14.9ms\tremaining: 29.7ms\n","67:\tlearn: 112.9911460\ttotal: 15.1ms\tremaining: 29.3ms\n","68:\tlearn: 111.9873389\ttotal: 15.2ms\tremaining: 28.8ms\n","69:\tlearn: 111.4315347\ttotal: 15.3ms\tremaining: 28.5ms\n","70:\tlearn: 110.4699740\ttotal: 15.5ms\tremaining: 28.1ms\n","71:\tlearn: 109.6885251\ttotal: 15.6ms\tremaining: 27.7ms\n","72:\tlearn: 108.8774588\ttotal: 15.7ms\tremaining: 27.4ms\n","73:\tlearn: 108.0700655\ttotal: 15.8ms\tremaining: 27ms\n","74:\tlearn: 107.4011713\ttotal: 16ms\tremaining: 26.6ms\n","75:\tlearn: 106.7167434\ttotal: 16.1ms\tremaining: 26.2ms\n","76:\tlearn: 106.1312142\ttotal: 16.2ms\tremaining: 25.9ms\n","77:\tlearn: 105.6703296\ttotal: 16.3ms\tremaining: 25.6ms\n","78:\tlearn: 104.9233858\ttotal: 16.5ms\tremaining: 25.2ms\n","79:\tlearn: 104.4532841\ttotal: 16.6ms\tremaining: 24.9ms\n","80:\tlearn: 103.8449069\ttotal: 16.7ms\tremaining: 24.6ms\n","81:\tlearn: 103.2402168\ttotal: 16.9ms\tremaining: 24.3ms\n","82:\tlearn: 102.7829659\ttotal: 17ms\tremaining: 24ms\n","83:\tlearn: 102.0237057\ttotal: 17.2ms\tremaining: 23.7ms\n","84:\tlearn: 101.3595079\ttotal: 17.3ms\tremaining: 23.4ms\n","85:\tlearn: 101.0114622\ttotal: 17.4ms\tremaining: 23.1ms\n","86:\tlearn: 100.1790523\ttotal: 17.5ms\tremaining: 22.8ms\n","87:\tlearn: 99.8819734\ttotal: 17.7ms\tremaining: 22.5ms\n","88:\tlearn: 99.3644127\ttotal: 17.8ms\tremaining: 22.2ms\n","89:\tlearn: 98.8041793\ttotal: 17.9ms\tremaining: 21.9ms\n","90:\tlearn: 98.3469151\ttotal: 18.1ms\tremaining: 21.6ms\n","91:\tlearn: 97.8434567\ttotal: 18.2ms\tremaining: 21.3ms\n","92:\tlearn: 97.3131379\ttotal: 18.3ms\tremaining: 21.1ms\n","93:\tlearn: 96.8037638\ttotal: 18.4ms\tremaining: 20.8ms\n","94:\tlearn: 96.3964833\ttotal: 18.6ms\tremaining: 20.5ms\n","95:\tlearn: 95.8878909\ttotal: 18.7ms\tremaining: 20.3ms\n","96:\tlearn: 95.5451560\ttotal: 18.8ms\tremaining: 20ms\n","97:\tlearn: 95.1358112\ttotal: 18.9ms\tremaining: 19.7ms\n","98:\tlearn: 94.7955603\ttotal: 19.1ms\tremaining: 19.4ms\n","99:\tlearn: 94.5122336\ttotal: 19.2ms\tremaining: 19.2ms\n","100:\tlearn: 94.1613919\ttotal: 19.3ms\tremaining: 18.9ms\n","101:\tlearn: 93.9236982\ttotal: 19.4ms\tremaining: 18.7ms\n","102:\tlearn: 93.6605972\ttotal: 19.6ms\tremaining: 18.4ms\n","103:\tlearn: 93.4049509\ttotal: 19.7ms\tremaining: 18.2ms\n","104:\tlearn: 93.0835665\ttotal: 19.8ms\tremaining: 17.9ms\n","105:\tlearn: 92.7911155\ttotal: 19.9ms\tremaining: 17.7ms\n","106:\tlearn: 92.4684953\ttotal: 20.1ms\tremaining: 17.4ms\n","107:\tlearn: 92.0779341\ttotal: 20.2ms\tremaining: 17.2ms\n","108:\tlearn: 91.7699173\ttotal: 20.3ms\tremaining: 17ms\n","109:\tlearn: 91.3068173\ttotal: 20.5ms\tremaining: 16.7ms\n","110:\tlearn: 90.8408762\ttotal: 20.6ms\tremaining: 16.5ms\n","111:\tlearn: 90.7058561\ttotal: 20.7ms\tremaining: 16.3ms\n","112:\tlearn: 90.2409886\ttotal: 20.8ms\tremaining: 16ms\n","113:\tlearn: 89.9690876\ttotal: 21ms\tremaining: 15.8ms\n","114:\tlearn: 89.7649372\ttotal: 21.1ms\tremaining: 15.6ms\n","115:\tlearn: 89.3291431\ttotal: 21.2ms\tremaining: 15.4ms\n","116:\tlearn: 88.8352207\ttotal: 21.3ms\tremaining: 15.1ms\n","117:\tlearn: 88.5826306\ttotal: 21.5ms\tremaining: 14.9ms\n","118:\tlearn: 88.3021002\ttotal: 21.6ms\tremaining: 14.7ms\n","119:\tlearn: 87.9832801\ttotal: 21.7ms\tremaining: 14.5ms\n","120:\tlearn: 87.7999356\ttotal: 21.9ms\tremaining: 14.3ms\n","121:\tlearn: 87.6338340\ttotal: 22ms\tremaining: 14.1ms\n","122:\tlearn: 87.3853280\ttotal: 22.1ms\tremaining: 13.9ms\n","123:\tlearn: 87.2081236\ttotal: 22.3ms\tremaining: 13.6ms\n","124:\tlearn: 87.0412073\ttotal: 22.4ms\tremaining: 13.4ms\n","125:\tlearn: 86.6166543\ttotal: 22.5ms\tremaining: 13.2ms\n","126:\tlearn: 86.4997603\ttotal: 22.7ms\tremaining: 13ms\n","127:\tlearn: 86.3474285\ttotal: 22.8ms\tremaining: 12.8ms\n","128:\tlearn: 86.1716406\ttotal: 23ms\tremaining: 12.7ms\n","129:\tlearn: 85.9443210\ttotal: 23.1ms\tremaining: 12.5ms\n","130:\tlearn: 85.5144252\ttotal: 23.3ms\tremaining: 12.3ms\n","131:\tlearn: 85.3051904\ttotal: 23.4ms\tremaining: 12.1ms\n","132:\tlearn: 84.9650416\ttotal: 23.6ms\tremaining: 11.9ms\n","133:\tlearn: 84.7982049\ttotal: 23.7ms\tremaining: 11.7ms\n","134:\tlearn: 84.5983669\ttotal: 23.8ms\tremaining: 11.5ms\n","135:\tlearn: 84.4098378\ttotal: 23.9ms\tremaining: 11.3ms\n","136:\tlearn: 84.2784682\ttotal: 24.1ms\tremaining: 11.1ms\n","137:\tlearn: 84.1880463\ttotal: 24.2ms\tremaining: 10.9ms\n","138:\tlearn: 83.9652107\ttotal: 24.3ms\tremaining: 10.7ms\n","139:\tlearn: 83.8875004\ttotal: 24.5ms\tremaining: 10.5ms\n","140:\tlearn: 83.7808000\ttotal: 24.6ms\tremaining: 10.3ms\n","141:\tlearn: 83.5040397\ttotal: 24.7ms\tremaining: 10.1ms\n","142:\tlearn: 83.1728094\ttotal: 24.8ms\tremaining: 9.89ms\n","143:\tlearn: 83.0515389\ttotal: 25ms\tremaining: 9.71ms\n","144:\tlearn: 82.9128432\ttotal: 25.1ms\tremaining: 9.52ms\n","145:\tlearn: 82.7843913\ttotal: 25.3ms\tremaining: 9.37ms\n","146:\tlearn: 82.7253265\ttotal: 25.5ms\tremaining: 9.18ms\n","147:\tlearn: 82.5456334\ttotal: 25.6ms\tremaining: 8.99ms\n","148:\tlearn: 82.4419816\ttotal: 25.7ms\tremaining: 8.81ms\n","149:\tlearn: 82.4057064\ttotal: 25.9ms\tremaining: 8.62ms\n","150:\tlearn: 82.2651979\ttotal: 26ms\tremaining: 8.43ms\n","151:\tlearn: 82.0355109\ttotal: 26.1ms\tremaining: 8.24ms\n","152:\tlearn: 81.7510674\ttotal: 26.2ms\tremaining: 8.06ms\n","153:\tlearn: 81.6766441\ttotal: 26.3ms\tremaining: 7.87ms\n","154:\tlearn: 81.5916383\ttotal: 26.5ms\tremaining: 7.69ms\n","155:\tlearn: 81.5380575\ttotal: 26.6ms\tremaining: 7.49ms\n","156:\tlearn: 81.2282339\ttotal: 26.7ms\tremaining: 7.31ms\n","157:\tlearn: 80.8471769\ttotal: 26.8ms\tremaining: 7.13ms\n","158:\tlearn: 80.7252631\ttotal: 27ms\tremaining: 6.95ms\n","159:\tlearn: 80.6393198\ttotal: 27.1ms\tremaining: 6.77ms\n","160:\tlearn: 80.5855434\ttotal: 27.2ms\tremaining: 6.59ms\n","161:\tlearn: 80.3226032\ttotal: 27.3ms\tremaining: 6.41ms\n","162:\tlearn: 80.2689665\ttotal: 27.5ms\tremaining: 6.24ms\n","163:\tlearn: 80.0458242\ttotal: 27.6ms\tremaining: 6.06ms\n","164:\tlearn: 79.9787589\ttotal: 27.7ms\tremaining: 5.88ms\n","165:\tlearn: 79.6844015\ttotal: 27.9ms\tremaining: 5.71ms\n","166:\tlearn: 79.4988132\ttotal: 28ms\tremaining: 5.53ms\n","167:\tlearn: 79.3030762\ttotal: 28.1ms\tremaining: 5.36ms\n","168:\tlearn: 79.1464648\ttotal: 28.3ms\tremaining: 5.18ms\n","169:\tlearn: 79.0801028\ttotal: 28.4ms\tremaining: 5.01ms\n","170:\tlearn: 78.9653976\ttotal: 28.5ms\tremaining: 4.83ms\n","171:\tlearn: 78.9034359\ttotal: 28.6ms\tremaining: 4.66ms\n","172:\tlearn: 78.7387772\ttotal: 28.8ms\tremaining: 4.49ms\n","173:\tlearn: 78.6538069\ttotal: 28.9ms\tremaining: 4.32ms\n","174:\tlearn: 78.6082514\ttotal: 29ms\tremaining: 4.14ms\n","175:\tlearn: 78.4146864\ttotal: 29.1ms\tremaining: 3.97ms\n","176:\tlearn: 78.2590052\ttotal: 29.3ms\tremaining: 3.8ms\n","177:\tlearn: 78.0210401\ttotal: 29.4ms\tremaining: 3.63ms\n","178:\tlearn: 77.9490488\ttotal: 29.5ms\tremaining: 3.46ms\n","179:\tlearn: 77.9078412\ttotal: 29.6ms\tremaining: 3.29ms\n","180:\tlearn: 77.8326388\ttotal: 29.8ms\tremaining: 3.13ms\n","181:\tlearn: 77.7004925\ttotal: 29.9ms\tremaining: 2.96ms\n","182:\tlearn: 77.4180908\ttotal: 30.1ms\tremaining: 2.79ms\n","183:\tlearn: 77.1319369\ttotal: 30.2ms\tremaining: 2.63ms\n","184:\tlearn: 77.0620242\ttotal: 30.4ms\tremaining: 2.46ms\n","185:\tlearn: 76.9156375\ttotal: 30.5ms\tremaining: 2.29ms\n","186:\tlearn: 76.8070399\ttotal: 30.7ms\tremaining: 2.13ms\n","187:\tlearn: 76.6421213\ttotal: 30.8ms\tremaining: 1.97ms\n","188:\tlearn: 76.4101110\ttotal: 30.9ms\tremaining: 1.8ms\n","189:\tlearn: 76.3139223\ttotal: 31.1ms\tremaining: 1.64ms\n","190:\tlearn: 76.1945643\ttotal: 31.2ms\tremaining: 1.47ms\n","191:\tlearn: 76.0831339\ttotal: 31.3ms\tremaining: 1.31ms\n","192:\tlearn: 75.9781916\ttotal: 31.5ms\tremaining: 1.14ms\n","193:\tlearn: 75.8154622\ttotal: 31.6ms\tremaining: 977us\n","194:\tlearn: 75.7428422\ttotal: 31.7ms\tremaining: 813us\n","195:\tlearn: 75.6010055\ttotal: 31.8ms\tremaining: 649us\n","196:\tlearn: 75.3301772\ttotal: 32ms\tremaining: 486us\n","197:\tlearn: 75.2775479\ttotal: 32.2ms\tremaining: 325us\n","198:\tlearn: 75.2073042\ttotal: 32.4ms\tremaining: 162us\n","199:\tlearn: 75.1279154\ttotal: 32.5ms\tremaining: 0us\n","Training on CPU\n","Estimator 0/300, Train metric: 282.0879\n","Estimator 1/300, Train metric: 279.8289\n","Estimator 2/300, Train metric: 278.2864\n","Estimator 3/300, Train metric: 277.0394\n","Estimator 4/300, Train metric: 274.9732\n","Estimator 5/300, Train metric: 272.6784\n","Estimator 6/300, Train metric: 270.7197\n","Estimator 7/300, Train metric: 268.3668\n","Estimator 8/300, Train metric: 266.7731\n","Estimator 9/300, Train metric: 264.7304\n","Estimator 10/300, Train metric: 262.5108\n","Estimator 11/300, Train metric: 260.6197\n","Estimator 12/300, Train metric: 258.7881\n","Estimator 13/300, Train metric: 257.4232\n","Estimator 14/300, Train metric: 255.1190\n","Estimator 15/300, Train metric: 253.2872\n","Estimator 16/300, Train metric: 251.2900\n","Estimator 17/300, Train metric: 249.3364\n","Estimator 18/300, Train metric: 247.2919\n","Estimator 19/300, Train metric: 246.4284\n","Estimator 20/300, Train metric: 244.3657\n","Estimator 21/300, Train metric: 242.5655\n","Estimator 22/300, Train metric: 241.4949\n","Estimator 23/300, Train metric: 239.6941\n","Estimator 24/300, Train metric: 238.5791\n","Estimator 25/300, Train metric: 237.2830\n","Estimator 26/300, Train metric: 235.9565\n","Estimator 27/300, Train metric: 234.1498\n","Estimator 28/300, Train metric: 232.5585\n","Estimator 29/300, Train metric: 231.0100\n","Estimator 30/300, Train metric: 229.1918\n","Estimator 31/300, Train metric: 227.7442\n","Estimator 32/300, Train metric: 225.8721\n","Estimator 33/300, Train metric: 224.5793\n","Estimator 34/300, Train metric: 224.1823\n","Estimator 35/300, Train metric: 222.8103\n","Estimator 36/300, Train metric: 222.0056\n","Estimator 37/300, Train metric: 220.0602\n","Estimator 38/300, Train metric: 219.1826\n","Estimator 39/300, Train metric: 218.4252\n","Estimator 40/300, Train metric: 217.9641\n","Estimator 41/300, Train metric: 216.1090\n","Estimator 42/300, Train metric: 215.1601\n","Estimator 43/300, Train metric: 213.2915\n","Estimator 44/300, Train metric: 212.4584\n","Estimator 45/300, Train metric: 211.7207\n","Estimator 46/300, Train metric: 210.1851\n","Estimator 47/300, Train metric: 208.7092\n","Estimator 48/300, Train metric: 207.2079\n","Estimator 49/300, Train metric: 206.0768\n","Estimator 50/300, Train metric: 204.7174\n","Estimator 51/300, Train metric: 202.9321\n","Estimator 52/300, Train metric: 202.1340\n","Estimator 53/300, Train metric: 200.9151\n","Estimator 54/300, Train metric: 200.0612\n","Estimator 55/300, Train metric: 199.3337\n","Estimator 56/300, Train metric: 197.8991\n","Estimator 57/300, Train metric: 197.1959\n","Estimator 58/300, Train metric: 195.8478\n","Estimator 59/300, Train metric: 194.6137\n","Estimator 60/300, Train metric: 193.9840\n","Estimator 61/300, Train metric: 193.3539\n","Estimator 62/300, Train metric: 191.9638\n","Estimator 63/300, Train metric: 190.9811\n","Estimator 64/300, Train metric: 190.2200\n","Estimator 65/300, Train metric: 189.1275\n","Estimator 66/300, Train metric: 187.8243\n","Estimator 67/300, Train metric: 186.5762\n","Estimator 68/300, Train metric: 185.9452\n","Estimator 69/300, Train metric: 184.7641\n","Estimator 70/300, Train metric: 184.2771\n","Estimator 71/300, Train metric: 183.0382\n","Estimator 72/300, Train metric: 181.8579\n","Estimator 73/300, Train metric: 181.2379\n","Estimator 74/300, Train metric: 180.6920\n","Estimator 75/300, Train metric: 179.8251\n","Estimator 76/300, Train metric: 178.4650\n","Estimator 77/300, Train metric: 177.8050\n","Estimator 78/300, Train metric: 176.5578\n","Estimator 79/300, Train metric: 176.0331\n","Estimator 80/300, Train metric: 175.5924\n","Estimator 81/300, Train metric: 174.5875\n","Estimator 82/300, Train metric: 174.0645\n","Estimator 83/300, Train metric: 173.5672\n","Estimator 84/300, Train metric: 172.6295\n","Estimator 85/300, Train metric: 171.5969\n","Estimator 86/300, Train metric: 170.6430\n","Estimator 87/300, Train metric: 170.0141\n","Estimator 88/300, Train metric: 169.3896\n","Estimator 89/300, Train metric: 168.8724\n","Estimator 90/300, Train metric: 168.3174\n","Estimator 91/300, Train metric: 167.6990\n","Estimator 92/300, Train metric: 166.4754\n","Estimator 93/300, Train metric: 165.4741\n","Estimator 94/300, Train metric: 164.5345\n","Estimator 95/300, Train metric: 163.9499\n","Estimator 96/300, Train metric: 162.8870\n","Estimator 97/300, Train metric: 162.5956\n","Estimator 98/300, Train metric: 161.8620\n","Estimator 99/300, Train metric: 161.1634\n","Estimator 100/300, Train metric: 159.9584\n","Estimator 101/300, Train metric: 159.1791\n","Estimator 102/300, Train metric: 158.3331\n","Estimator 103/300, Train metric: 157.9407\n","Estimator 104/300, Train metric: 157.6402\n","Estimator 105/300, Train metric: 157.0131\n","Estimator 106/300, Train metric: 156.8687\n","Estimator 107/300, Train metric: 156.0550\n","Estimator 108/300, Train metric: 155.0009\n","Estimator 109/300, Train metric: 154.6065\n","Estimator 110/300, Train metric: 153.7875\n","Estimator 111/300, Train metric: 153.4809\n","Estimator 112/300, Train metric: 153.0132\n","Estimator 113/300, Train metric: 152.0968\n","Estimator 114/300, Train metric: 152.0169\n","Estimator 115/300, Train metric: 151.5674\n","Estimator 116/300, Train metric: 150.9646\n","Estimator 117/300, Train metric: 150.3975\n","Estimator 118/300, Train metric: 150.0504\n","Estimator 119/300, Train metric: 149.1492\n","Estimator 120/300, Train metric: 148.2636\n","Estimator 121/300, Train metric: 147.7807\n","Estimator 122/300, Train metric: 146.8118\n","Estimator 123/300, Train metric: 146.6503\n","Estimator 124/300, Train metric: 146.2448\n","Estimator 125/300, Train metric: 145.5855\n","Estimator 126/300, Train metric: 145.0321\n","Estimator 127/300, Train metric: 144.1719\n","Estimator 128/300, Train metric: 143.3982\n","Estimator 129/300, Train metric: 143.0535\n","Estimator 130/300, Train metric: 142.5845\n","Estimator 131/300, Train metric: 142.1598\n","Estimator 132/300, Train metric: 141.5157\n","Estimator 133/300, Train metric: 140.7440\n","Estimator 134/300, Train metric: 139.8669\n","Estimator 135/300, Train metric: 139.4283\n","Estimator 136/300, Train metric: 139.1162\n","Estimator 137/300, Train metric: 138.7000\n","Estimator 138/300, Train metric: 138.0478\n","Estimator 139/300, Train metric: 137.3692\n","Estimator 140/300, Train metric: 137.0073\n","Estimator 141/300, Train metric: 136.1382\n","Estimator 142/300, Train metric: 135.9412\n","Estimator 143/300, Train metric: 135.5603\n","Estimator 144/300, Train metric: 135.4472\n","Estimator 145/300, Train metric: 135.0465\n","Estimator 146/300, Train metric: 134.8389\n","Estimator 147/300, Train metric: 134.1186\n","Estimator 148/300, Train metric: 134.0089\n","Estimator 149/300, Train metric: 133.9847\n","Estimator 150/300, Train metric: 133.1961\n","Estimator 151/300, Train metric: 132.9082\n","Estimator 152/300, Train metric: 132.4612\n","Estimator 153/300, Train metric: 131.6622\n","Estimator 154/300, Train metric: 131.0006\n","Estimator 155/300, Train metric: 130.8274\n","Estimator 156/300, Train metric: 130.5676\n","Estimator 157/300, Train metric: 130.2408\n","Estimator 158/300, Train metric: 129.6384\n","Estimator 159/300, Train metric: 129.0515\n","Estimator 160/300, Train metric: 128.5374\n","Estimator 161/300, Train metric: 128.3464\n","Estimator 162/300, Train metric: 128.0383\n","Estimator 163/300, Train metric: 127.4290\n","Estimator 164/300, Train metric: 127.1691\n","Estimator 165/300, Train metric: 126.5610\n","Estimator 166/300, Train metric: 126.2794\n","Estimator 167/300, Train metric: 126.0449\n","Estimator 168/300, Train metric: 125.9239\n","Estimator 169/300, Train metric: 125.2348\n","Estimator 170/300, Train metric: 124.8229\n","Estimator 171/300, Train metric: 124.5499\n","Estimator 172/300, Train metric: 124.2697\n","Estimator 173/300, Train metric: 124.0786\n","Estimator 174/300, Train metric: 123.5390\n","Estimator 175/300, Train metric: 123.4346\n","Estimator 176/300, Train metric: 123.2689\n","Estimator 177/300, Train metric: 123.2193\n","Estimator 178/300, Train metric: 122.9451\n","Estimator 179/300, Train metric: 122.8463\n","Estimator 180/300, Train metric: 122.6873\n","Estimator 181/300, Train metric: 122.3923\n","Estimator 182/300, Train metric: 122.1912\n","Estimator 183/300, Train metric: 121.9641\n","Estimator 184/300, Train metric: 121.8079\n","Estimator 185/300, Train metric: 121.5726\n","Estimator 186/300, Train metric: 120.9478\n","Estimator 187/300, Train metric: 120.7913\n","Estimator 188/300, Train metric: 120.3500\n","Estimator 189/300, Train metric: 120.1729\n","Estimator 190/300, Train metric: 120.0550\n","Estimator 191/300, Train metric: 119.6556\n","Estimator 192/300, Train metric: 119.2752\n","Estimator 193/300, Train metric: 119.1537\n","Estimator 194/300, Train metric: 118.7786\n","Estimator 195/300, Train metric: 118.4411\n","Estimator 196/300, Train metric: 117.9038\n","Estimator 197/300, Train metric: 117.5907\n","Estimator 198/300, Train metric: 117.3457\n","Estimator 199/300, Train metric: 117.2922\n","Estimator 200/300, Train metric: 116.7763\n","Estimator 201/300, Train metric: 116.5751\n","Estimator 202/300, Train metric: 116.3903\n","Estimator 203/300, Train metric: 115.8997\n","Estimator 204/300, Train metric: 115.3339\n","Estimator 205/300, Train metric: 115.0282\n","Estimator 206/300, Train metric: 114.8473\n","Estimator 207/300, Train metric: 114.4438\n","Estimator 208/300, Train metric: 114.3024\n","Estimator 209/300, Train metric: 114.1382\n","Estimator 210/300, Train metric: 113.9605\n","Estimator 211/300, Train metric: 113.8458\n","Estimator 212/300, Train metric: 113.3609\n","Estimator 213/300, Train metric: 112.8634\n","Estimator 214/300, Train metric: 112.4591\n","Estimator 215/300, Train metric: 112.0142\n","Estimator 216/300, Train metric: 111.9220\n","Estimator 217/300, Train metric: 111.7957\n","Estimator 218/300, Train metric: 111.6727\n","Estimator 219/300, Train metric: 111.5750\n","Estimator 220/300, Train metric: 111.1822\n","Estimator 221/300, Train metric: 111.2063\n","Estimator 222/300, Train metric: 111.1803\n","Estimator 223/300, Train metric: 111.0954\n","Estimator 224/300, Train metric: 110.7831\n","Estimator 225/300, Train metric: 110.6200\n","Estimator 226/300, Train metric: 110.4575\n","Estimator 227/300, Train metric: 110.4318\n","Estimator 228/300, Train metric: 110.0983\n","Estimator 229/300, Train metric: 109.8953\n","Estimator 230/300, Train metric: 109.7908\n","Estimator 231/300, Train metric: 109.7809\n","Estimator 232/300, Train metric: 109.6569\n","Estimator 233/300, Train metric: 109.5286\n","Estimator 234/300, Train metric: 109.3806\n","Estimator 235/300, Train metric: 109.2808\n","Estimator 236/300, Train metric: 109.1552\n","Estimator 237/300, Train metric: 108.7751\n","Estimator 238/300, Train metric: 108.6999\n","Estimator 239/300, Train metric: 108.4237\n","Estimator 240/300, Train metric: 108.3201\n","Estimator 241/300, Train metric: 108.0854\n","Estimator 242/300, Train metric: 107.7792\n","Estimator 243/300, Train metric: 107.6947\n","Estimator 244/300, Train metric: 107.4751\n","Estimator 245/300, Train metric: 107.3900\n","Estimator 246/300, Train metric: 107.2224\n","Estimator 247/300, Train metric: 107.1245\n","Estimator 248/300, Train metric: 106.7355\n","Estimator 249/300, Train metric: 106.5651\n","Estimator 250/300, Train metric: 106.4184\n","Estimator 251/300, Train metric: 106.4076\n","Estimator 252/300, Train metric: 106.0540\n","Estimator 253/300, Train metric: 105.8309\n","Estimator 254/300, Train metric: 105.6375\n","Estimator 255/300, Train metric: 105.4184\n","Estimator 256/300, Train metric: 105.2096\n","Estimator 257/300, Train metric: 104.9705\n","Estimator 258/300, Train metric: 104.8538\n","Estimator 259/300, Train metric: 104.7152\n","Estimator 260/300, Train metric: 104.6018\n","Estimator 261/300, Train metric: 104.5548\n","Estimator 262/300, Train metric: 104.4208\n","Estimator 263/300, Train metric: 104.2422\n","Estimator 264/300, Train metric: 103.9508\n","Estimator 265/300, Train metric: 103.8164\n","Estimator 266/300, Train metric: 103.4930\n","Estimator 267/300, Train metric: 103.4403\n","Estimator 268/300, Train metric: 103.2669\n","Estimator 269/300, Train metric: 103.0338\n","Estimator 270/300, Train metric: 103.0615\n","Estimator 271/300, Train metric: 102.6915\n","Estimator 272/300, Train metric: 102.5906\n","Estimator 273/300, Train metric: 102.4658\n","Estimator 274/300, Train metric: 102.4512\n","Estimator 275/300, Train metric: 102.2343\n","Estimator 276/300, Train metric: 101.9850\n","Estimator 277/300, Train metric: 101.8475\n","Estimator 278/300, Train metric: 101.7112\n","Estimator 279/300, Train metric: 101.5239\n","Estimator 280/300, Train metric: 101.4557\n","Estimator 281/300, Train metric: 101.1332\n","Estimator 282/300, Train metric: 100.8939\n","Estimator 283/300, Train metric: 100.8496\n","Estimator 284/300, Train metric: 100.5667\n","Estimator 285/300, Train metric: 100.2835\n","Estimator 286/300, Train metric: 100.0848\n","Estimator 287/300, Train metric: 99.9909\n","Estimator 288/300, Train metric: 99.9714\n","Estimator 289/300, Train metric: 99.8056\n","Estimator 290/300, Train metric: 99.4987\n","Estimator 291/300, Train metric: 99.3233\n","Estimator 292/300, Train metric: 99.2788\n","Estimator 293/300, Train metric: 99.1979\n","Estimator 294/300, Train metric: 99.0064\n","Estimator 295/300, Train metric: 98.9646\n","Estimator 296/300, Train metric: 98.8805\n","Estimator 297/300, Train metric: 98.7480\n","Estimator 298/300, Train metric: 98.7610\n","Estimator 299/300, Train metric: 98.7423\n","epoch 0  | loss: 152222.96875| val_0_mse: 153868.59375|  0:00:00s\n","epoch 1  | loss: 150483.84375| val_0_mse: 149211.76562|  0:00:00s\n","epoch 2  | loss: 148098.54688| val_0_mse: 146123.4375|  0:00:00s\n","epoch 3  | loss: 146813.6875| val_0_mse: 142923.75|  0:00:00s\n","epoch 4  | loss: 145641.4375| val_0_mse: 139571.01562|  0:00:00s\n","epoch 5  | loss: 144214.85938| val_0_mse: 136519.20312|  0:00:00s\n","epoch 6  | loss: 142774.70312| val_0_mse: 130839.44531|  0:00:00s\n","epoch 7  | loss: 141427.65625| val_0_mse: 128234.24219|  0:00:00s\n","epoch 8  | loss: 140470.4375| val_0_mse: 125901.22656|  0:00:01s\n","epoch 9  | loss: 139392.51562| val_0_mse: 127233.92969|  0:00:01s\n","epoch 10 | loss: 137775.375| val_0_mse: 125370.09375|  0:00:01s\n","epoch 11 | loss: 136867.04688| val_0_mse: 123779.72656|  0:00:01s\n","epoch 12 | loss: 135679.01562| val_0_mse: 117037.60156|  0:00:01s\n","epoch 13 | loss: 134373.6875| val_0_mse: 115694.82031|  0:00:01s\n","epoch 14 | loss: 132978.82812| val_0_mse: 111743.625|  0:00:01s\n","epoch 15 | loss: 131509.76562| val_0_mse: 106045.78906|  0:00:01s\n","epoch 16 | loss: 129657.66406| val_0_mse: 102557.25|  0:00:01s\n","epoch 17 | loss: 127836.61719| val_0_mse: 99394.75|  0:00:02s\n","epoch 18 | loss: 126900.94531| val_0_mse: 89472.71094|  0:00:02s\n","epoch 19 | loss: 123926.41406| val_0_mse: 86203.40625|  0:00:02s\n","epoch 20 | loss: 122157.60156| val_0_mse: 83909.70312|  0:00:02s\n","epoch 21 | loss: 120514.77344| val_0_mse: 83392.22656|  0:00:02s\n","epoch 22 | loss: 118986.94531| val_0_mse: 83436.9375|  0:00:02s\n","epoch 23 | loss: 116392.30469| val_0_mse: 82387.80469|  0:00:02s\n","epoch 24 | loss: 114996.66406| val_0_mse: 80996.09375|  0:00:02s\n","epoch 25 | loss: 113121.94531| val_0_mse: 78002.46094|  0:00:02s\n","epoch 26 | loss: 111182.10156| val_0_mse: 75745.58594|  0:00:03s\n","epoch 27 | loss: 109114.96875| val_0_mse: 74159.88281|  0:00:03s\n","epoch 28 | loss: 107008.03906| val_0_mse: 72657.45312|  0:00:03s\n","epoch 29 | loss: 104728.80469| val_0_mse: 69025.78906|  0:00:03s\n","epoch 30 | loss: 102237.02344| val_0_mse: 66913.63281|  0:00:03s\n","epoch 31 | loss: 99890.67969| val_0_mse: 64198.40234|  0:00:03s\n","epoch 32 | loss: 97655.89844| val_0_mse: 61991.75391|  0:00:03s\n","epoch 33 | loss: 95339.4375| val_0_mse: 59602.77734|  0:00:03s\n","epoch 34 | loss: 93096.5 | val_0_mse: 57099.30859|  0:00:04s\n","epoch 35 | loss: 90837.58594| val_0_mse: 55978.35547|  0:00:04s\n","epoch 36 | loss: 88576.17969| val_0_mse: 54073.39062|  0:00:04s\n","epoch 37 | loss: 86329.22656| val_0_mse: 52069.99609|  0:00:04s\n","epoch 38 | loss: 84124.28125| val_0_mse: 49767.74609|  0:00:04s\n","epoch 39 | loss: 81858.3125| val_0_mse: 48174.41797|  0:00:04s\n","epoch 40 | loss: 79638.64062| val_0_mse: 46926.17578|  0:00:04s\n","epoch 41 | loss: 77541.88281| val_0_mse: 45802.12109|  0:00:04s\n","epoch 42 | loss: 75334.63281| val_0_mse: 44531.26953|  0:00:04s\n","epoch 43 | loss: 73338.89844| val_0_mse: 43193.1875|  0:00:05s\n","epoch 44 | loss: 70874.53125| val_0_mse: 40430.82422|  0:00:05s\n","epoch 45 | loss: 68595.00781| val_0_mse: 36912.79297|  0:00:05s\n","epoch 46 | loss: 66002.33594| val_0_mse: 34434.05078|  0:00:05s\n","epoch 47 | loss: 63427.31641| val_0_mse: 32080.05273|  0:00:05s\n","epoch 48 | loss: 60870.39062| val_0_mse: 29804.92969|  0:00:05s\n","epoch 49 | loss: 58390.20312| val_0_mse: 28463.20117|  0:00:06s\n","epoch 50 | loss: 56176.30469| val_0_mse: 27368.60938|  0:00:06s\n","epoch 51 | loss: 53478.67969| val_0_mse: 26409.27148|  0:00:06s\n","epoch 52 | loss: 50096.48047| val_0_mse: 25989.80664|  0:00:06s\n","epoch 53 | loss: 47927.60938| val_0_mse: 25848.44531|  0:00:06s\n","epoch 54 | loss: 45952.78125| val_0_mse: 30743.19336|  0:00:06s\n","epoch 55 | loss: 44025.01953| val_0_mse: 32202.07031|  0:00:07s\n","epoch 56 | loss: 42105.9375| val_0_mse: 33121.45312|  0:00:07s\n","epoch 57 | loss: 40213.44141| val_0_mse: 31942.11914|  0:00:07s\n","epoch 58 | loss: 38334.84766| val_0_mse: 30866.24805|  0:00:07s\n","epoch 59 | loss: 36482.57422| val_0_mse: 28862.82812|  0:00:07s\n","epoch 60 | loss: 34695.96484| val_0_mse: 30562.27148|  0:00:07s\n","epoch 61 | loss: 33093.21875| val_0_mse: 30441.53906|  0:00:07s\n","epoch 62 | loss: 31524.08594| val_0_mse: 29033.20312|  0:00:08s\n","epoch 63 | loss: 36739.48828| val_0_mse: 28375.23242|  0:00:08s\n","\n","Early stopping occurred at epoch 63 with best_epoch = 53 and best_val_0_mse = 25848.44531\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pytorch_tabnet import TabNetRegressor  # As you confirmed, TabNet is available!\n","\n","def get_best_models_and_predict(best_scores_autosampler, X_train, y_train, X_test, y_test, file_path):\n","    # Convert input data to NumPy arrays\n","    X_train = np.array(X_train)\n","    y_train = np.array(y_train)\n","    X_test = np.array(X_test)\n","    y_test = np.array(y_test)\n","\n","    # Model mapping, now including TabNet!\n","    model_mapping = {\n","        'Random Forest': RandomForestRegressor,\n","        'Gradient Boosting': GradientBoostingRegressor,\n","        'XGBoost': XGBRegressor,\n","        'LightGBM': LGBMRegressor,\n","        'CatBoost': CatBoostRegressor,\n","        'GPBoost': GPBoostRegressor,\n","        'NGBoost': NGBRegressor,\n","        'HistGradientBoosting': HistGradientBoostingRegressor,\n","        'PGBM': PGBM,\n","        'TabNet': TabNetRegressor\n","    }\n","\n","    # Dictionary to store the best model for each type\n","    best_models = {}\n","\n","    # Select the best pruner for each model type\n","    for (model_name, pruner), params in best_scores_autosampler.items():\n","        current_score = params.get('test_mse', np.inf)\n","        if model_name not in best_models or current_score < best_models[model_name]['score']:\n","            best_models[model_name] = {\n","                'score': current_score,\n","                'params': params['best_params'],\n","                'pruner': pruner\n","            }\n","\n","    # Prepare DataFrame for predictions\n","    df = pd.read_csv(file_path)\n","    successful_plots = []\n","\n","    # Iterate over the best models to train and predict\n","    for model_name, model_info in best_models.items():\n","        best_params = dict(model_info['params'])  # Shallow copy to avoid mutation\n","        model_class = model_mapping.get(model_name)\n","        if not model_class:\n","            print(f\"Model {model_name} is not supported or not available. Skipping.\")\n","            continue\n","\n","        try:\n","            if model_name == 'PGBM':\n","                model = model_class()\n","                model.train((X_train, y_train), objective=mseloss_objective, metric=rmseloss_metric, params=best_params)\n","                predictions = model.predict(X_test)\n","            elif model_name == 'TabNet':\n","                # Handle TabNet data shape and training\n","                y_train_fit = y_train.reshape(-1, 1)\n","                model = model_class(**{k: v for k, v in best_params.items() if k != \"verbose\"})\n","                model.fit(X_train, y_train_fit, max_epochs=100, patience=10, batch_size=1024, eval_set=[(X_train, y_train_fit)])\n","                predictions = model.predict(X_test).flatten()\n","            else:\n","                if model_name == 'CatBoost':\n","                    best_params.pop('verbose', None)\n","                model = model_class(**best_params)\n","                model.fit(X_train, y_train)\n","                predictions = model.predict(X_test)\n","        except Exception as e:\n","            print(f\"Failed to train or predict with model {model_name}: {e}\")\n","            continue\n","\n","        # Ensure predictions are 1D for plotting and DataFrame\n","        predictions = np.array(predictions).flatten()\n","        df[f'{model_name} Predictions'] = predictions\n","\n","        # Plot actual vs. predicted\n","        try:\n","            plt.figure(figsize=(10, 6))\n","            plt.scatter(y_test, predictions, alpha=0.6)\n","            plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='red', lw=2)\n","            plt.xlabel(\"Actual\")\n","            plt.ylabel(\"Predicted\")\n","            plt.title(f\"Actual vs. Predicted Values ({model_name})\")\n","            plt.grid(True)\n","            plt.tight_layout()\n","            plot_path = f'temp_plot_{model_name}.png'\n","            plt.savefig(plot_path)\n","            plt.close()\n","            successful_plots.append((model_name, plot_path))\n","        except Exception as e:\n","            print(f\"Plotting failed for {model_name}: {e}\")\n","\n","    # Save predictions and plots to Excel\n","    with pd.ExcelWriter(file_path.replace('.csv', '_results.xlsx'), engine='xlsxwriter') as writer:\n","        df.to_excel(writer, sheet_name='Data', index=False)\n","        workbook = writer.book\n","        # Insert ONLY the images that actually exist\n","        for model_name, plot_path in successful_plots:\n","            if os.path.exists(plot_path):\n","                short_model_name = ''.join(word[0] for word in model_name.split())\n","                sheet_name = f'{short_model_name}_Plot'\n","                worksheet = workbook.add_worksheet(sheet_name)\n","                writer.sheets[sheet_name] = worksheet\n","                worksheet.insert_image('A1', plot_path)\n","            else:\n","                print(f\"Plot image does not exist and will not be inserted for {model_name}: {plot_path}\")\n","\n","    # Clean up ONLY the files that were actually created\n","    for _, plot_path in successful_plots:\n","        if os.path.exists(plot_path):\n","            os.remove(plot_path)\n","\n","    return df, best_models\n","\n","# Call the function\n","df, best_models = get_best_models_and_predict(best_scores_autosampler, X_train, y_train, X_test, y_test, \"./drive/MyDrive/streamflow/TDS/test.csv\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"WiVmP0QP3pKn","executionInfo":{"status":"ok","timestamp":1762771218249,"user_tz":-330,"elapsed":1568,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"}}},"outputs":[],"source":["plot_best_scores(best_scores_autosampler,\"./drive/MyDrive/streamflow/TDS/tds_ml_tabnet/HyperParameter_Tuning/test_results.xlsx\")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ynnJWQRA3pKn","executionInfo":{"status":"ok","timestamp":1762771413489,"user_tz":-330,"elapsed":195237,"user":{"displayName":"Mahesh Pal","userId":"08757421473100740410"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1903da8bba5145cbad971b4d8e8e42e5","14ba26b14ac4454eb67e4cbcd54bf3f6","53849ef2528e486e8e0084869d5f0b4d","81eda6cd8a67412782c3f4214f8ff0f5","84bfbb500f444c1a9948944455b7ea3d","c8098c36a0844385b1fe884a775db3a0","8b30577a65f944dc8a14d24ef8cc207a","8674a4dded43462885765618668765e1","0c9842de17d04d868e8acfbe175b78a0","7d17a07071fd4f67ac72bc53915bac8c","42e9b25edac8402f9e1ffd552664a684","bb638a038ae94d1c84c96d18a6031b9b","184841e5f43941eeb6ea511063f76563","1d69e236b9ee4ed69f4fdcb3cef5a151","e567233561e6452f8eb457add191dece","566d6c49e5d840b1841ea493a4403584","f9515fc6d8de452cac03b53f1f33984d","d89ebefdb75e47b2890d26eddaab5c5f","b5ea3773b2024a2897781c9bd14227d9","a75ce7727ad840b99fcf1721b3ea1a44","f13e36bbb4f04728bcbc9aab18c78b5d","abe573ad0ec94bb59b38c1d4bc154cb2","bf74e8492b00423e9b1afe9405901cc7","a8176e4d9e754ae8b878f6524dce48fd","a33fe310160540dda204c291a4557f98","1794332f0ce9477089782f4bc552fd15","c3eefbafcf4d4bc29869a46a2e2e450f","b94a5a48cbb148f18a87eb106fb4de26","234a8d19e18e4c14a4de416f439603a4","2ba56682f8cc4d7f858c3147ca71ec8b","d29b7680ef234cd2842e608c5ebef146","3251cc9961474653a57e47e51153a5df","7316a63062f04cf4ac95e669e33d1be3","f098d2ccad2f4c64b0e2453eca657d68","7f5632476d594041aa61a4fd7eae5ff0","6ffa468269fd45cda052a97544f4c2cc","fc609223d00040328759efb50de4e266","320b51275a9b45d7bdd6cd08de49fccd","e67c198603e2454196118e61c41b7d36","7abbf369eb224ab4b9cda4f4a326a16a","422932d7d54f4377addca6064f38eb53","c58eb48b61db46989869db2cf09d7950","b65d97596f924205ad01996f610f9d55","c1373b38922b40a9bff16664b7664fed","3d40de9be7fe4ae6a42f30429aae7f70","fe984f96ecbc4d8ebdd8ec6371e0814a","effb29bdf710456dbf4cbe480d9ced7f","b0250a0a00a04c0298a6203c43e74d74","1ed5391c33ac489181062d5d7a30c6ae","ca4c66806e8d4a7fab89de3d9b00d3ff","d661cf5e214c4c98bd82309844f4761d","40c4e0c24d354ed496833b4ac4cba50d","5dc9a2bd24e64dd1bc9f676f761d9af1","1dc4080c6c784a5c993ed18a75652a56","e59b0f587ba24744b61e7b354abdc3f2","1109e47f77fc4834b5648a5d8b7deeef","bdbfe2ff787f40d0bda60dd7707ae665","b87f1bcc3244424ba50d2c75db213b28","0ecd097e767546bb97b3c86a1790cd40","a923266926cd43ab9b6c9d05e42e48bd","aa598c12607a4c91a94dbf2e4ad63d73","b2c22ef8fdf74704a70e363174f74a5d","9b43559be8dc47209e1ec24bce62a0f2","ba089258069f41d6acc1aa6945564b1d","ee994b74a7624430a4db4c3dcc5f6e59","6aa6df4eac004ebc91080fa4f7245f41","eeefec7f30bf4dd2a53466f83e34b889","6608806b515840cc97229044497a1122","35e56f22636a4c0992800e441a612edc","b996fb6f574f40579977bc5c5f3bafaf","4680b43d9a9e47f6ba83543cb4f37699","e00f3b7568714ad8a5217506482c7818","30e0bcc567174b4e8ee9862024a22100","694e035491fd40578b3d4845f29b4c6c","570ad4c8609f4b508f881c71f8dcb755","512c0a9476e94c52b4adda0d10ac5bef","42ff1b2057a0419bbe09b68c48029d47","1c592e595a554323b67a270af33f9c96","f9ecaecc149940ec81548905933d5540","4560af29b805448686c0e11d4464b927","bd80237538914706a2f4f59c82993b7c","5d5eb19b740b4ea4963239e1d612e3db","54152f6474134e50b8d7335a19d9c034","d765a7971be24243a13457918b06b4a9","59addba3060a42ac81217e5c936a5faf","e25fc289dcc946b69f72544c49b1e1de","067f32a40eab426395c8331bd8d98305","22b37cb4049f48cb83cab90fabe6d6db"]},"outputId":"5b842160-bb00-477a-b77a-f4a652f9de4e"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:shap:Using 192 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1903da8bba5145cbad971b4d8e8e42e5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:shap:Using 192 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb638a038ae94d1c84c96d18a6031b9b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:shap:Using 192 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf74e8492b00423e9b1afe9405901cc7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:shap:Using 192 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f098d2ccad2f4c64b0e2453eca657d68"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:shap:Using 192 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d40de9be7fe4ae6a42f30429aae7f70"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:shap:Using 192 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1109e47f77fc4834b5648a5d8b7deeef"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:shap:Using 192 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeefec7f30bf4dd2a53466f83e34b889"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Model HistGradientBoosting is not supported or not available.\n","Model PGBM is not supported or not available.\n","epoch 0  | loss: 152222.96875| val_0_mse: 153868.59375|  0:00:00s\n","epoch 1  | loss: 150483.84375| val_0_mse: 149211.76562|  0:00:00s\n","epoch 2  | loss: 148098.54688| val_0_mse: 146123.4375|  0:00:00s\n","epoch 3  | loss: 146813.6875| val_0_mse: 142923.75|  0:00:00s\n","epoch 4  | loss: 145641.4375| val_0_mse: 139571.01562|  0:00:01s\n","epoch 5  | loss: 144214.85938| val_0_mse: 136519.20312|  0:00:01s\n","epoch 6  | loss: 142774.70312| val_0_mse: 130839.44531|  0:00:01s\n","epoch 7  | loss: 141427.65625| val_0_mse: 128234.24219|  0:00:01s\n","epoch 8  | loss: 140470.4375| val_0_mse: 125901.22656|  0:00:01s\n","epoch 9  | loss: 139392.51562| val_0_mse: 127233.92969|  0:00:01s\n","epoch 10 | loss: 137775.375| val_0_mse: 125370.09375|  0:00:01s\n","epoch 11 | loss: 136867.04688| val_0_mse: 123779.72656|  0:00:01s\n","epoch 12 | loss: 135679.01562| val_0_mse: 117037.60156|  0:00:01s\n","epoch 13 | loss: 134373.6875| val_0_mse: 115694.82031|  0:00:02s\n","epoch 14 | loss: 132978.82812| val_0_mse: 111743.625|  0:00:02s\n","epoch 15 | loss: 131509.76562| val_0_mse: 106045.78906|  0:00:02s\n","epoch 16 | loss: 129657.66406| val_0_mse: 102557.25|  0:00:02s\n","epoch 17 | loss: 127836.61719| val_0_mse: 99394.75|  0:00:02s\n","epoch 18 | loss: 126900.94531| val_0_mse: 89472.71094|  0:00:02s\n","epoch 19 | loss: 123926.41406| val_0_mse: 86203.40625|  0:00:03s\n","epoch 20 | loss: 122157.60156| val_0_mse: 83909.70312|  0:00:03s\n","epoch 21 | loss: 120514.77344| val_0_mse: 83392.22656|  0:00:03s\n","epoch 22 | loss: 118986.94531| val_0_mse: 83436.9375|  0:00:03s\n","epoch 23 | loss: 116392.30469| val_0_mse: 82387.80469|  0:00:03s\n","epoch 24 | loss: 114996.66406| val_0_mse: 80996.09375|  0:00:03s\n","epoch 25 | loss: 113121.94531| val_0_mse: 78002.46094|  0:00:03s\n","epoch 26 | loss: 111182.10156| val_0_mse: 75745.58594|  0:00:04s\n","epoch 27 | loss: 109114.96875| val_0_mse: 74159.88281|  0:00:04s\n","epoch 28 | loss: 107008.03906| val_0_mse: 72657.45312|  0:00:04s\n","epoch 29 | loss: 104728.80469| val_0_mse: 69025.78906|  0:00:04s\n","epoch 30 | loss: 102237.02344| val_0_mse: 66913.63281|  0:00:04s\n","epoch 31 | loss: 99890.67969| val_0_mse: 64198.40234|  0:00:04s\n","epoch 32 | loss: 97655.89844| val_0_mse: 61991.75391|  0:00:05s\n","epoch 33 | loss: 95339.4375| val_0_mse: 59602.77734|  0:00:05s\n","epoch 34 | loss: 93096.5 | val_0_mse: 57099.30859|  0:00:05s\n","epoch 35 | loss: 90837.58594| val_0_mse: 55978.35547|  0:00:05s\n","epoch 36 | loss: 88576.17969| val_0_mse: 54073.39062|  0:00:05s\n","epoch 37 | loss: 86329.22656| val_0_mse: 52069.99609|  0:00:05s\n","epoch 38 | loss: 84124.28125| val_0_mse: 49767.74609|  0:00:05s\n","epoch 39 | loss: 81858.3125| val_0_mse: 48174.41797|  0:00:06s\n","epoch 40 | loss: 79638.64062| val_0_mse: 46926.17578|  0:00:06s\n","epoch 41 | loss: 77541.88281| val_0_mse: 45802.12109|  0:00:06s\n","epoch 42 | loss: 75334.63281| val_0_mse: 44531.26953|  0:00:06s\n","epoch 43 | loss: 73338.89844| val_0_mse: 43193.1875|  0:00:06s\n","epoch 44 | loss: 70874.53125| val_0_mse: 40430.82422|  0:00:06s\n","epoch 45 | loss: 68595.00781| val_0_mse: 36912.79297|  0:00:06s\n","epoch 46 | loss: 66002.33594| val_0_mse: 34434.05078|  0:00:06s\n","epoch 47 | loss: 63427.31641| val_0_mse: 32080.05273|  0:00:06s\n","epoch 48 | loss: 60870.39062| val_0_mse: 29804.92969|  0:00:07s\n","epoch 49 | loss: 58390.20312| val_0_mse: 28463.20117|  0:00:07s\n","epoch 50 | loss: 56176.30469| val_0_mse: 27368.60938|  0:00:07s\n","epoch 51 | loss: 53478.67969| val_0_mse: 26409.27148|  0:00:07s\n","epoch 52 | loss: 50096.48047| val_0_mse: 25989.80664|  0:00:07s\n","epoch 53 | loss: 47927.60938| val_0_mse: 25848.44531|  0:00:07s\n","epoch 54 | loss: 45952.78125| val_0_mse: 30743.19336|  0:00:07s\n","epoch 55 | loss: 44025.01953| val_0_mse: 32202.07031|  0:00:07s\n","epoch 56 | loss: 42105.9375| val_0_mse: 33121.45312|  0:00:07s\n","epoch 57 | loss: 40213.44141| val_0_mse: 31942.11914|  0:00:08s\n","epoch 58 | loss: 38334.84766| val_0_mse: 30866.24805|  0:00:08s\n","epoch 59 | loss: 36482.57422| val_0_mse: 28862.82812|  0:00:08s\n","epoch 60 | loss: 34695.96484| val_0_mse: 30562.27148|  0:00:08s\n","epoch 61 | loss: 33093.21875| val_0_mse: 30441.53906|  0:00:08s\n","epoch 62 | loss: 31524.08594| val_0_mse: 29033.20312|  0:00:08s\n","epoch 63 | loss: 36739.48828| val_0_mse: 28375.23242|  0:00:08s\n","\n","Early stopping occurred at epoch 63 with best_epoch = 53 and best_val_0_mse = 25848.44531\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:shap:Using 192 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c592e595a554323b67a270af33f9c96"}},"metadata":{}}],"source":["generate_interpretml_explanations_summary_pruners(best_scores_autosampler, X_train, y_train, x_test, feature_names,excel_file_path = \"./drive/MyDrive/streamflow/TDS/tds_ml_tabnet/HyperParameter_Tuning/test_results.xlsx\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1903da8bba5145cbad971b4d8e8e42e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14ba26b14ac4454eb67e4cbcd54bf3f6","IPY_MODEL_53849ef2528e486e8e0084869d5f0b4d","IPY_MODEL_81eda6cd8a67412782c3f4214f8ff0f5"],"layout":"IPY_MODEL_84bfbb500f444c1a9948944455b7ea3d"}},"14ba26b14ac4454eb67e4cbcd54bf3f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8098c36a0844385b1fe884a775db3a0","placeholder":"​","style":"IPY_MODEL_8b30577a65f944dc8a14d24ef8cc207a","value":"100%"}},"53849ef2528e486e8e0084869d5f0b4d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8674a4dded43462885765618668765e1","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c9842de17d04d868e8acfbe175b78a0","value":96}},"81eda6cd8a67412782c3f4214f8ff0f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d17a07071fd4f67ac72bc53915bac8c","placeholder":"​","style":"IPY_MODEL_42e9b25edac8402f9e1ffd552664a684","value":" 96/96 [00:13&lt;00:00,  6.90it/s]"}},"84bfbb500f444c1a9948944455b7ea3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8098c36a0844385b1fe884a775db3a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b30577a65f944dc8a14d24ef8cc207a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8674a4dded43462885765618668765e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c9842de17d04d868e8acfbe175b78a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d17a07071fd4f67ac72bc53915bac8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42e9b25edac8402f9e1ffd552664a684":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb638a038ae94d1c84c96d18a6031b9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_184841e5f43941eeb6ea511063f76563","IPY_MODEL_1d69e236b9ee4ed69f4fdcb3cef5a151","IPY_MODEL_e567233561e6452f8eb457add191dece"],"layout":"IPY_MODEL_566d6c49e5d840b1841ea493a4403584"}},"184841e5f43941eeb6ea511063f76563":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9515fc6d8de452cac03b53f1f33984d","placeholder":"​","style":"IPY_MODEL_d89ebefdb75e47b2890d26eddaab5c5f","value":"100%"}},"1d69e236b9ee4ed69f4fdcb3cef5a151":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5ea3773b2024a2897781c9bd14227d9","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a75ce7727ad840b99fcf1721b3ea1a44","value":96}},"e567233561e6452f8eb457add191dece":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f13e36bbb4f04728bcbc9aab18c78b5d","placeholder":"​","style":"IPY_MODEL_abe573ad0ec94bb59b38c1d4bc154cb2","value":" 96/96 [00:00&lt;00:00, 198.86it/s]"}},"566d6c49e5d840b1841ea493a4403584":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9515fc6d8de452cac03b53f1f33984d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d89ebefdb75e47b2890d26eddaab5c5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5ea3773b2024a2897781c9bd14227d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a75ce7727ad840b99fcf1721b3ea1a44":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f13e36bbb4f04728bcbc9aab18c78b5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abe573ad0ec94bb59b38c1d4bc154cb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf74e8492b00423e9b1afe9405901cc7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8176e4d9e754ae8b878f6524dce48fd","IPY_MODEL_a33fe310160540dda204c291a4557f98","IPY_MODEL_1794332f0ce9477089782f4bc552fd15"],"layout":"IPY_MODEL_c3eefbafcf4d4bc29869a46a2e2e450f"}},"a8176e4d9e754ae8b878f6524dce48fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b94a5a48cbb148f18a87eb106fb4de26","placeholder":"​","style":"IPY_MODEL_234a8d19e18e4c14a4de416f439603a4","value":"100%"}},"a33fe310160540dda204c291a4557f98":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ba56682f8cc4d7f858c3147ca71ec8b","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d29b7680ef234cd2842e608c5ebef146","value":96}},"1794332f0ce9477089782f4bc552fd15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3251cc9961474653a57e47e51153a5df","placeholder":"​","style":"IPY_MODEL_7316a63062f04cf4ac95e669e33d1be3","value":" 96/96 [00:03&lt;00:00, 14.84it/s]"}},"c3eefbafcf4d4bc29869a46a2e2e450f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b94a5a48cbb148f18a87eb106fb4de26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"234a8d19e18e4c14a4de416f439603a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ba56682f8cc4d7f858c3147ca71ec8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d29b7680ef234cd2842e608c5ebef146":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3251cc9961474653a57e47e51153a5df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7316a63062f04cf4ac95e669e33d1be3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f098d2ccad2f4c64b0e2453eca657d68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f5632476d594041aa61a4fd7eae5ff0","IPY_MODEL_6ffa468269fd45cda052a97544f4c2cc","IPY_MODEL_fc609223d00040328759efb50de4e266"],"layout":"IPY_MODEL_320b51275a9b45d7bdd6cd08de49fccd"}},"7f5632476d594041aa61a4fd7eae5ff0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e67c198603e2454196118e61c41b7d36","placeholder":"​","style":"IPY_MODEL_7abbf369eb224ab4b9cda4f4a326a16a","value":"100%"}},"6ffa468269fd45cda052a97544f4c2cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_422932d7d54f4377addca6064f38eb53","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c58eb48b61db46989869db2cf09d7950","value":96}},"fc609223d00040328759efb50de4e266":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b65d97596f924205ad01996f610f9d55","placeholder":"​","style":"IPY_MODEL_c1373b38922b40a9bff16664b7664fed","value":" 96/96 [00:05&lt;00:00, 19.31it/s]"}},"320b51275a9b45d7bdd6cd08de49fccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e67c198603e2454196118e61c41b7d36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7abbf369eb224ab4b9cda4f4a326a16a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"422932d7d54f4377addca6064f38eb53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c58eb48b61db46989869db2cf09d7950":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b65d97596f924205ad01996f610f9d55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1373b38922b40a9bff16664b7664fed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d40de9be7fe4ae6a42f30429aae7f70":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe984f96ecbc4d8ebdd8ec6371e0814a","IPY_MODEL_effb29bdf710456dbf4cbe480d9ced7f","IPY_MODEL_b0250a0a00a04c0298a6203c43e74d74"],"layout":"IPY_MODEL_1ed5391c33ac489181062d5d7a30c6ae"}},"fe984f96ecbc4d8ebdd8ec6371e0814a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca4c66806e8d4a7fab89de3d9b00d3ff","placeholder":"​","style":"IPY_MODEL_d661cf5e214c4c98bd82309844f4761d","value":"100%"}},"effb29bdf710456dbf4cbe480d9ced7f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40c4e0c24d354ed496833b4ac4cba50d","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5dc9a2bd24e64dd1bc9f676f761d9af1","value":96}},"b0250a0a00a04c0298a6203c43e74d74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dc4080c6c784a5c993ed18a75652a56","placeholder":"​","style":"IPY_MODEL_e59b0f587ba24744b61e7b354abdc3f2","value":" 96/96 [00:02&lt;00:00, 36.61it/s]"}},"1ed5391c33ac489181062d5d7a30c6ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca4c66806e8d4a7fab89de3d9b00d3ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d661cf5e214c4c98bd82309844f4761d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40c4e0c24d354ed496833b4ac4cba50d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dc9a2bd24e64dd1bc9f676f761d9af1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1dc4080c6c784a5c993ed18a75652a56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e59b0f587ba24744b61e7b354abdc3f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1109e47f77fc4834b5648a5d8b7deeef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bdbfe2ff787f40d0bda60dd7707ae665","IPY_MODEL_b87f1bcc3244424ba50d2c75db213b28","IPY_MODEL_0ecd097e767546bb97b3c86a1790cd40"],"layout":"IPY_MODEL_a923266926cd43ab9b6c9d05e42e48bd"}},"bdbfe2ff787f40d0bda60dd7707ae665":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa598c12607a4c91a94dbf2e4ad63d73","placeholder":"​","style":"IPY_MODEL_b2c22ef8fdf74704a70e363174f74a5d","value":"100%"}},"b87f1bcc3244424ba50d2c75db213b28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b43559be8dc47209e1ec24bce62a0f2","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba089258069f41d6acc1aa6945564b1d","value":96}},"0ecd097e767546bb97b3c86a1790cd40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee994b74a7624430a4db4c3dcc5f6e59","placeholder":"​","style":"IPY_MODEL_6aa6df4eac004ebc91080fa4f7245f41","value":" 96/96 [00:01&lt;00:00, 58.25it/s]"}},"a923266926cd43ab9b6c9d05e42e48bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa598c12607a4c91a94dbf2e4ad63d73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2c22ef8fdf74704a70e363174f74a5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b43559be8dc47209e1ec24bce62a0f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba089258069f41d6acc1aa6945564b1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee994b74a7624430a4db4c3dcc5f6e59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aa6df4eac004ebc91080fa4f7245f41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eeefec7f30bf4dd2a53466f83e34b889":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6608806b515840cc97229044497a1122","IPY_MODEL_35e56f22636a4c0992800e441a612edc","IPY_MODEL_b996fb6f574f40579977bc5c5f3bafaf"],"layout":"IPY_MODEL_4680b43d9a9e47f6ba83543cb4f37699"}},"6608806b515840cc97229044497a1122":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e00f3b7568714ad8a5217506482c7818","placeholder":"​","style":"IPY_MODEL_30e0bcc567174b4e8ee9862024a22100","value":"100%"}},"35e56f22636a4c0992800e441a612edc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_694e035491fd40578b3d4845f29b4c6c","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_570ad4c8609f4b508f881c71f8dcb755","value":96}},"b996fb6f574f40579977bc5c5f3bafaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_512c0a9476e94c52b4adda0d10ac5bef","placeholder":"​","style":"IPY_MODEL_42ff1b2057a0419bbe09b68c48029d47","value":" 96/96 [00:24&lt;00:00,  4.18it/s]"}},"4680b43d9a9e47f6ba83543cb4f37699":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e00f3b7568714ad8a5217506482c7818":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30e0bcc567174b4e8ee9862024a22100":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"694e035491fd40578b3d4845f29b4c6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"570ad4c8609f4b508f881c71f8dcb755":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"512c0a9476e94c52b4adda0d10ac5bef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42ff1b2057a0419bbe09b68c48029d47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c592e595a554323b67a270af33f9c96":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9ecaecc149940ec81548905933d5540","IPY_MODEL_4560af29b805448686c0e11d4464b927","IPY_MODEL_bd80237538914706a2f4f59c82993b7c"],"layout":"IPY_MODEL_5d5eb19b740b4ea4963239e1d612e3db"}},"f9ecaecc149940ec81548905933d5540":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54152f6474134e50b8d7335a19d9c034","placeholder":"​","style":"IPY_MODEL_d765a7971be24243a13457918b06b4a9","value":"100%"}},"4560af29b805448686c0e11d4464b927":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59addba3060a42ac81217e5c936a5faf","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e25fc289dcc946b69f72544c49b1e1de","value":96}},"bd80237538914706a2f4f59c82993b7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_067f32a40eab426395c8331bd8d98305","placeholder":"​","style":"IPY_MODEL_22b37cb4049f48cb83cab90fabe6d6db","value":" 96/96 [00:13&lt;00:00,  7.58it/s]"}},"5d5eb19b740b4ea4963239e1d612e3db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54152f6474134e50b8d7335a19d9c034":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d765a7971be24243a13457918b06b4a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59addba3060a42ac81217e5c936a5faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e25fc289dcc946b69f72544c49b1e1de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"067f32a40eab426395c8331bd8d98305":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22b37cb4049f48cb83cab90fabe6d6db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}